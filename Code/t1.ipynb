{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../Dataset'\n",
    "\n",
    "path_github = os.path.join(path_dataset, 'GitHub')\n",
    "path_gitlab = os.path.join(path_dataset, 'GitLab')\n",
    "\n",
    "path_github_repo = os.path.join(path_github, 'Repo')\n",
    "path_gitlab_repo = os.path.join(path_gitlab, 'Repo')\n",
    "path_github_repo_raw = os.path.join(path_github_repo, 'Raw')\n",
    "path_gitlab_repo_raw = os.path.join(path_gitlab_repo, 'Raw')\n",
    "path_github_repo_scraped = os.path.join(path_github_repo, 'Scraped')\n",
    "path_gitlab_repo_scraped = os.path.join(path_gitlab_repo, 'Scraped')\n",
    "path_gitlab_repo_labelled = os.path.join(path_gitlab_repo, 'labelled')\n",
    "\n",
    "path_github_issue = os.path.join(path_github, 'Issue')\n",
    "path_gitlab_issue = os.path.join(path_gitlab, 'Issue')\n",
    "path_github_issue_raw = os.path.join(path_github_issue, 'Raw')\n",
    "path_gitlab_issue_raw = os.path.join(path_gitlab_issue, 'Raw')\n",
    "\n",
    "if not os.path.exists(path_github):\n",
    "    os.makedirs(path_github)\n",
    "\n",
    "if not os.path.exists(path_gitlab):\n",
    "    os.makedirs(path_gitlab)\n",
    "\n",
    "if not os.path.exists(path_github_repo):\n",
    "    os.makedirs(path_github_repo)\n",
    "\n",
    "if not os.path.exists(path_gitlab_repo):\n",
    "    os.makedirs(path_gitlab_repo)\n",
    "\n",
    "if not os.path.exists(path_github_issue):\n",
    "    os.makedirs(path_github_issue)\n",
    "\n",
    "if not os.path.exists(path_gitlab_issue):\n",
    "    os.makedirs(path_gitlab_issue)\n",
    "\n",
    "if not os.path.exists(path_github_repo_raw):\n",
    "    os.makedirs(path_github_repo_raw)\n",
    "\n",
    "if not os.path.exists(path_gitlab_repo_raw):\n",
    "    os.makedirs(path_gitlab_repo_raw)\n",
    "\n",
    "if not os.path.exists(path_github_issue_raw):\n",
    "    os.makedirs(path_github_issue_raw)\n",
    "\n",
    "if not os.path.exists(path_gitlab_issue_raw):\n",
    "    os.makedirs(path_gitlab_issue_raw)\n",
    "\n",
    "if not os.path.exists(path_github_repo_scraped):\n",
    "    os.makedirs(path_github_repo_scraped)\n",
    "\n",
    "if not os.path.exists(path_gitlab_repo_scraped):\n",
    "    os.makedirs(path_gitlab_repo_scraped)\n",
    "\n",
    "if not os.path.exists(path_gitlab_repo_labelled):\n",
    "    os.makedirs(path_gitlab_repo_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_token1 = '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'\n",
    "github_token2 = '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'\n",
    "github_token3 = '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'\n",
    "github_token4 = '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'\n",
    "github_token5 = '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'\n",
    "github_token6 = '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'\n",
    "github_token7 = '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'\n",
    "gitlab_token1 = 'glpat-LFsxferBHR75dL9XKvos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(path_dataset, 'Tools.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow: 21670\n"
     ]
    }
   ],
   "source": [
    "from github import Github\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def sleep_wrapper(func, **args):\n",
    "    time.sleep(3)\n",
    "    return func(**args)\n",
    "\n",
    "github = Github(login_or_token=github_token1)\n",
    "\n",
    "# scrape issues of Github dependents for each tool\n",
    "for index, row in df.iterrows():\n",
    "    if index != 11:\n",
    "        continue\n",
    "    file_name = os.path.join(path_github_repo_scraped, f'{row[\"Name\"]}2.json')\n",
    "    if os.path.exists(file_name):\n",
    "        repos = pd.read_json(file_name)\n",
    "        # filter out repos with only pr-based issues\n",
    "        repos = repos[repos['#Issues'] > repos['#Pull Requests']]\n",
    "        # filter out repos created before the tool's first release date\n",
    "        repos = repos[repos['Repo Creation Date'] > row['First Release Date']]\n",
    "        print(f'{row[\"Name\"]}: {repos[\"#Issues\"].sum() - repos[\"#Pull Requests\"].sum()}')\n",
    "        # scrape issues for the current tool\n",
    "        issues_list_data = pd.DataFrame()\n",
    "        errors_list_data = pd.DataFrame()\n",
    "        for repo_name in repos['Repo'].tolist():\n",
    "            try:\n",
    "                repo = sleep_wrapper(github.get_repo, full_name_or_id=repo_name)\n",
    "                issues = sleep_wrapper(repo.get_issues, state='all')\n",
    "                issues_data = pd.DataFrame()\n",
    "\n",
    "                for issue in issues:\n",
    "                    time.sleep(3)\n",
    "                    if not issue.pull_request:\n",
    "                        reactions = issue.get_reactions()\n",
    "\n",
    "                        issue_data = {}\n",
    "                        issue_data['Issue_link'] = issue.html_url\n",
    "                        issue_data['Issue_title'] = issue.title\n",
    "                        issue_data['Issue_label'] = [\n",
    "                            label.name for label in issue.labels]\n",
    "                        issue_data['Issue_creation_time'] = issue.created_at\n",
    "                        issue_data['Issue_closed_time'] = issue.closed_at\n",
    "                        issue_data['Issue_upvote_count'] = sum(\n",
    "                            reaction.content == '+1' for reaction in reactions)\n",
    "                        issue_data['Issue_downvote_count'] = sum(\n",
    "                            reaction.content == '-1' for reaction in reactions)\n",
    "                        issue_data['Issue_comment_count'] = issue.comments\n",
    "                        issue_data['Issue_body'] = issue.body\n",
    "\n",
    "                        issue_data = pd.DataFrame([issue_data])\n",
    "                        issues_data = pd.concat(\n",
    "                            [issues_data, issue_data], ignore_index=True)\n",
    "                issues_list_data = pd.concat([issues_list_data, issues_data], ignore_index=True)\n",
    "                issues_list_data.to_json(os.path.join(path_github_issue_raw, f'{row[\"Name\"]}2.json'), indent=4, orient='records')\n",
    "            except Exception as err:\n",
    "                error_data = {'Repo': repo_name, 'Error': err.status}\n",
    "                error_data = pd.DataFrame([error_data])\n",
    "                errors_list_data = pd.concat([errors_list_data, error_data], ignore_index=True)\n",
    "                errors_list_data.to_json(os.path.join(path_github_issue_raw, f'Error.{row[\"Name\"]}2.json'), indent=4, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb9e7b88a259684df50811b5249344f7cc06d54cdb1cf11111ce301ae44eac9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

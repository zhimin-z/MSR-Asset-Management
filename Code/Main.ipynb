{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../Dataset'\n",
    "\n",
    "path_github = os.path.join(path_dataset, 'GitHub')\n",
    "path_gitlab = os.path.join(path_dataset, 'GitLab')\n",
    "path_labeling = os.path.join(path_dataset, 'Labeling')\n",
    "\n",
    "path_github_repo = os.path.join(path_github, 'Repo')\n",
    "path_gitlab_repo = os.path.join(path_gitlab, 'Repo')\n",
    "path_github_issue = os.path.join(path_github, 'Issue')\n",
    "path_gitlab_issue = os.path.join(path_gitlab, 'Issue')\n",
    "\n",
    "if not os.path.exists(path_github):\n",
    "    os.makedirs(path_github)\n",
    "\n",
    "if not os.path.exists(path_gitlab):\n",
    "    os.makedirs(path_gitlab)\n",
    "\n",
    "if not os.path.exists(path_labeling):\n",
    "    os.makedirs(path_labeling)\n",
    "\n",
    "if not os.path.exists(path_github_repo):\n",
    "    os.makedirs(path_github_repo)\n",
    "\n",
    "if not os.path.exists(path_gitlab_repo):\n",
    "    os.makedirs(path_gitlab_repo)\n",
    "\n",
    "if not os.path.exists(path_github_issue):\n",
    "    os.makedirs(path_github_issue)\n",
    "\n",
    "if not os.path.exists(path_gitlab_issue):\n",
    "    os.makedirs(path_gitlab_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from subprocess import call\n",
    "\n",
    "# call([\"./Dependents_Lookup.sh\"], cwd=path_github_repo, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_repo = {\n",
    "    'Aim': 'aimhubio/aim',\n",
    "    'Amazon SageMaker': 'aws/sagemaker-python-sdk',\n",
    "    'Azure Machine Learning': 'Azure/azure-sdk-for-python',\n",
    "    'ClearML': 'allegroai/clearml',\n",
    "    'Codalab': 'codalab/codalab-worksheets',\n",
    "    'DVC': 'iterative/dvc',\n",
    "    'Determined': 'determined-ai/determined',\n",
    "    'Domino': 'dominodatalab/python-domino',\n",
    "    'Guild AI': 'guildai/guildai',\n",
    "    'Kedro': 'kedro-org/kedro',\n",
    "    'MLflow': 'mlflow/mlflow',\n",
    "    'MLRun': 'mlrun/mlrun',\n",
    "    'ModelDB': 'VertaAI/modeldb',\n",
    "    'Neptune': 'neptune-ai/neptune-client',\n",
    "    'Optuna': 'optuna/optuna',\n",
    "    'Polyaxon': 'polyaxon/polyaxon',\n",
    "    'Sacred': 'IDSIA/sacred',\n",
    "    'Valohai': 'valohai/valohai-cli',\n",
    "    'Weights & Biases': 'wandb/wandb'\n",
    "}\n",
    "\n",
    "tools_release_date = {\n",
    "    'Amazon SageMaker': '2017-11-19',\n",
    "    'Azure Machine Learning': '2015-02-18',\n",
    "    'cnvrg.io': '2020-03-31',\n",
    "    'Comet': '2017-01-01',\n",
    "    'Iterative Studio': '2021-05-12',\n",
    "    'Polyaxon': '2018-10-16',\n",
    "    'SigOpt': '2014-11-01',\n",
    "    'Vertex AI': '2019-03-01'\n",
    "}\n",
    "\n",
    "tools_link = {\n",
    "    'cnvrg.io': 'https://github.com/cnvrg',\n",
    "    'Comet': 'https://github.com/comet-ml',\n",
    "    'Iterative Studio': 'https://studio.iterative.ai',\n",
    "    'SigOpt': 'https://github.com/sigopt',\n",
    "    'Vertex AI': 'https://cloud.google.com/vertex-ai'\n",
    "}\n",
    "\n",
    "tools_keywords = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['amazon sagemaker', 'aws sagemaker', 'sagemaker'],\n",
    "    'Azure Machine Learning': ['microsoft azure machine learning', 'azure machine learning', 'microsoft azure ml', 'microsoft azureml', 'azure ml', 'azureml'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['google vertex ai', 'vertex ai'],\n",
    "    'Weights & Biases': ['weights and biases', 'weights & biases', 'weights&biases', 'wandb', 'W & B', 'W&B']\n",
    "}\n",
    "\n",
    "issue_labels = {\n",
    "    'bug',\n",
    "    'error',\n",
    "    'invalid',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scrape.GHMiner import GitHubMiner\n",
    "from Scrape.GLMiner import GitLabMiner\n",
    "\n",
    "github_miner = GitHubMiner(private_token=os.getenv('GITHUB_TOKEN'))\n",
    "gitlab_miner = GitLabMiner(private_token=os.getenv('GITLAB_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_data = pd.DataFrame()\n",
    "\n",
    "# scrape open-source asset-management tools\n",
    "for tool_name, tool_repo in tools_repo.items():\n",
    "    if tool_name in tools_release_date:\n",
    "        tool_data = github_miner.scrape_repo(repo_name=tool_repo, real_name=tool_name, release_time=pd.to_datetime(tools_release_date[tool_name]))\n",
    "    else:\n",
    "        tool_data = github_miner.scrape_repo(repo_name=tool_repo, real_name=tool_name)\n",
    "\n",
    "    if not tool_data.empty:\n",
    "        tools_data = pd.concat([tools_data, tool_data], ignore_index=True)\n",
    "\n",
    "# add closed-source asset-management tools\n",
    "for tool_name in tools_link.keys():\n",
    "    tool_data = {\n",
    "        'Name': tool_name,\n",
    "        'Link': tools_link[tool_name],\n",
    "        'First Release Date': pd.to_datetime(tools_release_date[tool_name])\n",
    "    }\n",
    "    tool_data = pd.DataFrame([tool_data])\n",
    "    tools_data = pd.concat([tools_data, tool_data], ignore_index=True)\n",
    "\n",
    "tools_data.to_json(os.path.join(path_dataset, 'Tools.json'),\n",
    "                   indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dependents = pd.DataFrame()\n",
    "\n",
    "# collect dependents for tools with coding patterns\n",
    "for tool_name in tools_keywords.keys():\n",
    "    github_dependents = []\n",
    "    gitlab_dependents = []\n",
    "\n",
    "    # collect Github dependents\n",
    "    file_name = os.path.join(path_github_repo, tool_name + '.json')\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, encoding='utf8') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            # either search by sourcegraph\n",
    "            if 'Results' in json_data:\n",
    "                for repo_file in json_data['Results']:\n",
    "                    # file name match pattern\n",
    "                    if 'FileMatch' == repo_file['__typename'] and repo_file['repository']['name'].startswith('github'):\n",
    "                        repo_name = repo_file['repository']['name'].removeprefix(\n",
    "                            'github.com/')\n",
    "                        github_dependents.append(repo_name)\n",
    "                    # code usage match pattern\n",
    "                    elif 'Repository' == repo_file['__typename'] and repo_file['name'].startswith('github'):\n",
    "                        repo_name = repo_file['name'].removeprefix(\n",
    "                            'github.com/')\n",
    "                        github_dependents.append(repo_name)\n",
    "            # or search by dependent graph\n",
    "            elif 'all_public_dependent_repos' in json_data:\n",
    "                for repo_file in json_data['all_public_dependent_repos']:\n",
    "                    github_dependents.append(repo_file['name'])\n",
    "\n",
    "    # collect Gitlab dependents\n",
    "    file_name = os.path.join(path_gitlab_repo, tool_name + '.json')\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, encoding='utf8') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            # search by sourcegraph exclusively\n",
    "            for repo_file in json_data['Results']:\n",
    "                # file name match pattern\n",
    "                if 'FileMatch' == repo_file['__typename'] and repo_file['repository']['name'].startswith('gitlab'):\n",
    "                    repo_name = repo_file['repository']['name'].removeprefix(\n",
    "                        'gitlab.com/')\n",
    "                    gitlab_dependents.append(repo_name)\n",
    "                # code usage match pattern\n",
    "                elif 'Repository' == repo_file['__typename'] and repo_file['name'].startswith('gitlab'):\n",
    "                    repo_name = repo_file['name'].removeprefix('gitlab.com/')\n",
    "                    gitlab_dependents.append(repo_name)\n",
    "\n",
    "    # remove tool repo from dependents if any\n",
    "    if tool_name in tools_repo and tools_repo[tool_name] in github_dependents:\n",
    "        github_dependents.remove(tools_repo[tool_name])\n",
    "\n",
    "    # no need to add tools without dependents\n",
    "    if not len(github_dependents) and not len(gitlab_dependents):\n",
    "        continue\n",
    "\n",
    "    dependent = {\n",
    "        'Tool': tool_name,\n",
    "        'GitHub Dependents': github_dependents,\n",
    "        'GitLab Dependents': gitlab_dependents\n",
    "    }\n",
    "\n",
    "    dependents = pd.concat(\n",
    "        [dependents, pd.DataFrame([dependent])], ignore_index=True)\n",
    "\n",
    "dependents.to_json(os.path.join(\n",
    "    path_dataset, 'Dependents.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37786\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "github_repos = set()\n",
    "gitlab_repos = set()\n",
    "\n",
    "for index, row in dependents.iterrows():\n",
    "    github_repos = github_repos.union(row['GitHub Dependents'])\n",
    "    gitlab_repos = gitlab_repos.union(row['GitLab Dependents'])\n",
    "\n",
    "print(len(github_repos))\n",
    "print(len(gitlab_repos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#GitHub Dependents</th>\n",
       "      <th>#GitLab Dependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aim</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>1114</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>826</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ClearML</td>\n",
       "      <td>498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Codalab</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comet</td>\n",
       "      <td>678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Determined</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Domino</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DVC</td>\n",
       "      <td>6099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Guild AI</td>\n",
       "      <td>67</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kedro</td>\n",
       "      <td>1184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLflow</td>\n",
       "      <td>1575</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLRun</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ModelDB</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Neptune</td>\n",
       "      <td>423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Optuna</td>\n",
       "      <td>6370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Polyaxon</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sacred</td>\n",
       "      <td>1918</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SigOpt</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Valohai</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>19952</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tool #GitHub Dependents #GitLab Dependents\n",
       "0                      Aim                189                  1\n",
       "1         Amazon SageMaker               1114                  3\n",
       "2   Azure Machine Learning                826                  0\n",
       "3                  ClearML                498                  0\n",
       "4                  Codalab                 40                  0\n",
       "5                    Comet                678                  0\n",
       "6               Determined                 36                  0\n",
       "7                   Domino                  1                  0\n",
       "8                      DVC               6099                  0\n",
       "9                 Guild AI                 67                  4\n",
       "10                   Kedro               1184                  0\n",
       "11                  MLflow               1575                  4\n",
       "12                   MLRun                 27                  0\n",
       "13                 ModelDB                  9                  0\n",
       "14                 Neptune                423                  0\n",
       "15                  Optuna               6370                  0\n",
       "16                Polyaxon                 36                  0\n",
       "17                  Sacred               1918                  0\n",
       "18                  SigOpt                 96                  0\n",
       "19                 Valohai                 31                  0\n",
       "20               Vertex AI                135                  0\n",
       "21        Weights & Biases              19952                  0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependents_summary = pd.DataFrame(\n",
    "    columns=['Tool', '#GitHub Dependents', '#GitLab Dependents'])\n",
    "for index, row in dependents.iterrows():\n",
    "    dependent_data = {\n",
    "        'Tool': row['Tool'],\n",
    "        '#GitHub Dependents': len(row['GitHub Dependents']),\n",
    "        '#GitLab Dependents': len(row['GitLab Dependents'])\n",
    "    }\n",
    "    dependent_data = pd.DataFrame([dependent_data])\n",
    "    dependents_summary = pd.concat(\n",
    "        [dependents_summary, dependent_data], ignore_index=True)\n",
    "dependents_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dependents = pd.read_json(os.path.join(path_dataset, 'Dependents.json'))\n",
    "df_tool = pd.read_json(os.path.join(path_dataset, 'Tools.json'))\n",
    "tool_release_dates = pd.Series(pd.to_datetime(df_tool['First Release Date'].values), index=df_tool['Name']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aim\n",
      "lu-ci/sigma/apex-sigma\n",
      "Amazon SageMaker\n",
      "fluidattacks/universe\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m repo_data \u001b[39m=\u001b[39m gitlab_miner\u001b[39m.\u001b[39mscrape_repo(repo)\n\u001b[0;32m     10\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m repo_data\u001b[39m.\u001b[39mempty) \u001b[39mand\u001b[39;00m (repo_data[\u001b[39m'\u001b[39m\u001b[39mRepo Created Date\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m tool_release_dates[row[\u001b[39m'\u001b[39m\u001b[39mTool\u001b[39m\u001b[39m'\u001b[39m]]) \u001b[39mand\u001b[39;00m repo_data[\u001b[39m'\u001b[39m\u001b[39m#Issue\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m]:\n\u001b[1;32m---> 11\u001b[0m     issues \u001b[39m=\u001b[39m gitlab_miner\u001b[39m.\u001b[39;49mscrape_issue(repo)\n\u001b[0;32m     12\u001b[0m     issues[\u001b[39m'\u001b[39m\u001b[39mTool\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mTool\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m     issues_raw \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([issues_raw, issues], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\zhimi\\Downloads\\MSR-Asset-Management\\Code\\Scrape\\GLMiner.py:45\u001b[0m, in \u001b[0;36mGitLabMiner.scrape_issue\u001b[1;34m(self, repo_name)\u001b[0m\n\u001b[0;32m     42\u001b[0m issue_data[\u001b[39m'\u001b[39m\u001b[39mComment_body\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m pd\u001b[39m.\u001b[39mnotna(issue\u001b[39m.\u001b[39mclosed_at):\n\u001b[1;32m---> 45\u001b[0m     issue_data[\u001b[39m'\u001b[39m\u001b[39mIssue_self_closed\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m issue\u001b[39m.\u001b[39;49mattributes[\u001b[39m\"\u001b[39;49m\u001b[39mclosed_by\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39m issue\u001b[39m.\u001b[39mauthor[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     46\u001b[0m     comment_body \u001b[39m=\u001b[39m []\n\u001b[0;32m     47\u001b[0m     \u001b[39mfor\u001b[39;00m comment \u001b[39min\u001b[39;00m comments:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# scrape issues of Gitlab dependents for each tool\n",
    "\n",
    "issues_raw = pd.DataFrame()\n",
    "\n",
    "for index, row in df_dependents.iterrows():\n",
    "    print(row['Tool'])\n",
    "    for repo in row['GitLab Dependents']:\n",
    "        print(repo)\n",
    "        repo_data = gitlab_miner.scrape_repo(repo)\n",
    "        if (not repo_data.empty) and (repo_data['Repo Created Date'].values[0] > tool_release_dates[row['Tool']]) and repo_data['#Issue'].values[0]:\n",
    "            issues = gitlab_miner.scrape_issue(repo)\n",
    "            issues['Tool'] = row['Tool']\n",
    "            issues_raw = pd.concat([issues_raw, issues], ignore_index=True)\n",
    "            issues_raw.to_json(os.path.join(path_gitlab_issue, 'raw.json'), indent=4, orient='records')\n",
    "    \n",
    "issues_raw.to_json(os.path.join(path_gitlab_issue, 'raw.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape issues of Github dependents for each tool\n",
    "\n",
    "issues_raw = pd.DataFrame()\n",
    "\n",
    "for index, row in df_dependents.iterrows():\n",
    "    print(row['Tool'])\n",
    "    for repo in row['GitHub Dependents']:\n",
    "        print(repo)\n",
    "        repo_data = gitlab_miner.scrape_repo(repo)\n",
    "        if (not repo_data.empty) and (repo_data['Repo Created Date'].values[0] > tool_release_dates[row['Tool']]) and repo_data['#Issue'].values[0]:\n",
    "            issues = gitlab_miner.scrape_issue(repo)\n",
    "            issues['Tool'] = row['Tool']\n",
    "            issues_raw = pd.concat([issues_raw, issues], ignore_index=True)\n",
    "            issues_raw.to_json(os.path.join(path_github_issue, 'raw.json'), indent=4, orient='records')\n",
    "    \n",
    "issues_raw.to_json(os.path.join(path_github_issue, 'raw.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2719"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exclude issues that are not related to each tool\n",
    "\n",
    "issues_gitlab = pd.read_json(os.path.join(path_gitlab_issue, 'raw.json'))\n",
    "issues_github = pd.read_json(os.path.join(path_github_issue, 'raw.json'))\n",
    "\n",
    "for index, row in issues_gitlab.iterrows():\n",
    "    for keyword in tools_keywords[row['Tool']]:\n",
    "        if keyword in row['Issue_title'].lower():\n",
    "            break\n",
    "    else:\n",
    "        issues_gitlab.drop(index, inplace=True)\n",
    "\n",
    "for index, row in issues_github.iterrows():\n",
    "    for keyword in tools_keywords[row['Tool']]:\n",
    "        if keyword in row['Issue_title'].lower():\n",
    "            break\n",
    "    else:\n",
    "        issues_github.drop(index, inplace=True)\n",
    "\n",
    "issues_github['Platform'] = 'Github'\n",
    "issues_gitlab['Platform'] = 'Gitlab'\n",
    "\n",
    "issues_raw = pd.concat([issues_gitlab, issues_github], ignore_index=True)        \n",
    "len(issues_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"experiments\"',\n",
       " '0.4.6',\n",
       " '1.1',\n",
       " '1.4',\n",
       " '1.6',\n",
       " '1.7',\n",
       " '2.0',\n",
       " '3 - Quality of Life',\n",
       " '3rd party',\n",
       " '3rd party update',\n",
       " ':bridge_at_night:  Bridge',\n",
       " ':bug: bug',\n",
       " ':rotating_light:',\n",
       " '? - Needs Triage',\n",
       " 'A: example-dvc-experiments',\n",
       " 'A: example-get-started',\n",
       " 'ADO',\n",
       " 'AI\\u202fFrameworks/ONNX',\n",
       " 'AML Compute Instance',\n",
       " 'API',\n",
       " 'API & Doc',\n",
       " 'Auto\\u202fML',\n",
       " 'BF',\n",
       " 'Cloud',\n",
       " 'Community',\n",
       " 'Compute',\n",
       " 'Core UI',\n",
       " 'DRL',\n",
       " 'Data Labeling',\n",
       " 'Data4ML',\n",
       " 'Data\\u202fDrift',\n",
       " 'Data\\u202fPrep\\u202fServices',\n",
       " 'Documentation',\n",
       " 'ERRATA_CANDIDATE',\n",
       " 'Enhancement',\n",
       " 'Environments',\n",
       " 'Evaluation',\n",
       " 'Experimentation UI',\n",
       " 'FAQ',\n",
       " 'Feature - Medium Priority',\n",
       " 'HIGH',\n",
       " 'HPO',\n",
       " 'Hyperdrive',\n",
       " 'Important',\n",
       " 'In the roadmap',\n",
       " 'Inf1',\n",
       " 'Inference',\n",
       " 'Ingestion',\n",
       " 'Issue: Bug Report 🐞',\n",
       " 'Issue: Feature Request',\n",
       " 'L',\n",
       " 'LOE: S',\n",
       " 'Localized',\n",
       " 'MLOps',\n",
       " 'NLP',\n",
       " 'NUM',\n",
       " 'Needs Triage',\n",
       " 'Not related to PyCaret',\n",
       " 'Notebook',\n",
       " 'Optional',\n",
       " 'P0',\n",
       " 'P1',\n",
       " 'P2',\n",
       " 'Pipelines',\n",
       " 'Priority 1',\n",
       " 'Reinforcement Learning',\n",
       " 'RepoOfficiel',\n",
       " 'Review One',\n",
       " 'Review Two',\n",
       " 'SDK',\n",
       " 'Stage: Technical Design 🎨',\n",
       " 'Stale',\n",
       " 'TA',\n",
       " 'TODO',\n",
       " 'TODO before 1.0',\n",
       " 'Training',\n",
       " 'Training Service',\n",
       " 'Trn1',\n",
       " 'Usage',\n",
       " 'VISION',\n",
       " 'WIP',\n",
       " 'WIP - Susankha',\n",
       " 'Workspace Management',\n",
       " '[module] pipeline',\n",
       " 'accelerator: tpu',\n",
       " 'accessibility',\n",
       " 'ai',\n",
       " 'aiplatform',\n",
       " 'air',\n",
       " 'alonet',\n",
       " 'api: aiplatform',\n",
       " 'api: vertex-ai',\n",
       " 'app-ui',\n",
       " 'apply',\n",
       " 'arc_ml',\n",
       " 'architecture',\n",
       " 'area / SDK-storage',\n",
       " 'area / integrations',\n",
       " 'area-getting-started',\n",
       " 'area-ml-resource-management',\n",
       " 'area-remote-desktop',\n",
       " 'area-remote-web',\n",
       " 'area-sign-in',\n",
       " 'area-telemetry',\n",
       " 'area-treeview',\n",
       " 'area-yaml',\n",
       " 'area/async',\n",
       " 'area/components',\n",
       " 'area/components/aws/sagemaker',\n",
       " 'area/operator',\n",
       " 'area/registry',\n",
       " 'area/samples',\n",
       " 'area:databuilder',\n",
       " 'arena::security',\n",
       " 'assigned',\n",
       " 'assigned-to-author',\n",
       " 'audience/technical',\n",
       " 'august-rewrite',\n",
       " 'automations',\n",
       " 'automl',\n",
       " 'awaiting response',\n",
       " 'awaiting-product-team-response',\n",
       " 'aws',\n",
       " 'azure',\n",
       " 'azure-provider',\n",
       " 'azureml',\n",
       " 'backlog',\n",
       " 'benchmark',\n",
       " 'bittensor',\n",
       " 'blocked',\n",
       " 'blocker',\n",
       " 'breakdown',\n",
       " 'breaking',\n",
       " 'bug',\n",
       " 'bug-bash',\n",
       " 'build',\n",
       " 'chapter: appendix-tools',\n",
       " 'checkpointing',\n",
       " 'chore',\n",
       " 'cleanup',\n",
       " 'closing-soon-if-no-response',\n",
       " 'code',\n",
       " 'concerns: agents',\n",
       " 'concerns: documentation',\n",
       " 'concerns: main API',\n",
       " 'configs',\n",
       " 'connectors',\n",
       " 'contribution welcomed',\n",
       " 'core/subsvc',\n",
       " 'customer-inquiry',\n",
       " 'customer-issue',\n",
       " 'cxp',\n",
       " 'data',\n",
       " 'data-sync',\n",
       " 'debt',\n",
       " 'dependencies',\n",
       " 'deploy',\n",
       " 'design',\n",
       " 'dev',\n",
       " 'dev workflow',\n",
       " 'devflows',\n",
       " 'discussion',\n",
       " 'diy_container',\n",
       " 'doc-bug',\n",
       " 'doc-enhancement',\n",
       " 'docker images',\n",
       " 'docs',\n",
       " 'documentation',\n",
       " 'duplicate',\n",
       " 'enhancement',\n",
       " 'enhancement request',\n",
       " 'env: new',\n",
       " 'env: sagemaker',\n",
       " 'environment',\n",
       " 'environment: slurm',\n",
       " 'ep:CUDA',\n",
       " 'epic',\n",
       " 'error',\n",
       " 'evaluate_model',\n",
       " 'example issue',\n",
       " 'example request',\n",
       " 'experimental',\n",
       " 'explore',\n",
       " 'feature',\n",
       " 'feature request',\n",
       " 'feature-request',\n",
       " 'fixme',\n",
       " 'forum',\n",
       " 'future release',\n",
       " 'good first issue',\n",
       " 'graphistry',\n",
       " 'guides',\n",
       " 'help wanted',\n",
       " 'hi-ml-azure',\n",
       " 'high-priority',\n",
       " 'improvement',\n",
       " 'improvements',\n",
       " 'in progress',\n",
       " 'inference',\n",
       " 'inferencing-benchmark',\n",
       " 'info-needed',\n",
       " 'infrastructure',\n",
       " 'integration',\n",
       " 'invalid',\n",
       " 'investigating',\n",
       " 'investigation',\n",
       " 'iteration-candidate',\n",
       " 'journey:intermediate',\n",
       " 'keep fresh',\n",
       " 'kind/bug',\n",
       " 'kind/enhancement',\n",
       " 'kind/feature',\n",
       " 'kind/question',\n",
       " 'kind/reproducibility',\n",
       " 'kind/usability',\n",
       " 'kubeflow',\n",
       " 'learn',\n",
       " 'lifecycle/frozen',\n",
       " 'lifecycle/stale',\n",
       " 'lightning',\n",
       " 'linting / formatting / cleaning',\n",
       " 'logger',\n",
       " 'logger: comet',\n",
       " 'logger: mlflow',\n",
       " 'logger: wandb',\n",
       " 'logging',\n",
       " 'looking into it',\n",
       " 'low priority',\n",
       " 'machine-learning',\n",
       " 'machine-learning/svc',\n",
       " 'major',\n",
       " 'metrics',\n",
       " 'missing_info',\n",
       " 'ml',\n",
       " 'ml-engineering',\n",
       " 'mlflow',\n",
       " 'model',\n",
       " 'module: text',\n",
       " 'must have',\n",
       " 'need-design-decision',\n",
       " 'needs triage',\n",
       " 'needs-more-info',\n",
       " 'needs-tests',\n",
       " 'needs-triage',\n",
       " 'neptune',\n",
       " 'new',\n",
       " 'new feature',\n",
       " 'new table',\n",
       " 'nnidev',\n",
       " 'no-issue-activity',\n",
       " 'normal',\n",
       " 'notebook',\n",
       " 'on hold',\n",
       " 'open for contribution',\n",
       " 'operationalization',\n",
       " 'optimization',\n",
       " 'organizational',\n",
       " 'p0-critical',\n",
       " 'p1',\n",
       " 'p1-important',\n",
       " 'p2-medium',\n",
       " 'p3-nice-to-have',\n",
       " 'phase / shipped',\n",
       " 'pipeline',\n",
       " 'pipeline 6: infer',\n",
       " 'pl',\n",
       " 'platform/aws',\n",
       " 'platform/other',\n",
       " 'plot_model',\n",
       " 'practice',\n",
       " 'pri/medium',\n",
       " 'priority 3 - nice to have',\n",
       " 'priority-p0',\n",
       " 'priority-p1',\n",
       " 'priority/important-longterm',\n",
       " 'priority/p1',\n",
       " 'priority: 1',\n",
       " 'priority: 2',\n",
       " 'priority: high',\n",
       " 'priority: medium',\n",
       " 'priority: p2',\n",
       " 'priority:high',\n",
       " 'priority:medium',\n",
       " 'priority_high',\n",
       " 'product-feedback',\n",
       " 'product-gap',\n",
       " 'product-issue',\n",
       " 'product-question',\n",
       " 'product::sorts',\n",
       " 'progress bar: rich',\n",
       " 'python',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quick-fix',\n",
       " 'refactor',\n",
       " 'refactoring',\n",
       " 'reporting and diagnostics',\n",
       " 'research',\n",
       " 'roadmap',\n",
       " 'sagemaker',\n",
       " 'sagemaker-dsk-v2',\n",
       " 'sagemaker_container',\n",
       " 'samples',\n",
       " 'scenario',\n",
       " 'sdk-docs',\n",
       " 'section',\n",
       " 'service update',\n",
       " 'service:executor',\n",
       " 'service:sm-executor',\n",
       " 'setup',\n",
       " 'snippets-request',\n",
       " 'spark',\n",
       " 'stale',\n",
       " 'stale :zzz:',\n",
       " 'status/triaged',\n",
       " 'status: phase 1',\n",
       " 'status: phase 2',\n",
       " 'status:completed',\n",
       " 'status:needs_reproducing',\n",
       " 'status:needs_votes',\n",
       " 'streaming',\n",
       " 'studio',\n",
       " 'support',\n",
       " 't-must-fix',\n",
       " 't-nice-to-have-fix',\n",
       " 't-unknown-sub-error',\n",
       " 'task',\n",
       " 'technical debt',\n",
       " 'test',\n",
       " 'tests',\n",
       " 'time_series',\n",
       " 'timecodes',\n",
       " 'to refine',\n",
       " 'todo',\n",
       " 'tooling and CI',\n",
       " 'topic:dependencies',\n",
       " 'topic:eval',\n",
       " 'topic:models',\n",
       " 'topic:reader',\n",
       " 'training-benchmark',\n",
       " 'triage',\n",
       " 'triage me',\n",
       " 'triage-needed',\n",
       " 'triage/intermediate-priotrity',\n",
       " 'triaged',\n",
       " 'tune',\n",
       " 'type / bug',\n",
       " 'type / code-health',\n",
       " 'type / enhancement',\n",
       " 'type/maintenance',\n",
       " 'type: bug',\n",
       " 'type: enhancement',\n",
       " 'type: feature request',\n",
       " 'type: question',\n",
       " 'type::bug',\n",
       " 'type:bug',\n",
       " 'type:feature',\n",
       " 'type:maintenance',\n",
       " 'type:question',\n",
       " 'up-for-grabs',\n",
       " 'upstream-azml',\n",
       " 'urgent',\n",
       " 'user raised',\n",
       " 'ux',\n",
       " 'v0.8.5',\n",
       " 'waiting',\n",
       " 'waiting feedback',\n",
       " \"won't fix\",\n",
       " 'wontfix',\n",
       " 'work-item',\n",
       " 'workflow',\n",
       " 'working as intended',\n",
       " '✨ feat',\n",
       " '민지',\n",
       " '찬국',\n",
       " '🐛 bug fix',\n",
       " '🐞 bug',\n",
       " '🐥 experiment',\n",
       " '🐺 Tracker',\n",
       " '👨\\u200d👩\\u200d👧\\u200d👧 discussion',\n",
       " '💎 New Component',\n",
       " '📜 Paper',\n",
       " '🔤 named-entity-recognition',\n",
       " '🔥 New Feature',\n",
       " '🛂 checkpoint',\n",
       " '🦉 dvc',\n",
       " '🧪 testing'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = set()\n",
    "for _, row in issues_raw['Issue_label'].map(set).items():\n",
    "    final = final.union(row)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "看一下issue_label需不需要新增！？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1657"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out issues that are not related to challenges\n",
    "\n",
    "for index, row in issues_raw.iterrows():\n",
    "    if not row['Issue_title'].isascii():\n",
    "        issues_raw.drop(index, inplace=True)\n",
    "        continue\n",
    "    \n",
    "    if not row['Issue_label']:\n",
    "        continue\n",
    "    else:\n",
    "        for label in row['Issue_label']:\n",
    "            for issue_label in issue_labels:\n",
    "                if issue_label in label.lower():\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            issues_raw.drop(index, inplace=True)\n",
    "\n",
    "del issues_raw['Issue_label']\n",
    "\n",
    "issues_raw.to_json(os.path.join(path_labeling, 'original.json'), indent=4, orient='records')\n",
    "len(issues_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add potential field to issues for later filling\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(path_labeling, 'original.json'))\n",
    "\n",
    "df_issues['Issue_original_content'] = np.nan\n",
    "df_issues['Issue_preprocessed_content'] = np.nan\n",
    "df_issues['Issue_gpt_summary_original'] = np.nan\n",
    "df_issues['Issue_gpt_summary'] = np.nan\n",
    "\n",
    "df_issues['Issue_score_count'] = df_issues['Issue_upvote_count'] - df_issues['Issue_downvote_count']\n",
    "df_issues['Comment_score_count'] = df_issues['Comment_upvote_count'] - df_issues['Comment_downvote_count']\n",
    "df_issues['Issue_body'] = df_issues['Issue_body'].fillna('')\n",
    "\n",
    "del df_issues['Issue_upvote_count']\n",
    "del df_issues['Issue_downvote_count']\n",
    "del df_issues['Comment_upvote_count']\n",
    "del df_issues['Comment_downvote_count']\n",
    "\n",
    "df_issues = df_issues.reindex(sorted(df_issues.columns), axis=1)\n",
    "df_issues.to_json(os.path.join(path_labeling, 'issues.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_old = pd.read_json(os.path.join(path_labeling, 'issues.json'))\n",
    "df_issues = pd.read_json(os.path.join(path_labeling, 'issues+.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    for i2, r2 in df_old.iterrows():\n",
    "        if row['Issue_link'] == r2['Issue_link']:\n",
    "            df_issues.at[index, 'Issue_original_content'] = r2['Issue_original_content']\n",
    "            df_issues.at[index, 'Issue_preprocessed_content'] = r2['Issue_preprocessed_content']\n",
    "            df_issues.at[index, 'Issue_gpt_summary_original'] = r2['Issue_gpt_summary_original']\n",
    "            df_issues.at[index, 'Issue_gpt_summary'] = r2['Issue_gpt_summary']\n",
    "            break\n",
    "\n",
    "df_issues.to_json(os.path.join(path_labeling, 'issues.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content preprocessing patterns\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import re\n",
    "    \n",
    "regex = r'''([a-z]*([a-z])\\2[a-z]*)|(<.*?>)|({.*?})|((!)?\\[.*?\\])|(\\(.*?\\))|(\\`{3}.+?\\`{3})|(\\`{2}.+?\\`{2})|(\\`{1}.+?\\`{1})|([^\\s]*[<=>]=[^\\s]+)|(@[^\\s]+)|([^\\s]*\\\\[^\\s]+)|([^\\s]+\\/[^\\s]+)|([^\\s]+\\.[^\\s]+)|([^\\s]+-[^\\s]+)|([^\\s]+_[^\\s]+)|(_+[^\\s]+_*)|(_*[^\\s]+_+)|([0-9\\|\\-\\r\\n\\t\\\"\\-#*=~:{}\\(\\)\\[\\]<>]+)'''\n",
    "\n",
    "def preprocess_text(text, remove_code=False):\n",
    "    text = text.lower().encode('ascii', errors='ignore').decode('ascii')\n",
    "\n",
    "    for tool_keywords in tools_keywords.values():\n",
    "        for tool_keyword in tool_keywords:\n",
    "            if tool_keyword in text:\n",
    "                text = text.replace(tool_keyword, '')\n",
    "\n",
    "    text = re.sub(regex, ' ', text, 0, re.DOTALL) if remove_code else text\n",
    "            \n",
    "    text = preprocess_string(text)\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support intel optim librari \n",
      "bert requesthandl responsehandl insid endpoint \n",
      "mnmg xgboost \n",
      "deploi \n",
      "refactor \n",
      "investig remov trialcatalog \n",
      "endpoint invoc return bodi \n",
      "remot uri \n",
      "rearchitect remot extens \n",
      "investig extens work vscode dev \n",
      "log cli \n",
      "telemetri cli \n",
      "test cli servic \n",
      "actual resourc class \n",
      "servic class entri point sdk \n",
      "rest servic resourc \n",
      "http servic interact rest servic \n",
      "templat servic manag yaml templat \n",
      "select servic resourc \n",
      "cli \n",
      "add sourc label yaml diagnost \n",
      "updat yaml accordingli \n",
      "chang load label extens load \n",
      "insid program extens \n",
      "chang comput label aml \n",
      "pariti new portal \n",
      "feedback command work linux \n",
      "run creat lot warn cloudpickl \n",
      "datastor configur present hardcod inneryedataset\n",
      "accuraci show set code studio \n",
      "investig experi track \n",
      "setup server \n",
      "integr \n",
      "startup expect ostyp \n",
      "import import solv problem thank\n",
      "combin param param work \n",
      "remov improv interfac \n",
      "node param write test\n",
      "us pytest virtualenv instal \n",
      "run ceph \n",
      "entri ensembl protein transform output \n",
      "implement repro \n",
      "sourc init \n",
      "docker \n",
      "config merg \n",
      "implement \n",
      "preprocess video put \n",
      "cmf precheck remot git \n",
      "exp run slow window \n",
      "pull fail remot \n",
      "pull fail remot \n",
      "pull fail remot \n",
      "python apt init file\n",
      "updat pipelin \n",
      " track dataset\n",
      "check \n",
      "remov faic exec send directli \n",
      "add statu executepi \n",
      "setup \n",
      "function data version setup \n",
      "set \n",
      "setup \n",
      "setup \n",
      " \n",
      "set experi \n",
      "add exampl us dep \n",
      "final pipelin \n",
      "setup \n",
      "work \n",
      "split data train hold track \n",
      "setup \n",
      "work \n",
      "set upload dataset \n",
      "set \n",
      "setup privat unshar data \n",
      "setup \n",
      "setup \n",
      "integr model \n",
      "utal notebook \n",
      "convert project \n",
      "renam repo refer stream \n",
      "error load \n",
      "enabl model type follow\n",
      "sdktechno add add\n",
      "sdktechno add technolog \n",
      "sdktechno \n",
      "add support model convers \n",
      "reus vectormultimaputil packag \n",
      "test backend experi test backend\n",
      "authent creat track token authhandl\n",
      "imag quai \n",
      "standard output \n",
      "adapt container sdk us \n",
      "updat \n",
      "catch except transform \n",
      "refactor ocallback callback \n",
      "docker set \n",
      "refactor clearbox wrapper univers wrapper \n",
      "creat contain azur storag \n",
      "deploy fail \n",
      "import param train client install\n",
      " \n",
      "hyper paramet tune \n",
      "add hyperparamet optim \n",
      "experi method work run \n",
      "attribut data model avail \n",
      "run tutori \n",
      "add wrapper tensorflow model \n",
      "undocu depend tqdm necessari sampl\n",
      "nba overund model import client \n",
      "set default zigzagcai\n",
      "refactor us notebook colab \n",
      "implement log block\n",
      "cmd train loss differ \n",
      "log val loss \n",
      "duplic session creat \n",
      "avalanch add log avalanch method \n",
      "add debug clean log \n",
      "add experi track \n",
      "logger \n",
      "add integr \n",
      "combin config pytorch config \n",
      "case worker chang command \n",
      "updat dep hydra launcher \n",
      "add offlin log \n",
      "map step epoch log \n",
      "high log precis recal \n",
      "save model file \n",
      "open browser \n",
      "work multiprocess enabl process\n",
      "visual bound box callback \n",
      "log imag \n",
      "setup \n",
      " \n",
      "creat mock project test purpos \n",
      "add leran rate \n",
      "configur weightsandbias\n",
      "log config \n",
      "add log plot \n",
      "fix import issu \n",
      "add tensorboard \n",
      "integr \n",
      "creat project \n",
      " \n",
      "creat submiss exampl polish leaderboard \n",
      "version version\n",
      "logger \n",
      "sweep compat pytorch lightn \n",
      "save checkpoint dir name runid \n",
      "logger \n",
      "turn model watch callback \n",
      "add callback code save \n",
      "setup track metric experi \n",
      "implement log text data \n",
      "issu settl\n",
      "experi config regist \n",
      "us log \n"
     ]
    }
   ],
   "source": [
    "# Experiment 1\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    issue = preprocess_text(row['Issue_title']) + ' ' + preprocess_text(str(row['Issue_body']))\n",
    "    if len(issue.split()) < 6:\n",
    "        df_issues.drop(index, inplace=True)\n",
    "        print(issue)\n",
    "    else:\n",
    "        df_issues.at[index, 'Issue_original_content'] = issue\n",
    "\n",
    "df_issues.to_json(os.path.join(path_labeling,\n",
    "                  'issues.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Issue_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Github</td>\n",
       "      <td>1497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gitlab</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Platform  Issue_title\n",
       "0   Github         1497\n",
       "1   Gitlab            3"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_issues = pd.read_json(os.path.join(path_labeling, 'issues.json'))\n",
    "df_issues.groupby('Platform').count()['Issue_title'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>Issue_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClearML</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comet</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DVC</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Determined</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kedro</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLflow</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Neptune</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Optuna</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sacred</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SigOpt</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tool  Issue_title\n",
       "0         Amazon SageMaker          334\n",
       "1   Azure Machine Learning          299\n",
       "2                  ClearML           17\n",
       "3                    Comet           39\n",
       "4                      DVC          128\n",
       "5               Determined            1\n",
       "6                    Kedro           98\n",
       "7                   MLflow          309\n",
       "8                  Neptune           37\n",
       "9                   Optuna            4\n",
       "10                  Sacred           36\n",
       "11                  SigOpt            8\n",
       "12               Vertex AI           17\n",
       "13        Weights & Biases          173"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_issues = pd.read_json(os.path.join(path_labeling, 'issues.json'))\n",
    "df_issues.groupby('Tool').count()['Issue_title'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt for gpt model\n",
    "\n",
    "import random\n",
    "\n",
    "prompt_issue = 'Your task is to provide a brief and accurate summary of the issue post. Your summary should be concise, highlighting only the most important aspects regarding the challenges faced by the user.\\n###'\n",
    "# prompt_fix = 'Given a challenge-discussion pair, please extract any possible solutions mentioned in the discussion and provide a brief summary of them. If no solution is mentioned, please indicate that there are no solutions provided.\\n###'\n",
    "\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0, 1)\n",
    "            time.sleep(sleep)\n",
    "            x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persisting on question 49\n",
      "persisting on question 99\n",
      "persisting on question 149\n",
      "persisting on question 199\n",
      "persisting on question 249\n",
      "persisting on question 299\n",
      "persisting on question 349\n",
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 42302 tokens. Please reduce the length of the messages. on issue https://github.com/Azure/MachineLearningNotebooks/issues/1668\n",
      "persisting on question 399\n",
      "persisting on question 449\n",
      "persisting on question 499\n",
      "persisting on question 549\n",
      "persisting on question 599\n",
      "persisting on question 649\n",
      "persisting on question 699\n",
      "persisting on question 749\n",
      "persisting on question 799\n",
      "persisting on question 849\n",
      "persisting on question 899\n",
      "persisting on question 949\n",
      "persisting on question 999\n",
      "persisting on question 1049\n",
      "persisting on question 1099\n",
      "persisting on question 1149\n",
      "persisting on question 1199\n",
      "persisting on question 1249\n",
      "persisting on question 1299\n",
      "persisting on question 1349\n",
      "persisting on question 1399\n",
      "persisting on question 1449\n",
      "persisting on question 1499\n",
      "persisting on question 1549\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    if index % 50 == 49:\n",
    "        print(f'persisting on question {index}')\n",
    "        df_issues.to_json(os.path.join(\n",
    "            path_labeling, 'issues.json'), indent=4, orient='records')\n",
    "\n",
    "    if pd.notna(row['Issue_gpt_summary_original']):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prompt = prompt_issue + 'Title: ' + row['Issue_title'] + ' Body: ' + row['Issue_body'] + '###\\n'\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-3.5-turbo-16k',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=150,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=50,\n",
    "            stream=False\n",
    "        )\n",
    "        df_issues.at[index, 'Issue_gpt_summary_original'] = response['choices'][0]['message']['content']\n",
    "        \n",
    "    except Exception as e:\n",
    "        # output unsuccesful requests\n",
    "        print(f'{e} on issue {row[\"Issue_link\"]}')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df_issues.to_json(os.path.join(\n",
    "    path_labeling, 'issues.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_issues.shape[0] == df_issues[df_issues['Issue_gpt_summary_original'].str.len() > 0].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    df_issues.at[index, 'Issue_gpt_summary'] = preprocess_text(row['Issue_gpt_summary_original'])\n",
    "    \n",
    "df_issues.to_json(os.path.join(\n",
    "    path_labeling, 'issues.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    try:\n",
    "        content = preprocess_text(row['Issue_title'], remove_code=True) + ' ' + preprocess_text(str(row['Issue_body']), remove_code=True)\n",
    "        df_issues.at[index, 'Issue_preprocessed_content'] = content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "df_issues.to_json(os.path.join(\n",
    "    path_labeling, 'issues.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Experiment 4\n",
    "\n",
    "# df_issues = pd.read_json(os.path.join(\n",
    "#     path_labeling, 'issues.json'))\n",
    "\n",
    "# for index, row in df_issues.iterrows():\n",
    "#     if pd.notna(row['Issue_closed_time']) and row['Comment_body']:\n",
    "#         df_issues.at[index, 'Comment_original_content'] = preprocess_text(row['Comment_body'])\n",
    "\n",
    "# df_issues.to_json(os.path.join(path_labeling,\n",
    "#                   'issues.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persisting on question 49\n",
      "persisting on question 99\n",
      "persisting on question 149\n",
      "persisting on question 199\n",
      "persisting on question 249\n",
      "persisting on question 299\n",
      "persisting on question 349\n",
      "persisting on question 399\n"
     ]
    }
   ],
   "source": [
    "# # Experiment 5\n",
    "\n",
    "# df_issues = pd.read_json(os.path.join(\n",
    "#     path_labeling, 'issues.json'))\n",
    "\n",
    "# for index, row in df_issues.iterrows():\n",
    "#     if index % 50 == 49:\n",
    "#         print(f'persisting on question {index}')\n",
    "#         df_issues.to_json(os.path.join(\n",
    "#             path_labeling, 'issues.json'), indent=4, orient='records')\n",
    "\n",
    "#     if pd.isna(row['Issue_closed_time']) or not row['Comment_body'] or pd.notna(row['Comment_gpt_summary_original']):\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         prompt = prompt_fix + 'Challenge: ' + row['Issue_gpt_summary_original'] + ' Discussion: ' + row['Comment_body'] + '###\\n'\n",
    "#         response = retry_with_backoff(\n",
    "#             openai.ChatCompletion.create,\n",
    "#             model='gpt-4-32k',\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"You are an accurate summarizer.\"},\n",
    "#                 {\"role\": \"user\", \"content\": prompt},\n",
    "#             ],\n",
    "#             temperature=0,\n",
    "#             max_tokens=150,\n",
    "#             top_p=1,\n",
    "#             frequency_penalty=0,\n",
    "#             presence_penalty=0,\n",
    "#             timeout=10,\n",
    "#             stream=False\n",
    "#         )\n",
    "#         content = response['choices'][0]['message']['content'].strip()\n",
    "#         df_issues.at[index, 'Comment_gpt_summary_original'] = content\n",
    "#         df_issues.at[index, 'Comment_gpt_summary'] = preprocess_text(content)\n",
    "#     except Exception as e:\n",
    "#         # output unsuccesful requests\n",
    "#         print(f'{e} on issue {row[\"Issue_link\"]}')\n",
    "\n",
    "#     time.sleep(1)\n",
    "\n",
    "# df_issues.to_json(os.path.join(\n",
    "#     path_labeling, 'issues.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Experiment 6\n",
    "\n",
    "# df_issues = pd.read_json(os.path.join(\n",
    "#     path_labeling, 'issues.json'))\n",
    "\n",
    "# for index, row in df_issues.iterrows():\n",
    "#     if pd.notna(row['Issue_closed_time']) and row['Comment_body']:\n",
    "#         df_issues.at[index, 'Comment_preprocessed_content'] = preprocess_text(row['Comment_body'], remove_code=True)\n",
    "\n",
    "# df_issues.to_json(os.path.join(\n",
    "#     path_labeling, 'issues.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size is based on the recommendation from https://www.calculator.net/sample-size-calculator.html\n",
    "\n",
    "sample_size = 306\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "df_sample = df_issues[df_issues['Issue_closed_time'].notna()].sample(n=sample_size, random_state=42)\n",
    "\n",
    "df_sample.to_json(os.path.join(\n",
    "    path_labeling, 'sample.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from github import Github\n",
    "# import time\n",
    "\n",
    "# token = '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'\n",
    "# g = Github(login_or_token=token)\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_labeling, 'original.json'))\n",
    "\n",
    "# repo_name_last = ''\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if row['Platform'] == 'Github':\n",
    "#         try:\n",
    "#             link = row['Issue_link']\n",
    "#             components = link.split('/')\n",
    "#             repo_name = f'{components[3]}/{components[4]}'\n",
    "            \n",
    "#             if repo_name != repo_name_last:\n",
    "#                 repo = g.get_repo(repo_name)\n",
    "#                 n_contributors = repo.get_contributors().totalCount\n",
    "#                 issues = repo.get_issues(state='all')\n",
    "                \n",
    "#             issue_number = int(components[-1])\n",
    "#             issue = repo.get_issue(number=issue_number)\n",
    "            \n",
    "#             df.at[index, 'Issue_title'] = issue.title\n",
    "#             df.at[index, 'Issue_label'] = [label.name for label in issue.labels]\n",
    "#             df.at[index, 'Issue_created_time'] = issue.created_at\n",
    "#             df.at[index, 'Issue_closed_time'] = issue.closed_at\n",
    "#             reactions = issue.get_reactions()\n",
    "#             df.at[index, 'Issue_upvote_count'] = sum(reaction.content == '+1' for reaction in reactions)\n",
    "#             df.at[index, 'Issue_downvote_count'] = sum(reaction.content == '-1' for reaction in reactions)\n",
    "#             df.at[index, 'Issue_body'] = issue.body\n",
    "#             df.at[index, 'Issue_comment_count'] = issue.comments\n",
    "#             df.at[index, 'Issue_repo_issue_count'] = issues.totalCount\n",
    "#             df.at[index, 'Issue_repo_watch_count'] = repo.subscribers_count\n",
    "#             df.at[index, 'Issue_repo_star_count'] = repo.stargazers_count\n",
    "#             df.at[index, 'Issue_repo_fork_count'] = repo.forks\n",
    "#             df.at[index, 'Issue_repo_contributor_count'] = n_contributors\n",
    "#             df.at[index, 'Issue_self_closed'] = np.nan\n",
    "            \n",
    "#             if(pd.notna(issue.closed_at)):\n",
    "#                 df.at[index, 'Issue_self_closed'] = issue.closed_by.id == issue.user.id\n",
    "#                 comments = []\n",
    "#                 upvotes = []\n",
    "#                 downvotes = []\n",
    "#                 for comment in issue.get_comments():\n",
    "#                     comments.append(comment.body)\n",
    "#                     reactions = comment.get_reactions()\n",
    "#                     upvote = sum(reaction.content == '+1' for reaction in reactions)\n",
    "#                     downvote = sum(reaction.content == '-1' for reaction in reactions)\n",
    "#                     upvotes.append(upvote)\n",
    "#                     downvotes.append(downvote)\n",
    "#                 df.at[index, 'Comment_body'] = ' '.join(comments)\n",
    "#                 df.at[index, 'Comment_upvote_count'] = sum(upvotes)\n",
    "#                 df.at[index, 'Comment_downvote_count'] = sum(downvotes)\n",
    "                \n",
    "#             repo_name_last = repo_name\n",
    "                \n",
    "#             # time.sleep(5)\n",
    "#         except Exception as e:\n",
    "#             print(f'{e} on issue {row[\"Issue_link\"]}')\n",
    "\n",
    "# df.to_json(os.path.join(path_labeling, 'original.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gitlab import Gitlab\n",
    "\n",
    "# token = 'glpat-SvwyWD6pbPNvbsBSvxdy'\n",
    "# g = Gitlab(private_token=token)\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_labeling, 'issues.json'))\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if row['Platform'] == 'Gitlab' and pd.notna(row['Issue_closed_time']):\n",
    "#         link = row['Issue_link']\n",
    "#         components = link.split('/')\n",
    "#         repo = g.projects.get(id=f'{components[3]}/{components[4]}')\n",
    "#         issue_number = int(components[-1])\n",
    "#         issue = repo.issues.get(issue_number)\n",
    "#         Issue_self_closed = issue.__getattr__(\"closed_by\")[\"id\"] == issue.author[\"id\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

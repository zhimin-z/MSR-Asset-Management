{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_dataset = '../Dataset'\n",
    "\n",
    "path_github = os.path.join(path_dataset, 'GitHub')\n",
    "path_gitlab = os.path.join(path_dataset, 'GitLab')\n",
    "path_labeling = os.path.join(path_dataset, 'Labeling')\n",
    "\n",
    "path_github_repo = os.path.join(path_github, 'Repo')\n",
    "path_gitlab_repo = os.path.join(path_gitlab, 'Repo')\n",
    "path_github_repo_raw = os.path.join(path_github_repo, 'Raw')\n",
    "path_gitlab_repo_raw = os.path.join(path_gitlab_repo, 'Raw')\n",
    "path_github_repo_scraped = os.path.join(path_github_repo, 'Scraped')\n",
    "path_gitlab_repo_scraped = os.path.join(path_gitlab_repo, 'Scraped')\n",
    "\n",
    "path_github_issue = os.path.join(path_github, 'Issue')\n",
    "path_gitlab_issue = os.path.join(path_gitlab, 'Issue')\n",
    "path_github_issue_raw = os.path.join(path_github_issue, 'Raw')\n",
    "path_gitlab_issue_raw = os.path.join(path_gitlab_issue, 'Raw')\n",
    "path_github_issue_filtered = os.path.join(path_github_issue, 'Filtered')\n",
    "path_gitlab_issue_filtered = os.path.join(path_gitlab_issue, 'Filtered')\n",
    "\n",
    "if not os.path.exists(path_github):\n",
    "    os.makedirs(path_github)\n",
    "\n",
    "if not os.path.exists(path_gitlab):\n",
    "    os.makedirs(path_gitlab)\n",
    "\n",
    "if not os.path.exists(path_labeling):\n",
    "    os.makedirs(path_labeling)\n",
    "\n",
    "if not os.path.exists(path_github_repo):\n",
    "    os.makedirs(path_github_repo)\n",
    "\n",
    "if not os.path.exists(path_gitlab_repo):\n",
    "    os.makedirs(path_gitlab_repo)\n",
    "\n",
    "if not os.path.exists(path_github_issue):\n",
    "    os.makedirs(path_github_issue)\n",
    "\n",
    "if not os.path.exists(path_gitlab_issue):\n",
    "    os.makedirs(path_gitlab_issue)\n",
    "\n",
    "if not os.path.exists(path_github_repo_raw):\n",
    "    os.makedirs(path_github_repo_raw)\n",
    "\n",
    "if not os.path.exists(path_gitlab_repo_raw):\n",
    "    os.makedirs(path_gitlab_repo_raw)\n",
    "\n",
    "if not os.path.exists(path_github_issue_raw):\n",
    "    os.makedirs(path_github_issue_raw)\n",
    "\n",
    "if not os.path.exists(path_gitlab_issue_raw):\n",
    "    os.makedirs(path_gitlab_issue_raw)\n",
    "\n",
    "if not os.path.exists(path_github_issue_filtered):\n",
    "    os.makedirs(path_github_issue_filtered)\n",
    "\n",
    "if not os.path.exists(path_gitlab_issue_filtered):\n",
    "    os.makedirs(path_gitlab_issue_filtered)\n",
    "\n",
    "if not os.path.exists(path_github_repo_scraped):\n",
    "    os.makedirs(path_github_repo_scraped)\n",
    "\n",
    "if not os.path.exists(path_gitlab_repo_scraped):\n",
    "    os.makedirs(path_gitlab_repo_scraped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_repo = {\n",
    "    'Aim': 'aimhubio/aim',\n",
    "    'Amazon SageMaker': 'aws/sagemaker-python-sdk',\n",
    "    'Azure Machine Learning': 'Azure/azure-sdk-for-python',\n",
    "    'ClearML': 'allegroai/clearml',\n",
    "    'Codalab': 'codalab/codalab-worksheets',\n",
    "    'DVC': 'iterative/dvc',\n",
    "    'Determined': 'determined-ai/determined',\n",
    "    'Domino': 'dominodatalab/python-domino',\n",
    "    'Guild AI': 'guildai/guildai',\n",
    "    'Kedro': 'kedro-org/kedro',\n",
    "    'MLflow': 'mlflow/mlflow',\n",
    "    'MLRun': 'mlrun/mlrun',\n",
    "    'ModelDB': 'VertaAI/modeldb',\n",
    "    'Neptune': 'neptune-ai/neptune-client',\n",
    "    'Optuna': 'optuna/optuna',\n",
    "    'Polyaxon': 'polyaxon/polyaxon',\n",
    "    'Sacred': 'IDSIA/sacred',\n",
    "    'Valohai': 'valohai/valohai-cli',\n",
    "    'Weights & Biases': 'wandb/wandb'\n",
    "}\n",
    "\n",
    "tools_release_date = {\n",
    "    'Amazon SageMaker': '2017-11-19',\n",
    "    'Azure Machine Learning': '2015-02-18',\n",
    "    'cnvrg.io': '2020-03-31',\n",
    "    'Comet': '2017-01-01',\n",
    "    'Iterative Studio': '2021-05-12',\n",
    "    'Polyaxon': '2018-10-16',\n",
    "    'SigOpt': '2014-11-01',\n",
    "    'Vertex AI': '2019-03-01'\n",
    "}\n",
    "\n",
    "tools_link = {\n",
    "    'cnvrg.io': 'https://github.com/cnvrg',\n",
    "    'Comet': 'https://github.com/comet-ml',\n",
    "    'Iterative Studio': 'https://studio.iterative.ai',\n",
    "    'SigOpt': 'https://github.com/sigopt',\n",
    "    'Vertex AI': 'https://cloud.google.com/vertex-ai'\n",
    "}\n",
    "\n",
    "tools_keywords = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['amazon sagemaker', 'aws sagemaker', 'sagemaker'],\n",
    "    'Azure Machine Learning': ['microsoft azure machine learning', 'azure machine learning', 'microsoft azure ml', 'microsoft azureml', 'azure ml', 'azureml'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['google vertex ai', 'vertex ai'],\n",
    "    'Weights & Biases': ['weights & biases', 'weights&biases', 'W & B', 'W&B', 'weights and biases', 'wandb']\n",
    "}\n",
    "\n",
    "ignore_tools = {\n",
    "\n",
    "}\n",
    "\n",
    "issue_labels = {\n",
    "    'bug',\n",
    "    'error',\n",
    "    'invalid',\n",
    "    'looking into it',\n",
    "    'waiting feedback',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scrape.GHMiner import GitHubMiner\n",
    "from Scrape.GLMiner import GitLabMiner\n",
    "\n",
    "github_miner = GitHubMiner(private_token=os.getenv('GITHUB_TOKEN'))\n",
    "gitlab_miner = GitLabMiner(private_token=os.getenv('GITLAB_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repo</th>\n",
       "      <th>Link</th>\n",
       "      <th>Repo Creation Date</th>\n",
       "      <th>Last Commit Date</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Language</th>\n",
       "      <th>Size</th>\n",
       "      <th>#Star</th>\n",
       "      <th>#Watch</th>\n",
       "      <th>#Fork</th>\n",
       "      <th>#Contributors</th>\n",
       "      <th>#Branches</th>\n",
       "      <th>#Releases</th>\n",
       "      <th>#Commits</th>\n",
       "      <th>#Pull Requests</th>\n",
       "      <th>#Pull Requests (Open)</th>\n",
       "      <th>#Issues</th>\n",
       "      <th>#Issues (Open)</th>\n",
       "      <th>Name</th>\n",
       "      <th>First Release Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aimhubio/aim</td>\n",
       "      <td>https://github.com/aimhubio/aim</td>\n",
       "      <td>2019-05-31 18:25:07</td>\n",
       "      <td>2023-03-20 22:41:11</td>\n",
       "      <td>[python, ai, data-science, data-visualization,...</td>\n",
       "      <td>Python</td>\n",
       "      <td>63726.0</td>\n",
       "      <td>3276.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>1753.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2582.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>Aim</td>\n",
       "      <td>2022-01-22 13:45:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aws/sagemaker-python-sdk</td>\n",
       "      <td>https://github.com/aws/sagemaker-python-sdk</td>\n",
       "      <td>2017-11-14 01:03:33</td>\n",
       "      <td>2023-03-21 05:48:29</td>\n",
       "      <td>[aws, mxnet, tensorflow, machine-learning, pyt...</td>\n",
       "      <td>Python</td>\n",
       "      <td>109008.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>945.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>2941.0</td>\n",
       "      <td>2373.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>3573.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>2017-11-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Azure/azure-sdk-for-python</td>\n",
       "      <td>https://github.com/Azure/azure-sdk-for-python</td>\n",
       "      <td>2012-04-24 16:46:12</td>\n",
       "      <td>2023-03-21 20:55:01</td>\n",
       "      <td>[python, azure, azure-sdk, hacktoberfest]</td>\n",
       "      <td>Python</td>\n",
       "      <td>558786.0</td>\n",
       "      <td>3555.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>2807.0</td>\n",
       "      <td>14027.0</td>\n",
       "      <td>21583.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>29413.0</td>\n",
       "      <td>964.0</td>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>2015-02-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allegroai/clearml</td>\n",
       "      <td>https://github.com/allegroai/clearml</td>\n",
       "      <td>2019-06-10 08:18:32</td>\n",
       "      <td>2023-03-17 12:35:36</td>\n",
       "      <td>[version-control, experiment-manager, version,...</td>\n",
       "      <td>Python</td>\n",
       "      <td>42846.0</td>\n",
       "      <td>4218.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2028.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>945.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>ClearML</td>\n",
       "      <td>2019-06-11 17:27:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>codalab/codalab-worksheets</td>\n",
       "      <td>https://github.com/codalab/codalab-worksheets</td>\n",
       "      <td>2014-11-30 22:33:18</td>\n",
       "      <td>2023-03-17 05:11:06</td>\n",
       "      <td>[]</td>\n",
       "      <td>Python</td>\n",
       "      <td>27818.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>4562.0</td>\n",
       "      <td>2269.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4424.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>Codalab</td>\n",
       "      <td>2017-05-14 00:32:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>iterative/dvc</td>\n",
       "      <td>https://github.com/iterative/dvc</td>\n",
       "      <td>2017-03-04 08:16:33</td>\n",
       "      <td>2023-03-21 21:33:53</td>\n",
       "      <td>[data-science, machine-learning, reproducibili...</td>\n",
       "      <td>Python</td>\n",
       "      <td>18209.0</td>\n",
       "      <td>11241.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>8569.0</td>\n",
       "      <td>4739.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8925.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>DVC</td>\n",
       "      <td>2017-05-04 08:03:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>determined-ai/determined</td>\n",
       "      <td>https://github.com/determined-ai/determined</td>\n",
       "      <td>2020-04-07 16:12:29</td>\n",
       "      <td>2023-03-21 22:42:13</td>\n",
       "      <td>[deep-learning, machine-learning, ml-platform,...</td>\n",
       "      <td>Python</td>\n",
       "      <td>114547.0</td>\n",
       "      <td>2099.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>5250.0</td>\n",
       "      <td>6044.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>6304.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Determined</td>\n",
       "      <td>2020-04-08 20:01:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dominodatalab/python-domino</td>\n",
       "      <td>https://github.com/dominodatalab/python-domino</td>\n",
       "      <td>2016-05-16 22:58:02</td>\n",
       "      <td>2023-03-21 16:11:30</td>\n",
       "      <td>[]</td>\n",
       "      <td>Python</td>\n",
       "      <td>507.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Domino</td>\n",
       "      <td>2020-08-05 05:16:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>guildai/guildai</td>\n",
       "      <td>https://github.com/guildai/guildai</td>\n",
       "      <td>2017-09-27 18:57:50</td>\n",
       "      <td>2023-03-21 21:20:02</td>\n",
       "      <td>[]</td>\n",
       "      <td>Python</td>\n",
       "      <td>17387.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5561.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>Guild AI</td>\n",
       "      <td>2022-04-28 14:31:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kedro-org/kedro</td>\n",
       "      <td>https://github.com/kedro-org/kedro</td>\n",
       "      <td>2019-04-18 10:29:56</td>\n",
       "      <td>2023-03-21 11:07:22</td>\n",
       "      <td>[pipeline, kedro, hacktoberfest, mlops, experi...</td>\n",
       "      <td>Python</td>\n",
       "      <td>173129.0</td>\n",
       "      <td>8189.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>771.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2201.0</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2227.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>Kedro</td>\n",
       "      <td>2019-06-03 16:15:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mlflow/mlflow</td>\n",
       "      <td>https://github.com/mlflow/mlflow</td>\n",
       "      <td>2018-06-05 16:05:58</td>\n",
       "      <td>2023-03-21 06:01:14</td>\n",
       "      <td>[machine-learning, ai, ml, mlflow, apache-spar...</td>\n",
       "      <td>Python</td>\n",
       "      <td>134182.0</td>\n",
       "      <td>13888.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>3258.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3827.0</td>\n",
       "      <td>5130.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>7927.0</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>MLflow</td>\n",
       "      <td>2018-06-27 16:19:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mlrun/mlrun</td>\n",
       "      <td>https://github.com/mlrun/mlrun</td>\n",
       "      <td>2019-09-01 16:59:19</td>\n",
       "      <td>2023-03-21 13:52:55</td>\n",
       "      <td>[mlops, python, data-science, machine-learning...</td>\n",
       "      <td>Python</td>\n",
       "      <td>47768.0</td>\n",
       "      <td>927.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>2989.0</td>\n",
       "      <td>3076.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3295.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>MLRun</td>\n",
       "      <td>2019-09-08 21:21:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>VertaAI/modeldb</td>\n",
       "      <td>https://github.com/VertaAI/modeldb</td>\n",
       "      <td>2016-10-19 01:07:26</td>\n",
       "      <td>2023-03-21 22:15:38</td>\n",
       "      <td>[machine-learning, model-management, modeldb, ...</td>\n",
       "      <td>Java</td>\n",
       "      <td>49081.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3729.0</td>\n",
       "      <td>3546.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>3679.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>ModelDB</td>\n",
       "      <td>2020-04-01 03:47:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>neptune-ai/neptune-client</td>\n",
       "      <td>https://github.com/neptune-ai/neptune-client</td>\n",
       "      <td>2019-02-11 11:25:57</td>\n",
       "      <td>2023-03-21 20:17:39</td>\n",
       "      <td>[pytorch, keras, lightgbm, xgboost, optuna, te...</td>\n",
       "      <td>Python</td>\n",
       "      <td>8732.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1395.0</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1308.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Neptune</td>\n",
       "      <td>2019-04-02 11:58:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>optuna/optuna</td>\n",
       "      <td>https://github.com/optuna/optuna</td>\n",
       "      <td>2018-02-21 06:12:56</td>\n",
       "      <td>2023-03-20 08:11:38</td>\n",
       "      <td>[python, machine-learning, parallel, distribut...</td>\n",
       "      <td>Python</td>\n",
       "      <td>18106.0</td>\n",
       "      <td>7795.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>814.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>15266.0</td>\n",
       "      <td>2998.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4378.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>Optuna</td>\n",
       "      <td>2018-05-10 08:41:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>polyaxon/polyaxon</td>\n",
       "      <td>https://github.com/polyaxon/polyaxon</td>\n",
       "      <td>2016-12-26 12:48:47</td>\n",
       "      <td>2023-03-21 17:55:30</td>\n",
       "      <td>[deep-learning, machine-learning, artificial-i...</td>\n",
       "      <td>None</td>\n",
       "      <td>126377.0</td>\n",
       "      <td>3275.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10077.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1463.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>Polyaxon</td>\n",
       "      <td>2018-10-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>IDSIA/sacred</td>\n",
       "      <td>https://github.com/IDSIA/sacred</td>\n",
       "      <td>2014-03-31 18:05:29</td>\n",
       "      <td>2023-02-24 14:28:50</td>\n",
       "      <td>[python, machine-learning, infrastructure, rep...</td>\n",
       "      <td>Python</td>\n",
       "      <td>6168.0</td>\n",
       "      <td>4013.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>912.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>Sacred</td>\n",
       "      <td>2016-01-13 18:56:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>valohai/valohai-cli</td>\n",
       "      <td>https://github.com/valohai/valohai-cli</td>\n",
       "      <td>2017-02-08 12:46:54</td>\n",
       "      <td>2023-03-14 11:44:04</td>\n",
       "      <td>[machine-learning, client, api, command-line, ...</td>\n",
       "      <td>Python</td>\n",
       "      <td>627.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Valohai</td>\n",
       "      <td>2019-07-26 10:05:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>wandb/wandb</td>\n",
       "      <td>https://github.com/wandb/wandb</td>\n",
       "      <td>2017-03-24 05:46:23</td>\n",
       "      <td>2023-03-20 20:45:48</td>\n",
       "      <td>[machine-learning, experiment-track, deep-lear...</td>\n",
       "      <td>Python</td>\n",
       "      <td>73890.0</td>\n",
       "      <td>5671.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>612.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>2867.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>5203.0</td>\n",
       "      <td>738.0</td>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>2018-11-11 21:54:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/cnvrg</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cnvrg.io</td>\n",
       "      <td>2020-03-31 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/comet-ml</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Comet</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://studio.iterative.ai</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Iterative Studio</td>\n",
       "      <td>2021-05-12 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/sigopt</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SigOpt</td>\n",
       "      <td>2014-11-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://cloud.google.com/vertex-ai</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>2019-03-01 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Repo  \\\n",
       "0                  aimhubio/aim   \n",
       "1      aws/sagemaker-python-sdk   \n",
       "2    Azure/azure-sdk-for-python   \n",
       "3             allegroai/clearml   \n",
       "4    codalab/codalab-worksheets   \n",
       "5                 iterative/dvc   \n",
       "6      determined-ai/determined   \n",
       "7   dominodatalab/python-domino   \n",
       "8               guildai/guildai   \n",
       "9               kedro-org/kedro   \n",
       "10                mlflow/mlflow   \n",
       "11                  mlrun/mlrun   \n",
       "12              VertaAI/modeldb   \n",
       "13    neptune-ai/neptune-client   \n",
       "14                optuna/optuna   \n",
       "15            polyaxon/polyaxon   \n",
       "16                 IDSIA/sacred   \n",
       "17          valohai/valohai-cli   \n",
       "18                  wandb/wandb   \n",
       "19                          NaN   \n",
       "20                          NaN   \n",
       "21                          NaN   \n",
       "22                          NaN   \n",
       "23                          NaN   \n",
       "\n",
       "                                              Link  Repo Creation Date  \\\n",
       "0                  https://github.com/aimhubio/aim 2019-05-31 18:25:07   \n",
       "1      https://github.com/aws/sagemaker-python-sdk 2017-11-14 01:03:33   \n",
       "2    https://github.com/Azure/azure-sdk-for-python 2012-04-24 16:46:12   \n",
       "3             https://github.com/allegroai/clearml 2019-06-10 08:18:32   \n",
       "4    https://github.com/codalab/codalab-worksheets 2014-11-30 22:33:18   \n",
       "5                 https://github.com/iterative/dvc 2017-03-04 08:16:33   \n",
       "6      https://github.com/determined-ai/determined 2020-04-07 16:12:29   \n",
       "7   https://github.com/dominodatalab/python-domino 2016-05-16 22:58:02   \n",
       "8               https://github.com/guildai/guildai 2017-09-27 18:57:50   \n",
       "9               https://github.com/kedro-org/kedro 2019-04-18 10:29:56   \n",
       "10                https://github.com/mlflow/mlflow 2018-06-05 16:05:58   \n",
       "11                  https://github.com/mlrun/mlrun 2019-09-01 16:59:19   \n",
       "12              https://github.com/VertaAI/modeldb 2016-10-19 01:07:26   \n",
       "13    https://github.com/neptune-ai/neptune-client 2019-02-11 11:25:57   \n",
       "14                https://github.com/optuna/optuna 2018-02-21 06:12:56   \n",
       "15            https://github.com/polyaxon/polyaxon 2016-12-26 12:48:47   \n",
       "16                 https://github.com/IDSIA/sacred 2014-03-31 18:05:29   \n",
       "17          https://github.com/valohai/valohai-cli 2017-02-08 12:46:54   \n",
       "18                  https://github.com/wandb/wandb 2017-03-24 05:46:23   \n",
       "19                        https://github.com/cnvrg                 NaT   \n",
       "20                     https://github.com/comet-ml                 NaT   \n",
       "21                     https://studio.iterative.ai                 NaT   \n",
       "22                       https://github.com/sigopt                 NaT   \n",
       "23              https://cloud.google.com/vertex-ai                 NaT   \n",
       "\n",
       "      Last Commit Date                                             Topics  \\\n",
       "0  2023-03-20 22:41:11  [python, ai, data-science, data-visualization,...   \n",
       "1  2023-03-21 05:48:29  [aws, mxnet, tensorflow, machine-learning, pyt...   \n",
       "2  2023-03-21 20:55:01          [python, azure, azure-sdk, hacktoberfest]   \n",
       "3  2023-03-17 12:35:36  [version-control, experiment-manager, version,...   \n",
       "4  2023-03-17 05:11:06                                                 []   \n",
       "5  2023-03-21 21:33:53  [data-science, machine-learning, reproducibili...   \n",
       "6  2023-03-21 22:42:13  [deep-learning, machine-learning, ml-platform,...   \n",
       "7  2023-03-21 16:11:30                                                 []   \n",
       "8  2023-03-21 21:20:02                                                 []   \n",
       "9  2023-03-21 11:07:22  [pipeline, kedro, hacktoberfest, mlops, experi...   \n",
       "10 2023-03-21 06:01:14  [machine-learning, ai, ml, mlflow, apache-spar...   \n",
       "11 2023-03-21 13:52:55  [mlops, python, data-science, machine-learning...   \n",
       "12 2023-03-21 22:15:38  [machine-learning, model-management, modeldb, ...   \n",
       "13 2023-03-21 20:17:39  [pytorch, keras, lightgbm, xgboost, optuna, te...   \n",
       "14 2023-03-20 08:11:38  [python, machine-learning, parallel, distribut...   \n",
       "15 2023-03-21 17:55:30  [deep-learning, machine-learning, artificial-i...   \n",
       "16 2023-02-24 14:28:50  [python, machine-learning, infrastructure, rep...   \n",
       "17 2023-03-14 11:44:04  [machine-learning, client, api, command-line, ...   \n",
       "18 2023-03-20 20:45:48  [machine-learning, experiment-track, deep-lear...   \n",
       "19                 NaT                                                NaN   \n",
       "20                 NaT                                                NaN   \n",
       "21                 NaT                                                NaN   \n",
       "22                 NaT                                                NaN   \n",
       "23                 NaT                                                NaN   \n",
       "\n",
       "   Language      Size    #Star  #Watch   #Fork  #Contributors  #Branches  \\\n",
       "0    Python   63726.0   3276.0    36.0   206.0           54.0       81.0   \n",
       "1    Python  109008.0   1800.0   134.0   945.0          327.0       14.0   \n",
       "2    Python  558786.0   3555.0   367.0  2264.0          398.0      616.0   \n",
       "3    Python   42846.0   4218.0    86.0   567.0           69.0        3.0   \n",
       "4    Python   27818.0    138.0    18.0    79.0           53.0      137.0   \n",
       "5    Python   18209.0  11241.0   135.0  1038.0          259.0       22.0   \n",
       "6    Python  114547.0   2099.0    66.0   292.0           81.0      178.0   \n",
       "7    Python     507.0     53.0    28.0    53.0           33.0       53.0   \n",
       "8    Python   17387.0    781.0    13.0    71.0           21.0       68.0   \n",
       "9    Python  173129.0   8189.0   104.0   771.0          171.0       31.0   \n",
       "10   Python  134182.0  13888.0   287.0  3258.0          456.0      218.0   \n",
       "11   Python   47768.0    927.0    26.0   182.0           59.0       24.0   \n",
       "12     Java   49081.0   1559.0    71.0   268.0           49.0      580.0   \n",
       "13   Python    8732.0    372.0    14.0    38.0           30.0       41.0   \n",
       "14   Python   18106.0   7795.0   121.0   814.0          193.0       24.0   \n",
       "15     None  126377.0   3275.0    78.0   319.0           90.0       16.0   \n",
       "16   Python    6168.0   4013.0    70.0   370.0           94.0       11.0   \n",
       "17   Python     627.0     13.0     6.0     6.0            8.0        9.0   \n",
       "18   Python   73890.0   5671.0    47.0   448.0          120.0      612.0   \n",
       "19      NaN       NaN      NaN     NaN     NaN            NaN        NaN   \n",
       "20      NaN       NaN      NaN     NaN     NaN            NaN        NaN   \n",
       "21      NaN       NaN      NaN     NaN     NaN            NaN        NaN   \n",
       "22      NaN       NaN      NaN     NaN     NaN            NaN        NaN   \n",
       "23      NaN       NaN      NaN     NaN     NaN            NaN        NaN   \n",
       "\n",
       "    #Releases  #Commits  #Pull Requests  #Pull Requests (Open)  #Issues  \\\n",
       "0        49.0    2042.0          1753.0                   25.0   2582.0   \n",
       "1       493.0    2941.0          2373.0                   49.0   3573.0   \n",
       "2      2807.0   14027.0         21583.0                  158.0  29413.0   \n",
       "3        77.0    2028.0           194.0                    2.0    945.0   \n",
       "4       116.0    4562.0          2269.0                   24.0   4424.0   \n",
       "5       429.0    8569.0          4739.0                   12.0   8925.0   \n",
       "6        79.0    5250.0          6044.0                   75.0   6304.0   \n",
       "7        15.0     203.0           135.0                    5.0    170.0   \n",
       "8         2.0    5561.0            71.0                    2.0    481.0   \n",
       "9        36.0    2201.0          1041.0                    8.0   2227.0   \n",
       "10       65.0    3827.0          5130.0                  148.0   7927.0   \n",
       "11      380.0    2989.0          3076.0                   50.0   3295.0   \n",
       "12        2.0    3729.0          3546.0                   96.0   3679.0   \n",
       "13      132.0    1395.0          1126.0                   18.0   1308.0   \n",
       "14       57.0   15266.0          2998.0                   22.0   4378.0   \n",
       "15        0.0   10077.0           398.0                    2.0   1463.0   \n",
       "16       12.0    1340.0           362.0                    2.0    912.0   \n",
       "17        2.0     544.0           190.0                    1.0    274.0   \n",
       "18      111.0    5191.0          2867.0                  214.0   5203.0   \n",
       "19        NaN       NaN             NaN                    NaN      NaN   \n",
       "20        NaN       NaN             NaN                    NaN      NaN   \n",
       "21        NaN       NaN             NaN                    NaN      NaN   \n",
       "22        NaN       NaN             NaN                    NaN      NaN   \n",
       "23        NaN       NaN             NaN                    NaN      NaN   \n",
       "\n",
       "    #Issues (Open)                    Name  First Release Date  \n",
       "0            249.0                     Aim 2022-01-22 13:45:58  \n",
       "1            476.0        Amazon SageMaker 2017-11-19 00:00:00  \n",
       "2            964.0  Azure Machine Learning 2015-02-18 00:00:00  \n",
       "3            305.0                 ClearML 2019-06-11 17:27:11  \n",
       "4            390.0                 Codalab 2017-05-14 00:32:55  \n",
       "5            595.0                     DVC 2017-05-04 08:03:08  \n",
       "6            100.0              Determined 2020-04-08 20:01:20  \n",
       "7             16.0                  Domino 2020-08-05 05:16:39  \n",
       "8            196.0                Guild AI 2022-04-28 14:31:07  \n",
       "9            274.0                   Kedro 2019-06-03 16:15:43  \n",
       "10          1051.0                  MLflow 2018-06-27 16:19:13  \n",
       "11           105.0                   MLRun 2019-09-08 21:21:26  \n",
       "12           177.0                 ModelDB 2020-04-01 03:47:14  \n",
       "13            33.0                 Neptune 2019-04-02 11:58:35  \n",
       "14           121.0                  Optuna 2018-05-10 08:41:56  \n",
       "15           121.0                Polyaxon 2018-10-16 00:00:00  \n",
       "16            94.0                  Sacred 2016-01-13 18:56:23  \n",
       "17            17.0                 Valohai 2019-07-26 10:05:34  \n",
       "18           738.0        Weights & Biases 2018-11-11 21:54:26  \n",
       "19             NaN                cnvrg.io 2020-03-31 00:00:00  \n",
       "20             NaN                   Comet 2017-01-01 00:00:00  \n",
       "21             NaN        Iterative Studio 2021-05-12 00:00:00  \n",
       "22             NaN                  SigOpt 2014-11-01 00:00:00  \n",
       "23             NaN               Vertex AI 2019-03-01 00:00:00  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools_data = pd.DataFrame()\n",
    "\n",
    "# scrape open-source asset-management tools\n",
    "for tool_name, tool_repo in tools_repo.items():\n",
    "    if tool_name in tools_release_date:\n",
    "        tool_data, error_data = github_miner.scrape_repo(\n",
    "            repo_name=tool_repo, real_name=tool_name, release_date=pd.to_datetime(tools_release_date[tool_name]))\n",
    "    else:\n",
    "        tool_data, error_data = github_miner.scrape_repo(\n",
    "            repo_name=tool_repo, real_name=tool_name)\n",
    "\n",
    "    if not tool_data.empty:\n",
    "        tools_data = pd.concat([tools_data, tool_data], ignore_index=True)\n",
    "    else:\n",
    "        print(error_data)\n",
    "\n",
    "# add closed-source asset-management tools\n",
    "for tool_name in tools_link.keys():\n",
    "    tool_data = {\n",
    "        'Name': tool_name,\n",
    "        'Link': tools_link[tool_name],\n",
    "        'First Release Date': pd.to_datetime(tools_release_date[tool_name])\n",
    "    }\n",
    "    tool_data = pd.DataFrame([tool_data])\n",
    "    tools_data = pd.concat([tools_data, tool_data], ignore_index=True)\n",
    "\n",
    "tools_data.to_json(os.path.join(path_dataset, 'Tools.json'),\n",
    "                   indent=4, orient='records')\n",
    "tools_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dependents = pd.DataFrame()\n",
    "\n",
    "# collect dependents for tools with coding patterns\n",
    "for tool_name in tools_keywords.keys():\n",
    "    github_dependents = []\n",
    "    gitlab_dependents = []\n",
    "\n",
    "    # collect Github dependents\n",
    "    file_name = os.path.join(path_github_repo_raw, tool_name + '.json')\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, encoding='utf8') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            # either search by sourcegraph\n",
    "            if 'Results' in json_data:\n",
    "                for repo_file in json_data['Results']:\n",
    "                    # file name match pattern\n",
    "                    if 'FileMatch' == repo_file['__typename'] and repo_file['repository']['name'].startswith('github'):\n",
    "                        repo_name = repo_file['repository']['name'].removeprefix(\n",
    "                            'github.com/')\n",
    "                        github_dependents.append(repo_name)\n",
    "                    # code usage match pattern\n",
    "                    elif 'Repository' == repo_file['__typename'] and repo_file['name'].startswith('github'):\n",
    "                        repo_name = repo_file['name'].removeprefix(\n",
    "                            'github.com/')\n",
    "                        github_dependents.append(repo_name)\n",
    "            # or search by dependent graph\n",
    "            elif 'all_public_dependent_repos' in json_data:\n",
    "                for repo_file in json_data['all_public_dependent_repos']:\n",
    "                    github_dependents.append(repo_file['name'])\n",
    "\n",
    "    # collect Gitlab dependents\n",
    "    file_name = os.path.join(path_gitlab_repo_raw, tool_name + '.json')\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, encoding='utf8') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            # search by sourcegraph exclusively\n",
    "            for repo_file in json_data['Results']:\n",
    "                # file name match pattern\n",
    "                if 'FileMatch' == repo_file['__typename'] and repo_file['repository']['name'].startswith('gitlab'):\n",
    "                    repo_name = repo_file['repository']['name'].removeprefix(\n",
    "                        'gitlab.com/')\n",
    "                    gitlab_dependents.append(repo_name)\n",
    "                # code usage match pattern\n",
    "                elif 'Repository' == repo_file['__typename'] and repo_file['name'].startswith('gitlab'):\n",
    "                    repo_name = repo_file['name'].removeprefix('gitlab.com/')\n",
    "                    gitlab_dependents.append(repo_name)\n",
    "\n",
    "    # remove tool repo from dependents if any\n",
    "    if tool_name in tools_repo and tools_repo[tool_name] in github_dependents:\n",
    "        github_dependents.remove(tools_repo[tool_name])\n",
    "\n",
    "    # no need to add tools without dependents\n",
    "    if not len(github_dependents) and not len(gitlab_dependents):\n",
    "        continue\n",
    "\n",
    "    dependent = {\n",
    "        'Tool': tool_name,\n",
    "        'GitHub Dependents': github_dependents,\n",
    "        'GitLab Dependents': gitlab_dependents\n",
    "    }\n",
    "\n",
    "    dependents = pd.concat(\n",
    "        [dependents, pd.DataFrame([dependent])], ignore_index=True)\n",
    "\n",
    "dependents.to_json(os.path.join(\n",
    "    path_dataset, 'Dependents.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#GitHub Dependents</th>\n",
       "      <th>#GitLab Dependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aim</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>931</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ClearML</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Codalab</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comet</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Determined</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Domino</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DVC</td>\n",
       "      <td>4229</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Guild AI</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kedro</td>\n",
       "      <td>838</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLflow</td>\n",
       "      <td>1189</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLRun</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ModelDB</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Neptune</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Optuna</td>\n",
       "      <td>2606</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Polyaxon</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sacred</td>\n",
       "      <td>1289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SigOpt</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Valohai</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>10730</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tool #GitHub Dependents #GitLab Dependents\n",
       "0                      Aim                 92                  1\n",
       "1         Amazon SageMaker                931                  3\n",
       "2   Azure Machine Learning                689                  0\n",
       "3                  ClearML                303                  0\n",
       "4                  Codalab                 30                  0\n",
       "5                    Comet                480                  0\n",
       "6               Determined                 44                  0\n",
       "7                   Domino                  2                  0\n",
       "8                      DVC               4229                  0\n",
       "9                 Guild AI                 53                  4\n",
       "10                   Kedro                838                  0\n",
       "11                  MLflow               1189                  3\n",
       "12                   MLRun                 17                  0\n",
       "13                 ModelDB                  7                  0\n",
       "14                 Neptune                280                  0\n",
       "15                  Optuna               2606                  0\n",
       "16                Polyaxon                 35                  0\n",
       "17                  Sacred               1289                  0\n",
       "18                  SigOpt                 55                  0\n",
       "19                 Valohai                 31                  0\n",
       "20               Vertex AI                 96                  0\n",
       "21        Weights & Biases              10730                  0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependents_summary = pd.DataFrame(\n",
    "    columns=['Tool', '#GitHub Dependents', '#GitLab Dependents'])\n",
    "for index, row in dependents.iterrows():\n",
    "    dependent_data = {\n",
    "        'Tool': row['Tool'],\n",
    "        '#GitHub Dependents': len(row['GitHub Dependents']),\n",
    "        '#GitLab Dependents': len(row['GitLab Dependents'])\n",
    "    }\n",
    "    dependent_data = pd.DataFrame([dependent_data])\n",
    "    dependents_summary = pd.concat(\n",
    "        [dependents_summary, dependent_data], ignore_index=True)\n",
    "dependents_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dependents = pd.read_json(os.path.join(path_dataset, 'Dependents.json'))\n",
    "df_tools = pd.read_json(os.path.join(path_dataset, 'Tools.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape Gitlab dependents general information for each tool\n",
    "for index, row in df_dependents.iterrows():\n",
    "    print(f'{index}: {row[\"Tool\"]}')\n",
    "    repos_data, errors_data = gitlab_miner.scrape_repo_list(\n",
    "        row['GitLab Dependents'])\n",
    "\n",
    "    if not repos_data.empty:\n",
    "        repos_data = repos_data.sort_values(by='#Issues', ascending=False)\n",
    "        repos_data.to_json(os.path.join(\n",
    "            path_gitlab_repo_scraped, f'{row[\"Tool\"]}.json'), indent=4, orient='records')\n",
    "\n",
    "    if not errors_data.empty:\n",
    "        errors_data.to_json(os.path.join(path_gitlab_repo_scraped,\n",
    "                            f'Discarded.{row[\"Tool\"]}.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape issues of Gitlab dependents for each tool\n",
    "for index, row in df_tools.iterrows():\n",
    "    file_name = os.path.join(path_gitlab_repo_scraped, f'{row[\"Name\"]}.json')\n",
    "    if os.path.exists(file_name):\n",
    "        repos = pd.read_json(file_name)\n",
    "        # filter out repos without any issues\n",
    "        repos = repos[repos['#Issues'] > 0]\n",
    "        # filter out repos created before the tool's first release date\n",
    "        repos = repos[repos['Repo Creation Date'] > row['First Release Date']]\n",
    "        print(f'{row[\"Name\"]}: {repos[\"#Issues\"].sum()}')\n",
    "        # scrape issues for the current tool\n",
    "        issues = gitlab_miner.scrape_issue_list(repos['Repo'].tolist())\n",
    "        if not issues.empty:\n",
    "            issues.to_json(os.path.join(path_gitlab_issue_raw,\n",
    "                                        f'{row[\"Name\"]}.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude Gitlab issues that are not related to each tool\n",
    "valid_issues_all = pd.DataFrame()\n",
    "\n",
    "for file_name in glob.glob(os.path.join(path_gitlab_issue_raw, '*.json')):\n",
    "    issues = pd.read_json(file_name)\n",
    "    valid_issues = pd.DataFrame()\n",
    "    valid_fixes = pd.DataFrame()\n",
    "    tool_name = os.path.split(file_name)[1].split('.')[0]\n",
    "\n",
    "    for index, issue in issues.iterrows():\n",
    "        for keyword in tools_keywords[tool_name]:\n",
    "            if 'amazon' in keyword:\n",
    "                continue\n",
    "            if keyword in issue['Issue_title'].lower():\n",
    "                valid_issue = pd.DataFrame([issue])\n",
    "                valid_issues = pd.concat(\n",
    "                    [valid_issues, valid_issue], ignore_index=True)\n",
    "                if not pd.isnull(issue['Issue_closed_time']):\n",
    "                    valid_fixes = pd.concat(\n",
    "                        [valid_fixes, valid_issue], ignore_index=True)\n",
    "                break\n",
    "\n",
    "valid_issues_all = valid_issues_all[~valid_issues_all['Tool'].isin(\n",
    "    ignore_tools)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arena::security', 'improvement', 'learn', 'product::sorts', 'type::bug'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = set()\n",
    "for _, row in valid_issues_all['Issue_label'].map(set).items():\n",
    "    final = final.union(row)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out issues that are not related to challenges\n",
    "valid_issues_filtered = []\n",
    "\n",
    "for index, row in valid_issues_all.iterrows():\n",
    "    if not row['Issue_title'].isascii():\n",
    "        continue\n",
    "\n",
    "    break_sign = False\n",
    "    title = row['Issue_title'].lower()\n",
    "    for label_repo in row['Issue_label']:\n",
    "        for issue_label in issue_labels:\n",
    "            if not break_sign and issue_label in label_repo.lower():\n",
    "                valid_issues_filtered.append(row)\n",
    "                break_sign = True\n",
    "            elif not break_sign and issue_label in title:\n",
    "                valid_issues_filtered.append(row)\n",
    "                break_sign = True\n",
    "\n",
    "valid_issues_filtered = pd.concat(valid_issues_filtered, axis=1).T\n",
    "valid_issues_filtered.to_json(os.path.join(\n",
    "    path_gitlab_issue_filtered, 'issues.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#Issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tool  #Issue\n",
       "0  Amazon SageMaker       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_issues = valid_issues_filtered.groupby(\n",
    "    'Tool').count()['Issue_title'].reset_index()\n",
    "summary_issues.rename(columns={'Issue_title': '#Issue'}, inplace=True)\n",
    "summary_issues = summary_issues.astype({'#Issue': 'int32'})\n",
    "summary_issues.to_csv(os.path.join(\n",
    "    path_gitlab_issue, 'summary.csv'), index=False)\n",
    "summary_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape Github dependents general information for each tool\n",
    "for index, row in df_dependents.iterrows():\n",
    "    print(f'{index}: {row[\"Tool\"]}')\n",
    "    repos_data, errors_data = github_miner.scrape_repo_list(\n",
    "        row['GitHub Dependents'])\n",
    "\n",
    "    if not repos_data.empty:\n",
    "        repos_data = repos_data.sort_values(by='#Issues', ascending=False)\n",
    "        repos_data.to_json(os.path.join(\n",
    "            path_github_repo_scraped, f'{row[\"Tool\"]}.json'), indent=4, orient='records')\n",
    "\n",
    "    if not errors_data.empty:\n",
    "        errors_data.to_json(os.path.join(path_github_repo_scraped,\n",
    "                            f'Discarded.{row[\"Tool\"]}.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape issues of Github dependents for each tool\n",
    "for index, row in df_tools.iterrows():\n",
    "    file_name = os.path.join(path_github_repo_scraped, f'{row[\"Name\"]}.json')\n",
    "    if os.path.exists(file_name):\n",
    "        repos = pd.read_json(file_name)\n",
    "        # filter out repos with only pr-based issues\n",
    "        repos = repos[repos['#Issues'] > repos['#Pull Requests']]\n",
    "        # filter out repos created before the tool's first release date\n",
    "        repos = repos[repos['Repo Creation Date'] > row['First Release Date']]\n",
    "        print(\n",
    "            f'{row[\"Name\"]}: {repos[\"#Issues\"].sum() - repos[\"#Pull Requests\"].sum()}')\n",
    "        # scrape issues for the current tool\n",
    "        issues = github_miner.scrape_issue_list(repos['Repo'].tolist())\n",
    "        if not issues.empty:\n",
    "            issues.to_json(os.path.join(path_github_issue_raw,\n",
    "                           f'{row[\"Name\"]}.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude Github issues that are not related to each tool\n",
    "valid_issues_all = pd.DataFrame()\n",
    "\n",
    "for file_name in glob.glob(os.path.join(path_github_issue_raw, '*.json')):\n",
    "    issues = pd.read_json(file_name)\n",
    "    valid_issues = pd.DataFrame()\n",
    "    valid_fixes = pd.DataFrame()\n",
    "    tool_name = os.path.split(file_name)[1].split('.')[0]\n",
    "\n",
    "    for index, issue in issues.iterrows():\n",
    "        for keyword in tools_keywords[tool_name]:\n",
    "            if keyword in issue['Issue_title'].lower():\n",
    "                valid_issue = pd.DataFrame([issue])\n",
    "                valid_issues = pd.concat(\n",
    "                    [valid_issues, valid_issue], ignore_index=True)\n",
    "                if pd.notna(issue['Issue_closed_time']):\n",
    "                    valid_fixes = pd.concat(\n",
    "                        [valid_fixes, valid_issue], ignore_index=True)\n",
    "                break\n",
    "\n",
    "valid_issues_all = valid_issues_all[~valid_issues_all['Tool'].isin(\n",
    "    ignore_tools)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"experiments\"',\n",
       " '0.4.6',\n",
       " '1.1',\n",
       " '1.4',\n",
       " '1.6',\n",
       " '1.7',\n",
       " '2.0',\n",
       " '3 - Quality of Life',\n",
       " '3rd party',\n",
       " '3rd party update',\n",
       " ':bridge_at_night:  Bridge',\n",
       " ':bug: bug',\n",
       " ':rotating_light:',\n",
       " '? - Needs Triage',\n",
       " 'A: example-dvc-experiments',\n",
       " 'A: example-get-started',\n",
       " 'ADO',\n",
       " 'AI\\u202fFrameworks/ONNX',\n",
       " 'AML Compute Instance',\n",
       " 'API',\n",
       " 'API & Doc',\n",
       " 'Auto\\u202fML',\n",
       " 'BF',\n",
       " 'Cloud',\n",
       " 'Community',\n",
       " 'Compute',\n",
       " 'Core UI',\n",
       " 'DRL',\n",
       " 'Data Labeling',\n",
       " 'Data4ML',\n",
       " 'Data\\u202fDrift',\n",
       " 'Data\\u202fPrep\\u202fServices',\n",
       " 'Documentation',\n",
       " 'ERRATA_CANDIDATE',\n",
       " 'Enhancement',\n",
       " 'Environments',\n",
       " 'Evaluation',\n",
       " 'Experimentation UI',\n",
       " 'FAQ',\n",
       " 'Feature - Medium Priority',\n",
       " 'HIGH',\n",
       " 'HPO',\n",
       " 'Hyperdrive',\n",
       " 'Important',\n",
       " 'In the roadmap',\n",
       " 'Inf1',\n",
       " 'Inference',\n",
       " 'Ingestion',\n",
       " 'Issue: Bug Report 🐞',\n",
       " 'Issue: Feature Request',\n",
       " 'L',\n",
       " 'LOE: S',\n",
       " 'Localized',\n",
       " 'MLOps',\n",
       " 'NLP',\n",
       " 'NUM',\n",
       " 'Needs Triage',\n",
       " 'Not related to PyCaret',\n",
       " 'Notebook',\n",
       " 'Optional',\n",
       " 'P0',\n",
       " 'P1',\n",
       " 'P2',\n",
       " 'Pipelines',\n",
       " 'Priority 1',\n",
       " 'Reinforcement Learning',\n",
       " 'RepoOfficiel',\n",
       " 'Review One',\n",
       " 'Review Two',\n",
       " 'SDK',\n",
       " 'Stage: Technical Design 🎨',\n",
       " 'Stale',\n",
       " 'TA',\n",
       " 'TODO',\n",
       " 'TODO before 1.0',\n",
       " 'Training',\n",
       " 'Training Service',\n",
       " 'Trn1',\n",
       " 'Usage',\n",
       " 'VISION',\n",
       " 'WIP',\n",
       " 'WIP - Susankha',\n",
       " 'Workspace Management',\n",
       " '[module] pipeline',\n",
       " 'accelerator: tpu',\n",
       " 'accessibility',\n",
       " 'ai',\n",
       " 'aiplatform',\n",
       " 'air',\n",
       " 'alonet',\n",
       " 'api: aiplatform',\n",
       " 'api: vertex-ai',\n",
       " 'app-ui',\n",
       " 'apply',\n",
       " 'arc_ml',\n",
       " 'architecture',\n",
       " 'area / SDK-storage',\n",
       " 'area / integrations',\n",
       " 'area-getting-started',\n",
       " 'area-ml-resource-management',\n",
       " 'area-remote-desktop',\n",
       " 'area-remote-web',\n",
       " 'area-sign-in',\n",
       " 'area-telemetry',\n",
       " 'area-treeview',\n",
       " 'area-yaml',\n",
       " 'area/async',\n",
       " 'area/components',\n",
       " 'area/components/aws/sagemaker',\n",
       " 'area/operator',\n",
       " 'area/registry',\n",
       " 'area/samples',\n",
       " 'area:databuilder',\n",
       " 'assigned',\n",
       " 'assigned-to-author',\n",
       " 'audience/technical',\n",
       " 'august-rewrite',\n",
       " 'automations',\n",
       " 'automl',\n",
       " 'awaiting response',\n",
       " 'awaiting-product-team-response',\n",
       " 'aws',\n",
       " 'azure',\n",
       " 'azure-provider',\n",
       " 'azureml',\n",
       " 'backlog',\n",
       " 'benchmark',\n",
       " 'bittensor',\n",
       " 'blocked',\n",
       " 'blocker',\n",
       " 'breakdown',\n",
       " 'breaking',\n",
       " 'bug',\n",
       " 'bug-bash',\n",
       " 'build',\n",
       " 'chapter: appendix-tools',\n",
       " 'checkpointing',\n",
       " 'chore',\n",
       " 'cleanup',\n",
       " 'closing-soon-if-no-response',\n",
       " 'code',\n",
       " 'concerns: agents',\n",
       " 'concerns: documentation',\n",
       " 'concerns: main API',\n",
       " 'configs',\n",
       " 'connectors',\n",
       " 'contribution welcomed',\n",
       " 'core/subsvc',\n",
       " 'customer-inquiry',\n",
       " 'customer-issue',\n",
       " 'cxp',\n",
       " 'data',\n",
       " 'data-sync',\n",
       " 'debt',\n",
       " 'dependencies',\n",
       " 'deploy',\n",
       " 'design',\n",
       " 'dev',\n",
       " 'dev workflow',\n",
       " 'devflows',\n",
       " 'discussion',\n",
       " 'diy_container',\n",
       " 'doc-bug',\n",
       " 'doc-enhancement',\n",
       " 'docker images',\n",
       " 'docs',\n",
       " 'documentation',\n",
       " 'duplicate',\n",
       " 'enhancement',\n",
       " 'enhancement request',\n",
       " 'env: new',\n",
       " 'env: sagemaker',\n",
       " 'environment',\n",
       " 'environment: slurm',\n",
       " 'ep:CUDA',\n",
       " 'epic',\n",
       " 'error',\n",
       " 'evaluate_model',\n",
       " 'example issue',\n",
       " 'example request',\n",
       " 'experimental',\n",
       " 'explore',\n",
       " 'feature',\n",
       " 'feature request',\n",
       " 'feature-request',\n",
       " 'fixme',\n",
       " 'forum',\n",
       " 'functionality',\n",
       " 'future release',\n",
       " 'good first issue',\n",
       " 'graphistry',\n",
       " 'gui',\n",
       " 'guides',\n",
       " 'help wanted',\n",
       " 'hi-ml-azure',\n",
       " 'high-priority',\n",
       " 'improvement',\n",
       " 'improvements',\n",
       " 'in progress',\n",
       " 'inference',\n",
       " 'inferencing-benchmark',\n",
       " 'info-needed',\n",
       " 'infrastructure',\n",
       " 'integration',\n",
       " 'invalid',\n",
       " 'investigating',\n",
       " 'investigation',\n",
       " 'iteration-candidate',\n",
       " 'journey:intermediate',\n",
       " 'keep fresh',\n",
       " 'kind/bug',\n",
       " 'kind/enhancement',\n",
       " 'kind/feature',\n",
       " 'kind/question',\n",
       " 'kind/reproducibility',\n",
       " 'kind/usability',\n",
       " 'kubeflow',\n",
       " 'lifecycle/frozen',\n",
       " 'lifecycle/stale',\n",
       " 'lightning',\n",
       " 'linting / formatting / cleaning',\n",
       " 'logger',\n",
       " 'logger: comet',\n",
       " 'logger: mlflow',\n",
       " 'logger: wandb',\n",
       " 'logging',\n",
       " 'looking into it',\n",
       " 'low priority',\n",
       " 'machine-learning',\n",
       " 'machine-learning/svc',\n",
       " 'major',\n",
       " 'metrics',\n",
       " 'missing_info',\n",
       " 'ml',\n",
       " 'ml-engineering',\n",
       " 'mlflow',\n",
       " 'model',\n",
       " 'module: text',\n",
       " 'must have',\n",
       " 'need-design-decision',\n",
       " 'needs triage',\n",
       " 'needs-more-info',\n",
       " 'needs-tests',\n",
       " 'needs-triage',\n",
       " 'neptune',\n",
       " 'new',\n",
       " 'new feature',\n",
       " 'new table',\n",
       " 'nnidev',\n",
       " 'no-issue-activity',\n",
       " 'normal',\n",
       " 'notebook',\n",
       " 'on hold',\n",
       " 'open for contribution',\n",
       " 'operationalization',\n",
       " 'optimization',\n",
       " 'organizational',\n",
       " 'p0-critical',\n",
       " 'p1',\n",
       " 'p1-important',\n",
       " 'p2-medium',\n",
       " 'p3-nice-to-have',\n",
       " 'phase / shipped',\n",
       " 'pipeline',\n",
       " 'pipeline 6: infer',\n",
       " 'pl',\n",
       " 'platform/aws',\n",
       " 'platform/other',\n",
       " 'plot_model',\n",
       " 'practice',\n",
       " 'pri/medium',\n",
       " 'priority 3 - nice to have',\n",
       " 'priority-p0',\n",
       " 'priority-p1',\n",
       " 'priority/important-longterm',\n",
       " 'priority/p1',\n",
       " 'priority: 1',\n",
       " 'priority: 2',\n",
       " 'priority: high',\n",
       " 'priority: medium',\n",
       " 'priority: p2',\n",
       " 'priority:high',\n",
       " 'priority:medium',\n",
       " 'priority_high',\n",
       " 'product-feedback',\n",
       " 'product-gap',\n",
       " 'product-issue',\n",
       " 'product-question',\n",
       " 'progress bar: rich',\n",
       " 'python',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quick-fix',\n",
       " 'refactor',\n",
       " 'refactoring',\n",
       " 'reporting and diagnostics',\n",
       " 'research',\n",
       " 'roadmap',\n",
       " 'sagemaker',\n",
       " 'sagemaker-dsk-v2',\n",
       " 'sagemaker_container',\n",
       " 'samples',\n",
       " 'scenario',\n",
       " 'sdk-docs',\n",
       " 'section',\n",
       " 'service update',\n",
       " 'service:executor',\n",
       " 'service:sm-executor',\n",
       " 'setup',\n",
       " 'snippets-request',\n",
       " 'spark',\n",
       " 'stale',\n",
       " 'stale :zzz:',\n",
       " 'status/triaged',\n",
       " 'status: phase 1',\n",
       " 'status: phase 2',\n",
       " 'status:completed',\n",
       " 'status:needs_reproducing',\n",
       " 'status:needs_votes',\n",
       " 'streaming',\n",
       " 'studio',\n",
       " 'support',\n",
       " 't-must-fix',\n",
       " 't-nice-to-have-fix',\n",
       " 't-unknown-sub-error',\n",
       " 'task',\n",
       " 'technical debt',\n",
       " 'test',\n",
       " 'tests',\n",
       " 'time_series',\n",
       " 'timecodes',\n",
       " 'to refine',\n",
       " 'todo',\n",
       " 'tooling and CI',\n",
       " 'topic:dependencies',\n",
       " 'topic:eval',\n",
       " 'topic:models',\n",
       " 'topic:reader',\n",
       " 'training-benchmark',\n",
       " 'triage',\n",
       " 'triage me',\n",
       " 'triage-needed',\n",
       " 'triage/intermediate-priotrity',\n",
       " 'triaged',\n",
       " 'tune',\n",
       " 'type / bug',\n",
       " 'type / code-health',\n",
       " 'type / enhancement',\n",
       " 'type/maintenance',\n",
       " 'type: bug',\n",
       " 'type: docs',\n",
       " 'type: enhancement',\n",
       " 'type: feature request',\n",
       " 'type: question',\n",
       " 'type:bug',\n",
       " 'type:feature',\n",
       " 'type:maintenance',\n",
       " 'type:question',\n",
       " 'up-for-grabs',\n",
       " 'upstream-azml',\n",
       " 'urgent',\n",
       " 'user raised',\n",
       " 'ux',\n",
       " 'v0.8.5',\n",
       " 'waiting',\n",
       " 'waiting feedback',\n",
       " \"won't fix\",\n",
       " 'wontfix',\n",
       " 'work-item',\n",
       " 'workflow',\n",
       " 'working as intended',\n",
       " '✨ feat',\n",
       " '민지',\n",
       " '찬국',\n",
       " '🐛 bug fix',\n",
       " '🐞 bug',\n",
       " '🐥 experiment',\n",
       " '🐺 Tracker',\n",
       " '👨\\u200d👩\\u200d👧\\u200d👧 discussion',\n",
       " '💎 New Component',\n",
       " '📜 Paper',\n",
       " '🔤 named-entity-recognition',\n",
       " '🔥 New Feature',\n",
       " '🛂 checkpoint',\n",
       " '🦉 dvc',\n",
       " '🧪 testing'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = set()\n",
    "for _, row in valid_issues_all['Issue_label'].map(set).items():\n",
    "    final = final.union(row)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out issues that are not related to challenges\n",
    "valid_issues_filtered = []\n",
    "\n",
    "for index, row in valid_issues_all.iterrows():\n",
    "    if not row['Issue_title'].isascii():\n",
    "        continue\n",
    "\n",
    "    break_sign = False\n",
    "    title = row['Issue_title'].lower()\n",
    "    for label_repo in row['Issue_label']:\n",
    "        for issue_label in issue_labels:\n",
    "            if not break_sign and issue_label in label_repo.lower():\n",
    "                valid_issues_filtered.append(row)\n",
    "                break_sign = True\n",
    "            elif not break_sign and issue_label in title:\n",
    "                valid_issues_filtered.append(row)\n",
    "                break_sign = True\n",
    "\n",
    "valid_issues_filtered = pd.concat(valid_issues_filtered, axis=1).T\n",
    "valid_issues_filtered.to_json(os.path.join(\n",
    "    path_github_issue_filtered, 'issues.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine Github and Gitlab issues\n",
    "df_issue_github = pd.read_json(os.path.join(\n",
    "    path_github_issue_filtered, 'issues.json'))\n",
    "df_issue_gitlab = pd.read_json(os.path.join(\n",
    "    path_gitlab_issue_filtered, 'issues.json'))\n",
    "\n",
    "df_issue_github['Platform'] = 'Github'\n",
    "df_issue_gitlab['Platform'] = 'Gitlab'\n",
    "\n",
    "df_issues = pd.concat([df_issue_github, df_issue_gitlab], ignore_index=True)\n",
    "del df_issues['Issue_label']\n",
    "\n",
    "df_issues.to_json(os.path.join(\n",
    "    path_labeling, 'original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content preprocessing patterns\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from string import ascii_lowercase\n",
    "import re\n",
    "\n",
    "regex = r\"(<.*?>)|({.*?})|((!)?\\[.*?\\])|(\\(.*?\\))|(\\`{3}.+?\\`{3})|(\\`{2}.+?\\`{2})|(\\`{1}.+?\\`{1})|([^\\s]*[<=>]=[^\\s]+)|(@[^\\s]+)|((https?:\\/)?\\/[^\\s]+)|([^\\s]*\\\\[^\\s]+)|([^\\s]+\\/[^\\s]+)|([^\\s]+\\.[^\\s]+)|([^\\s]+-[^\\s]+)|([^\\s]+_[^\\s]+)|(_+[^\\s]+_*)|(_*[^\\s]+_+)|([0-9\\|\\-\\r\\n\\t\\\"\\-#*=~:{}\\(\\)\\[\\]<>]+)\"\n",
    "\n",
    "len_max = len('xxxxxxxxxxxxxxxx')\n",
    "\n",
    "def preprocess_text(text, remove_code=False):\n",
    "    text = text.lower().encode('ascii', errors='ignore').decode('ascii')\n",
    "\n",
    "    for tool_keywords in tools_keywords.values():\n",
    "        for tool_keyword in tool_keywords:\n",
    "            if tool_keyword in text:\n",
    "                text = text.replace(tool_keyword, '')\n",
    "\n",
    "    text = re.sub(regex, ' ', text, 0, re.DOTALL) if remove_code else text\n",
    "    \n",
    "    # remove repeated letters\n",
    "    for time in range(3, len_max + 1):\n",
    "        for letter in ascii_lowercase:\n",
    "            text = text.replace(letter * time, '')\n",
    "            \n",
    "    text = preprocess_string(text)\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt for gpt model\n",
    "\n",
    "import random\n",
    "\n",
    "prompt_issue = 'Your task is to provide a brief and accurate summary of the challenges that the user has encountered based on the given text. Your summary should be concise, highlighting only the most important details related to the challenges faced by the user. Please note that your response should focus on providing an objective and factual summary of the challenges without including any personal opinions or biases.\\n###'\n",
    "prompt_fix = 'Given a challenge-discussion pair, please extract any possible solutions mentioned in the discussion and provide a brief and accurate summary of them. If no solution is mentioned, please indicate that there are no solutions provided. Please note that your response should focus on providing an objective and factual summary without including any personal opinions or biases.\\n###'\n",
    "\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0, 1)\n",
    "            time.sleep(sleep)\n",
    "            x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add potential field to issues for later filling\n",
    "df_issues = pd.read_json(os.path.join(path_labeling, 'original.json'))\n",
    "\n",
    "df_issues['Issue_original_content'] = ''\n",
    "df_issues['Issue_preprocessed_content'] = ''\n",
    "df_issues['Issue_gpt_summary_original'] = ''\n",
    "df_issues['Issue_gpt_summary'] = ''\n",
    "df_issues['Fix_original_content'] = ''\n",
    "df_issues['Fix_preprocessed_content'] = ''\n",
    "df_issues['Fix_gpt_summary_original'] = ''\n",
    "df_issues['Fix_gpt_summary'] = ''\n",
    "\n",
    "df_issues.to_json(os.path.join(path_labeling,\n",
    "                  'issues.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modulenotfounderror modul name tensorboard \n",
      "combin param param work \n",
      "error load \n",
      "deploy fail \n",
      "log val loss \n",
      "fix import issu \n",
      "logger \n"
     ]
    }
   ],
   "source": [
    "# Experiment 1\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    issue = preprocess_text(row['Issue_title']) + ' ' + preprocess_text(str(row['Issue_body']))\n",
    "    \n",
    "    if len(issue.split()) < 6 or len(issue) < 30:\n",
    "        df_issues.drop(index, inplace=True)\n",
    "        print(issue)\n",
    "    else:\n",
    "        df_issues.at[index, 'Issue_original_content'] = issue\n",
    "\n",
    "df_issues.to_json(os.path.join(path_labeling,\n",
    "                  'issues.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persisting on question 49\n",
      "persisting on question 99\n",
      "persisting on question 149\n",
      "persisting on question 199\n",
      "persisting on question 249\n",
      "persisting on question 299\n",
      "persisting on question 349\n",
      "persisting on question 399\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    if index % 50 == 49:\n",
    "        print(f'persisting on question {index}')\n",
    "        df_issues.to_json(os.path.join(\n",
    "            path_labeling, 'issues.json'), indent=4, orient='records')\n",
    "\n",
    "    if row['Issue_gpt_summary_original']:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prompt = prompt_issue + 'Title: ' + row['Issue_title'] + ' Body: ' + row['Issue_body'] + '###\\n'\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an accurate summarizer.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=150,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=10,\n",
    "            stream=False\n",
    "        )\n",
    "        content = response['choices'][0]['message']['content'].strip()\n",
    "        df_issues.at[index, 'Issue_gpt_summary_original'] = content\n",
    "        df_issues.at[index, 'Issue_gpt_summary'] = preprocess_text(content)\n",
    "    except Exception as e:\n",
    "        # output unsuccesful requests\n",
    "        print(f'{e} on issue {row[\"Issue_link\"]}')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df_issues.to_json(os.path.join(\n",
    "    path_labeling, 'issues.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "assert df_issues.shape[0] == df_issues[df_issues['Issue_gpt_summary_original'].str.len() > 0].shape[0]\n",
    "\n",
    "# output the number of asset-management-related Git issues\n",
    "len(df_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    content = preprocess_text(row['Issue_title'], remove_code=True) + ' ' + preprocess_text(str(row['Issue_body']), remove_code=True)\n",
    "    df_issues.at[index, 'Issue_preprocessed_content'] = content\n",
    "\n",
    "df_issues.to_json(os.path.join(\n",
    "    path_labeling, 'issues.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    discussion = ' '.join(row['Answer_list'])\n",
    "    df_issues.at[index, 'Fix_original_content'] = preprocess_text(discussion)\n",
    "\n",
    "df_issues.to_json(os.path.join(path_labeling,\n",
    "                  'issues.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persisting on question 49\n",
      "persisting on question 99\n",
      "persisting on question 149\n",
      "persisting on question 199\n",
      "persisting on question 249\n",
      "persisting on question 299\n",
      "persisting on question 349\n",
      "persisting on question 399\n"
     ]
    }
   ],
   "source": [
    "# Experiment 5\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    if index % 50 == 49:\n",
    "        print(f'persisting on question {index}')\n",
    "        df_issues.to_json(os.path.join(\n",
    "            path_labeling, 'issues.json'), indent=4, orient='records')\n",
    "        \n",
    "    discussion = ' '.join(row['Answer_list'])\n",
    "\n",
    "    if not discussion or row['Fix_gpt_summary_original']:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prompt = prompt_fix + 'Challenge: ' + row['Issue_gpt_summary_original'] + ' Discussion: ' + discussion + '###\\n'\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an accurate summarizer.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=150,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=10,\n",
    "            stream=False\n",
    "        )\n",
    "        content = response['choices'][0]['message']['content'].strip()\n",
    "        df_issues.at[index, 'Fix_gpt_summary_original'] = content\n",
    "        df_issues.at[index, 'Fix_gpt_summary'] = preprocess_text(content)\n",
    "    except Exception as e:\n",
    "        # output unsuccesful requests\n",
    "        print(f'{e} on issue {row[\"Issue_link\"]}')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df_issues.to_json(os.path.join(\n",
    "    path_labeling, 'issues.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That number is actually an Epoch Time value, so it looks like a newer version of the same Role was generated. I'm not sure whether that is happening manually or programatically for your specific instance, but either way you should verify that the new Role has all the same IAM permissions as the old role. I'm not sure what you mean when you say that the role could be updated manually vs programatically. We basically set this system up and let it just run without any administrative interventions  outside the SWB ui for many months-- The only thing we've updated is the study data source, adding a few users, etc. Do you mean when the stack is updated in the SWB UI it's considered manual?  The old stack fs role no longer exists, even though it's still noted in the stack S3mounts parameters. That's the key issue and why permission has been denied. The new stack has the new role, and so it's able to assume the role and sync the files. The sync for the old stack did work when it was first launched, so no permissions errors should have been present in the role itself before it was somehow replaced.  So the key is, that at some point the private workspace study role has been deleted and a new role is made. This doesn't happen for all studies, as the read-only study retained it's role. Since the sagemaker instance was provisioned, were there any changes made to the external data source, like importing new folders from that bucket, or updating the associated CFN template in any way? We have added more studies to the data source associated with that bucket, since the instance was launched. When we have new students, we add a folder in the bucket, then add that folder into the data source as a new study, which requires an update stack operation when the data source changes. For reference, we did update the data source on the 15th through the UI by adding another folder, but we did not push the new stack until the 16th.  We noticed this mount\\\\/sync error on the 16th, and we pushed out the stack changes and created a new instance to see if not having pushed that stack update was the root of the issue. However, after the stack update the new instances still had this issue. Thanks for the information! I believe adding the new data source replaced the active filesystem role, irregardless of the old one being used since the timestamp on the new filesystem role matches the time you edited the data sources. I will add a backlog item for this fixing this problem.  Hi all, just wanted to add some additional context -  I also see this issue in RStudio EC2 instances (SWB v5.0.0). When permissions for a linked data source are updated, the RStudio instance no longer syncs to that data source. I'm not sure why the original `fs-role` disappeared. Those roles are unique per byobStudyId-envStudyPermission-hostingAccount combination and only get deleted when all workspaces on a hosting account mounting that study with that permission are terminated (stopping shouldn't have done this).  To unblock your work, could you perform the following steps: 1. Copy the `studies` folder into a new directory in that workspace, to preserve the last state locally. 2. Create the missing role in IAM and copy over all policies from the new one 3. Delete the unsynced study folder in `studies`, and run the `mount_s3.sh` script on the instance  Note: However if there were files placed inside the study folder before stopping that instance that never got synced to S3, they probably cannot be recovered now even after remounting the study. While this will probably work, we have > 30 instances that these individual folders are associated with, in the same bucket. The syncing issue happened to most of them and it would be quite tedious to manually apply this change to all of them.   Were you unable to reproduce this error? I'll run a few tests to see if this is still effecting us (with the new Sagemaker auto-stop and credentials changes from v5.2.5). Curious to know where this has ended up in the backlog. We're still unable to use SWB meaningfully in production with this particular bug. (As we have many instances using a data source, and changes to a data source stack are likely, we can't manually fix the issue for `n` instances every time we need to update a data source.)  Additionally, I wanted to note that our developers cannot access the database or AWS permissions for our applications in our FISMA prod environment. There's a formal review process for changes there, and this makes manual intervention an untenable option for us. The role creation step in the workaround does not need to be done for each instance, it only needs to be done once per filesystem (fs) role that has disappeared (filesystem roles are shared based on the permission levels they grant access). This can be done by the IT admin who has access to the BYOB account (where the fs role needs to be created). Only steps 1 and 3 in the workaround needs to be done per workspace (by the researcher who is logging into it) to attempt remounting the study.  For new instances though, a new filesystem role should be created if a fs role for the respective byobStudyId-envStudyPermission-hostingAccount combination does not already exist. Do you see this issue when creating a new workspace instance with BYOB study mounted? I am not sure I understand your question. I think we're only using the byob study feature to create these instances, and when I test in our dev account we're only using one data source.  I took the time to launch version 5.2.7 from AWS mainline in one of our accounts. I'll show the exact steps I used to reproduce this error, to isolate our forked changes from the issue.   Because we're using this in a FISMA, government regulated environment, we can't just access it whenever we want to update settings\\\\/configs\\\\/permissions on the AWS console without a lot of oversight, and these data sources should be changing as we onboard more users and projects. Hi @srpiatt !   I am looking at this now. I am going to start by following your steps to reproduce as exactly as possible. I will let you know if I have any questions.  I want to acknowledge your comment about FISMA--I'll try to figure out what is going on so you don't have to access the AWS console. Thanks for that context.  Excited to figure this out with you! Mar Hi @srpiatt \\\\/ All , As @maghirardelli shared earlier - The team is currently analyzing the entire setup based on the detailed steps described in the ticket, the findings so far show that the issue isn't reproduced on a fresh new installation of 5.2.7 on our side.  However, we're going to provide a detailed steps of our findings we collected per step to eliminate any potential differences that may have point to the root cause. Additionally, couple of questions that asking for more information for further analysis.  I apologize for not providing this feedback earlier this week but rest assured, the team understands the importance and impact for this permission issue. Let's continue our investigation over this ticket for clear visibility and transparency to all stakeholders. Looking forward for the summary of findings that will be posted shortly Hi @srpiatt !  Thank you so much for the thorough reproduction steps! Sadly, I was unable to reproduce using the steps provided. Here is exactly what I did. Did I perhaps miss a step or misread a step you gave me? Additionally, I have bolded some questions that would help me diagnose when the fs role deletion is happening for you. It requires you reproducing again sadly, but it should get us closer to figuring this out.  * Upgraded an existing environment to tagged release v5.2.7, using my already existing stage file already working for the environment. * I used an existing setup account, index, and project. I updated the permissions on my already-onboarded account to be current (shown as Up-to-Date in SWB Accounts page). * I created a bucket in my bucket account with two folders: ‘study_A’ and ‘study_B’ and SSE-S3 encryption. * Launched sagemaker (ml.t3.medium) with study_A attached.  Tested that instance had study_A folder mounted. Could see fs role in the bucket account.  xCan you see the fs role from the S3 mounts after stopping the instance in the IAM Console in the bucket account?     * After stopping the instance, in DDB Table `<stage>-<regionshortname>-sw-ResourceUsages` in the main account, what is item `RES#roles-only-access-study-study_A||read-true||write-false`? Mine looks like this:  Add new study to existing data source and bucket: study_B. This looks like a Stack Update. CloudFormation Change Set says this is the only change: `arn:aws:iam::<account>:policy\\\\/swb-H0y9ChAeNBwSzWHhtQM4Ze-app-1674668875980`.     * Did you select “Stack Update” in the SWB UI like this? After clicking “Update Stack” in SWB UI and going to CloudFormation, what does the Change Set include at this point for you? This should be displayed right before you click “Update” in CloudFormation.  * Launched second sagemaker, with study_A and study_B  Connected and tested second instance: both study folders mounted. Both fs roles are still visible in the IAM Console. Note: the FS role for study_A is the same as it was for the first instance, unlike your report. * Started first instance and connected     * Instance DOES have access to study_A, unlike your report     * Role that is noted in S3Mounts for the first instance STILL EXISTS in AWS console, unlike your report  Additional questions:  1. Can you run this CloudTrail query in the CloudTrail console (go to CloudTrail>Event History>Lookup attributes): Event name = DeleteRole. Looking at the table, can you find the event where the Resource name column value equals the fs role for Study A that disappeared? In CloudWatch logs, log group `\\\\/aws\\\\/lambda\\\\/<regionshortname>-sw-backend-<stage>-workflowLoopRunner`, searching for “fs”, what are the results? This would have to be in the log stream that captures the time of the instance creation\\\\/stopping\\\\/creation again like in the reproduction steps.  Answers to these questions will help me narrow down when\\\\/why this is happening for you! I look forward to hearing from you to continue investigating!  Thanks, Mar  Thanks for the response! I'm trying out your steps today and documenting for the questions you had. I'll get back to you shortly. :) Good morning @srpiatt, Please let us know if you're able to get some findings following @maghirardelli's questions. Good morning @srpiatt!   Were you able to get any more information from these steps?  Thanks, Mar Good morning! I haven't been able to reproduce this issue again, although I have been trying for several days. Yesterday, I was able to find the cloudtrail event for the instance that took place on the 20th, which I'll put below. I'll continue to try to reproduce, so I can get the details for you, but as it is intermittent it could take a while to show up (and it's probably why you haven't been able to easily reproduce). As soon as it happens again for me, I'll fill in all the questions from the steps that cause it, and I'll leave the system in that state so we can do any debugging sessions you want.   Hi @srpiatt , thank you for the information you have shared with us, this sounds like a great plan. I look forward hearing back form you about the details I attempted to reproduce this many times, but I was unable to reproduce with these steps. Frustrated that I felt insane, I dove into the code to see if I can trace the ways that permissions are getting added and removed. I looked up the CloudTrail event you noted, the DeleteRole event, and found all instances that create that event, then walked back where they originate. I noticed a couple comments in one of the files, https:\\\\/\\\\/github.com\\\\/awslabs\\\\/service-workbench-on-aws\\\\/blob\\\\/mainline\\\\/addons\\\\/addon-base-raas\\\\/packages\\\\/base-raas-services\\\\/lib\\\\/study\\\\/study-operation-service.js#L160-L164:  > We start by de-allocating previously allocated resources for the study (such as filesystem roles, and certain statements from the bucket policy). We do that even if the user was not removed from the study. Doing this logic makes the whole logic of propagating permissions much easier.  So I did all the reproduction steps you noted, but with the addition of changing the permissions (at the end). And that reproduced the error. So it's somewhere in that area of the code that's causing this. It's possible that the time above that I was able to reproduce, I missed recording this critical difference.  Below are the answers to your questions, which resulted in no reproduction, with the addition of study permission changes near the end that did reproduce this issue.  ----  -   I created a bucket in my bucket account with two folders: ‘study_A’ and ‘study_B’ and SSE-S3 encryption. \\t- I am using no encryption for these folders -   Launched sagemaker (ml.m5.large) with study_A attached.  Then stopped instance. \\t-  IAM fs role DDB Table\\xa0`<stage>-<regionshortname>-sw-ResourceUsages` value for `RES#roles-only-access-study-study_A||read-true||write-false`?  Add new study to existing data source and bucket: study_B     - Stack update changeset Launched second sagemaker, with study_A and study_B  Connected and tested second instance: both study folders mounted. \\t- IAM fs role Started first instance and connected: study_A folder is mounted     -   AIM fs role No reproduction to this point.  ----  Additional permission steps: _Both first and second instance were launched by root user._ - Stopped first instance-- only second instance is on (with Study_A and stuby_B mounted)     - IAM fs roles Edited study_A: move root admin user to read\\\\/write user.     - Admin       - before: root       - after: none     - Read\\\\/Write       - before: none       - after: root  - Role is now replaced. Instance two (with study_A & study_B) which was turned on, now shows permission error for study_A  CloudTrail DeleteRole event   started instance one \\t- instance one (with only study_A) does not mount the study If you update BYOB study permissions while other workspaces are using it (active or stopped), those workspaces will lose access to that study. This is documented in `docs\\\\/docs\\\\/user_guide\\\\/sidebar\\\\/common\\\\/studies\\\\/studies_page.md` and is a known limitation of the feature. Try terminating your environment, then make the changes, and then create a new workspace and see if everything works as expected. Thanks! Hi! Since we have established this is a documented limitation of BYOB, I am moving this ticket to closing soon if no response and will resolve in a week!  Thanks, Marianna Closing due to inactivity!\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 'That number is actually an Epoch Time value, so it looks like a newer version of the same Role was generated. I\\'m not sure whether that is happening manually or programatically for your specific instance, but either way you should verify that the new Role has all the same IAM permissions as the old role. I\\'m not sure what you mean when you say that the role could be updated manually vs programatically. We basically set this system up and let it just run without any administrative interventions  outside the SWB ui for many months-- The only thing we\\'ve updated is the study data source, adding a few users, etc. Do you mean when the stack is updated in the SWB UI it\\'s considered manual?\\r\\n\\r\\nThe old stack fs role no longer exists, even though it\\'s still noted in the stack S3mounts parameters. That\\'s the key issue and why permission has been denied. The new stack has the new role, and so it\\'s able to assume the role and sync the files. The sync for the old stack did work when it was first launched, so no permissions errors should have been present in the role itself before it was somehow replaced.\\r\\n\\r\\nSo the key is, that at some point the private workspace study role has been deleted and a new role is made. This doesn\\'t happen for all studies, as the read-only study retained it\\'s role. Since the sagemaker instance was provisioned, were there any changes made to the external data source, like importing new folders from that bucket, or updating the associated CFN template in any way? We have added more studies to the data source associated with that bucket, since the instance was launched. When we have new students, we add a folder in the bucket, then add that folder into the data source as a new study, which requires an update stack operation when the data source changes. For reference, we did update the data source on the 15th through the UI by adding another folder, but we did not push the new stack until the 16th.  We noticed this mount\\\\/sync error on the 16th, and we pushed out the stack changes and created a new instance to see if not having pushed that stack update was the root of the issue. However, after the stack update the new instances still had this issue. Thanks for the information! I believe adding the new data source replaced the active filesystem role, irregardless of the old one being used since the timestamp on the new filesystem role matches the time you edited the data sources. I will add a backlog item for this fixing this problem.  Hi all, just wanted to add some additional context - \\r\\nI also see this issue in RStudio EC2 instances (SWB v5.0.0). When permissions for a linked data source are updated, the RStudio instance no longer syncs to that data source. I\\'m not sure why the original `fs-role` disappeared. Those roles are unique per byobStudyId-envStudyPermission-hostingAccount combination and only get deleted when all workspaces on a hosting account mounting that study with that permission are terminated (stopping shouldn\\'t have done this).\\r\\n\\r\\nTo unblock your work, could you perform the following steps:\\r\\n1. Copy the `studies` folder into a new directory in that workspace, to preserve the last state locally.\\r\\n2. Create the missing role in IAM and copy over all policies from the new one\\r\\n3. Delete the unsynced study folder in `studies`, and run the `mount_s3.sh` script on the instance\\r\\n\\r\\nNote: However if there were files placed inside the study folder before stopping that instance that never got synced to S3, they probably cannot be recovered now even after remounting the study. While this will probably work, we have > 30 instances that these individual folders are associated with, in the same bucket. The syncing issue happened to most of them and it would be quite tedious to manually apply this change to all of them. \\r\\n\\r\\nWere you unable to reproduce this error? I\\'ll run a few tests to see if this is still effecting us (with the new Sagemaker auto-stop and credentials changes from v5.2.5). Curious to know where this has ended up in the backlog. We\\'re still unable to use SWB meaningfully in production with this particular bug. (As we have many instances using a data source, and changes to a data source stack are likely, we can\\'t manually fix the issue for `n` instances every time we need to update a data source.)\\r\\n\\r\\nAdditionally, I wanted to note that our developers cannot access the database or AWS permissions for our applications in our FISMA prod environment. There\\'s a formal review process for changes there, and this makes manual intervention an untenable option for us. The role creation step in the workaround does not need to be done for each instance, it only needs to be done once per filesystem (fs) role that has disappeared (filesystem roles are shared based on the permission levels they grant access). This can be done by the IT admin who has access to the BYOB account (where the fs role needs to be created).\\r\\nOnly steps 1 and 3 in the workaround needs to be done per workspace (by the researcher who is logging into it) to attempt remounting the study.\\r\\n\\r\\nFor new instances though, a new filesystem role should be created if a fs role for the respective byobStudyId-envStudyPermission-hostingAccount combination does not already exist. Do you see this issue when creating a new workspace instance with BYOB study mounted? I am not sure I understand your question. I think we\\'re only using the byob study feature to create these instances, and when I test in our dev account we\\'re only using one data source.\\r\\n\\r\\nI took the time to launch version 5.2.7 from AWS mainline in one of our accounts. I\\'ll show the exact steps I used to reproduce this error, to isolate our forked changes from the issue.\\r\\n\\r\\n Because we\\'re using this in a FISMA, government regulated environment, we can\\'t just access it whenever we want to update settings\\\\/configs\\\\/permissions on the AWS console without a lot of oversight, and these data sources should be changing as we onboard more users and projects. Hi @srpiatt ! \\r\\n\\r\\nI am looking at this now. I am going to start by following your steps to reproduce as exactly as possible. I will let you know if I have any questions.\\r\\n\\r\\nI want to acknowledge your comment about FISMA--I\\'ll try to figure out what is going on so you don\\'t have to access the AWS console. Thanks for that context.\\r\\n\\r\\nExcited to figure this out with you!\\r\\nMar Hi @srpiatt \\\\/ All ,\\r\\nAs @maghirardelli shared earlier - The team is currently analyzing the entire setup based on the detailed steps described in the ticket, the findings so far show that the issue isn\\'t reproduced on a fresh new installation of 5.2.7 on our side. \\r\\nHowever, we\\'re going to provide a detailed steps of our findings we collected per step to eliminate any potential differences that may have point to the root cause. Additionally, couple of questions that asking for more information for further analysis.\\r\\n\\r\\nI apologize for not providing this feedback earlier this week but rest assured, the team understands the importance and impact for this permission issue. Let\\'s continue our investigation over this ticket for clear visibility and transparency to all stakeholders.\\r\\nLooking forward for the summary of findings that will be posted shortly Hi @srpiatt !\\r\\n\\r\\nThank you so much for the thorough reproduction steps! Sadly, I was unable to reproduce using the steps provided. Here is exactly what I did. Did I perhaps miss a step or misread a step you gave me? Additionally, I have bolded some questions that would help me diagnose when the fs role deletion is happening for you. It requires you reproducing again sadly, but it should get us closer to figuring this out.\\r\\n\\r\\n* Upgraded an existing environment to tagged release v5.2.7, using my already existing stage file already working for the environment.\\r\\n* I used an existing setup account, index, and project. I updated the permissions on my already-onboarded account to be current (shown as Up-to-Date in SWB Accounts page).\\r\\n* I created a bucket in my bucket account with two folders: ‘study_A’ and ‘study_B’ and SSE-S3 encryption.\\r\\n* Launched sagemaker (ml.t3.medium) with study_A attached.\\r\\n Tested that instance had study_A folder mounted. Could see fs role in the bucket account. \\r\\nx**Can you see the fs role from the S3 mounts after stopping the instance in the IAM Console in the bucket account?**\\r\\n    * **After stopping the instance, in DDB Table `<stage>-<regionshortname>-sw-ResourceUsages` in the main account, what is item `RES#roles-only-access-study-study_A||read-true||write-false`?** Mine looks like this:\\r\\n Add new study to existing data source and bucket: study_B. This looks like a Stack Update. CloudFormation Change Set says this is the only change: `arn:aws:iam::<account>:policy\\\\/swb-H0y9ChAeNBwSzWHhtQM4Ze-app-1674668875980`.\\r\\n    * **Did you select “Stack Update” in the SWB UI like this?**\\r\\n**After clicking “Update Stack” in SWB UI and going to CloudFormation, what does the Change Set include at this point for you? This should be displayed right before you click “Update” in CloudFormation.**\\r\\n\\r\\n* Launched second sagemaker, with study_A and study_B\\r\\n Connected and tested second instance: both study folders mounted. Both fs roles are still visible in the IAM Console. Note: the FS role for study_A is the same as it was for the first instance, unlike your report.\\r\\n* Started first instance and connected\\r\\n    * Instance DOES have access to study_A, unlike your report\\r\\n    * Role that is noted in S3Mounts for the first instance STILL EXISTS in AWS console, unlike your report\\r\\n\\r\\n**Additional questions:**\\r\\n\\r\\n1. **Can you run this CloudTrail query in the CloudTrail console (go to CloudTrail>Event History>Lookup attributes): Event name = DeleteRole. Looking at the table, can you find the event where the Resource name column value equals the fs role for Study A that disappeared?**\\r\\n**In CloudWatch logs, log group `\\\\/aws\\\\/lambda\\\\/<regionshortname>-sw-backend-<stage>-workflowLoopRunner`, searching for “fs”, what are the results? This would have to be in the log stream that captures the time of the instance creation\\\\/stopping\\\\/creation again like in the reproduction steps.**\\r\\n\\r\\nAnswers to these questions will help me narrow down when\\\\/why this is happening for you! I look forward to hearing from you to continue investigating!\\r\\n\\r\\nThanks,\\r\\nMar\\r\\n Thanks for the response! I\\'m trying out your steps today and documenting for the questions you had. I\\'ll get back to you shortly. :) Good morning @srpiatt,\\r\\nPlease let us know if you\\'re able to get some findings following @maghirardelli\\'s questions. Good morning @srpiatt! \\r\\n\\r\\nWere you able to get any more information from these steps?\\r\\n\\r\\nThanks,\\r\\nMar Good morning! I haven\\'t been able to reproduce this issue again, although I have been trying for several days. Yesterday, I was able to find the cloudtrail event for the instance that took place on the 20th, which I\\'ll put below. I\\'ll continue to try to reproduce, so I can get the details for you, but as it is intermittent it could take a while to show up (and it\\'s probably why you haven\\'t been able to easily reproduce). As soon as it happens again for me, I\\'ll fill in all the questions from the steps that cause it, and I\\'ll leave the system in that state so we can do any debugging sessions you want.\\r\\n\\r\\n Hi @srpiatt , thank you for the information you have shared with us, this sounds like a great plan. I look forward hearing back form you about the details I attempted to reproduce this many times, but I was unable to reproduce with these steps. Frustrated that I felt insane, I dove into the code to see if I can trace the ways that permissions are getting added and removed. I looked up the CloudTrail event you noted, the DeleteRole event, and found all instances that create that event, then walked back where they originate. I noticed a couple comments in one of the files, https:\\\\/\\\\/github.com\\\\/awslabs\\\\/service-workbench-on-aws\\\\/blob\\\\/mainline\\\\/addons\\\\/addon-base-raas\\\\/packages\\\\/base-raas-services\\\\/lib\\\\/study\\\\/study-operation-service.js#L160-L164:\\r\\n\\r\\n> We start by de-allocating previously allocated resources for the study (such as filesystem roles, and certain statements from the bucket policy). We do that even if the user was not removed from the study. Doing this logic makes the whole logic of propagating permissions much easier.\\r\\n\\r\\nSo I did all the reproduction steps you noted, but with the addition of changing the permissions (at the end). And that reproduced the error. So it\\'s somewhere in that area of the code that\\'s causing this. It\\'s possible that the time above that I was able to reproduce, I missed recording this critical difference.\\r\\n\\r\\nBelow are the answers to your questions, which resulted in no reproduction, with the addition of study permission changes near the end that did reproduce this issue.\\r\\n\\r\\n----\\r\\n\\r\\n-   I created a bucket in my bucket account with two folders: ‘study_A’ and ‘study_B’ and SSE-S3 encryption.\\r\\n\\t- **I am using no encryption for these folders**\\r\\n-   Launched sagemaker (**ml.m5.large**) with study_A attached.\\r\\n Then stopped instance.\\r\\n\\t-  IAM fs role DDB Table\\xa0`<stage>-<regionshortname>-sw-ResourceUsages` value for `RES#roles-only-access-study-study_A||read-true||write-false`?\\r\\n Add new study to existing data source and bucket: study_B\\r\\n    - Stack update changeset Launched second sagemaker, with study_A and study_B\\r\\n Connected and tested second instance: both study folders mounted.\\r\\n\\t- IAM fs role Started first instance and connected: study_A folder is mounted\\r\\n    -   AIM fs role No reproduction to this point.\\r\\n\\r\\n----\\r\\n\\r\\n**Additional permission steps:**\\r\\n_Both first and second instance were launched by root user._\\r\\n- Stopped first instance-- only second instance is on (with Study_A and stuby_B mounted)\\r\\n    - IAM fs roles Edited study_A: move root admin user to read\\\\/write user.\\r\\n    - Admin\\r\\n      - before: root\\r\\n      - after: none\\r\\n    - Read\\\\/Write\\r\\n      - before: none\\r\\n      - after: root\\r\\n - Role is now replaced. Instance two (with study_A & study_B) which was turned on, now shows permission error for study_A \\r\\nCloudTrail DeleteRole event\\r\\n\\r\\n started instance one\\r\\n\\t- instance one (with only study_A) does not mount the study If you update BYOB study permissions while other workspaces are using it (active or stopped), those workspaces will lose access to that study. This is documented in `docs\\\\/docs\\\\/user_guide\\\\/sidebar\\\\/common\\\\/studies\\\\/studies_page.md` and is a known limitation of the feature. Try terminating your environment, then make the changes, and then create a new workspace and see if everything works as expected. Thanks! Hi! Since we have established this is a documented limitation of BYOB, I am moving this ticket to closing soon if no response and will resolve in a week!\\r\\n\\r\\nThanks,\\r\\nMarianna Closing due to inactivity!'\n",
    "answer = t.replace('**', '').replace('\\r\\n', ' ')\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    if row['Fix_gpt_summary']:\n",
    "        continue\n",
    "    df_issues.at[index, 'Fix_gpt_summary'] = preprocess_text(row['Fix_gpt_summary_original'])\n",
    "\n",
    "df_issues.to_json(os.path.join(\n",
    "    path_labeling, 'issues.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    answers = ' '.join(row['Answer_list'])\n",
    "    df_issues.at[index, 'Fix_preprocessed_content'] = preprocess_text(answers, remove_code=True)\n",
    "\n",
    "df_issues.to_json(os.path.join(\n",
    "    path_labeling, 'issues.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size is based on the recommendation from https://www.calculator.net/sample-size-calculator.html\n",
    "\n",
    "sample_size = 197\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'issues.json'))\n",
    "\n",
    "df_sample = df_issues[df_issues['Issue_gpt_summary'].str.len() > 0].sample(n=sample_size, random_state=42)\n",
    "\n",
    "df_sample.to_json(os.path.join(\n",
    "    path_labeling, 'sample.json'), indent=4, orient='records')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

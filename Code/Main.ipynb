{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "from Scrape.GHMiner import GitHubMiner\n",
    "from Scrape.GLMiner import GitLabMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../Dataset'\n",
    "\n",
    "path_github = os.path.join(path_dataset, 'GitHub')\n",
    "path_gitlab = os.path.join(path_dataset, 'GitLab')\n",
    "path_labeling = os.path.join(path_dataset, 'Labeling')\n",
    "\n",
    "path_github_repo = os.path.join(path_github, 'Repo')\n",
    "path_gitlab_repo = os.path.join(path_gitlab, 'Repo')\n",
    "path_github_discussion = os.path.join(path_github, 'Discussion')\n",
    "path_github_issue = os.path.join(path_github, 'Issue')\n",
    "path_gitlab_issue = os.path.join(path_gitlab, 'Issue')\n",
    "\n",
    "if not os.path.exists(path_github):\n",
    "    os.makedirs(path_github)\n",
    "\n",
    "if not os.path.exists(path_gitlab):\n",
    "    os.makedirs(path_gitlab)\n",
    "\n",
    "if not os.path.exists(path_labeling):\n",
    "    os.makedirs(path_labeling)\n",
    "\n",
    "if not os.path.exists(path_github_repo):\n",
    "    os.makedirs(path_github_repo)\n",
    "\n",
    "if not os.path.exists(path_gitlab_repo):\n",
    "    os.makedirs(path_gitlab_repo)\n",
    "\n",
    "if not os.path.exists(path_github_discussion):\n",
    "    os.makedirs(path_github_discussion)\n",
    "\n",
    "if not os.path.exists(path_github_issue):\n",
    "    os.makedirs(path_github_issue)\n",
    "\n",
    "if not os.path.exists(path_gitlab_issue):\n",
    "    os.makedirs(path_gitlab_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_miner = GitHubMiner(private_token=os.getenv('GITHUB_TOKEN'))\n",
    "gitlab_miner = GitLabMiner(private_token=os.getenv('GITLAB_TOKEN'))\n",
    "\n",
    "tools_repo = {\n",
    "    'Aim': 'aimhubio/aim',\n",
    "    'Amazon SageMaker': 'aws/sagemaker-python-sdk',\n",
    "    'Azure Machine Learning': 'Azure/azure-sdk-for-python',\n",
    "    'ClearML': 'allegroai/clearml',\n",
    "    'Codalab': 'codalab/codalab-worksheets',\n",
    "    'DVC': 'iterative/dvc',\n",
    "    'Determined': 'determined-ai/determined',\n",
    "    'Domino': 'dominodatalab/python-domino',\n",
    "    'Guild AI': 'guildai/guildai',\n",
    "    'Kedro': 'kedro-org/kedro',\n",
    "    'MLflow': 'mlflow/mlflow',\n",
    "    'MLRun': 'mlrun/mlrun',\n",
    "    'ModelDB': 'VertaAI/modeldb',\n",
    "    'Neptune': 'neptune-ai/neptune-client',\n",
    "    'Optuna': 'optuna/optuna',\n",
    "    'Polyaxon': 'polyaxon/polyaxon',\n",
    "    'Sacred': 'IDSIA/sacred',\n",
    "    'Valohai': 'valohai/valohai-cli',\n",
    "    'Weights & Biases': 'wandb/wandb'\n",
    "}\n",
    "\n",
    "tools_release_date = {\n",
    "    'Amazon SageMaker': '2017-11-19',\n",
    "    'Azure Machine Learning': '2015-02-18',\n",
    "    'cnvrg.io': '2020-03-31',\n",
    "    'Comet': '2017-01-01',\n",
    "    'Iterative Studio': '2021-05-12',\n",
    "    'Polyaxon': '2018-10-16',\n",
    "    'SigOpt': '2014-11-01',\n",
    "    'Vertex AI': '2019-03-01'\n",
    "}\n",
    "\n",
    "tools_link = {\n",
    "    'cnvrg.io': 'https://github.com/cnvrg',\n",
    "    'Comet': 'https://github.com/comet-ml',\n",
    "    'Iterative Studio': 'https://studio.iterative.ai',\n",
    "    'SigOpt': 'https://github.com/sigopt',\n",
    "    'Vertex AI': 'https://cloud.google.com/vertex-ai'\n",
    "}\n",
    "\n",
    "tools_keywords = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['sagemaker'],\n",
    "    'Azure Machine Learning': ['azure machine learning', 'azure ml', 'azure-ml', 'azureml'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild ai', 'guild-ai', 'guildai'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['vertex ai', 'vertex-ai', 'vertexai'],\n",
    "    'Weights & Biases': ['weights and biases', 'wandb', 'weights & biases', 'weights&biases', 'w & b', 'w&b']\n",
    "}\n",
    "\n",
    "issue_labels = {\n",
    "    'bug',\n",
    "    'crash',\n",
    "    'error',\n",
    "    'invalid',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_data = pd.DataFrame()\n",
    "\n",
    "# scrape open-source asset-management tools\n",
    "for tool_name, tool_repo in tools_repo.items():\n",
    "    if tool_name in tools_release_date:\n",
    "        tool_data = github_miner.scrape_repo(repo_name=tool_repo, real_name=tool_name, release_time=pd.to_datetime(tools_release_date[tool_name]))\n",
    "    else:\n",
    "        tool_data = github_miner.scrape_repo(repo_name=tool_repo, real_name=tool_name)\n",
    "\n",
    "    if not tool_data.empty:\n",
    "        tools_data = pd.concat([tools_data, tool_data], ignore_index=True)\n",
    "\n",
    "# add closed-source asset-management tools\n",
    "for tool_name in tools_link.keys():\n",
    "    tool_data = {\n",
    "        'Name': tool_name,\n",
    "        'Link': tools_link[tool_name],\n",
    "        'First Release Date': pd.to_datetime(tools_release_date[tool_name])\n",
    "    }\n",
    "    tool_data = pd.DataFrame([tool_data])\n",
    "    tools_data = pd.concat([tools_data, tool_data], ignore_index=True)\n",
    "\n",
    "tools_data.to_json(os.path.join(path_dataset, 'Tools.json'),\n",
    "                   indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37782\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "github_dependents = {}\n",
    "gitlab_dependents = {}\n",
    "\n",
    "# collect dependents for tools with coding patterns\n",
    "for tool_name in tools_keywords.keys():\n",
    "    # collect Github dependents\n",
    "    file_name = os.path.join(path_github_repo, tool_name + '.json')\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, encoding='utf8') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            # either search by sourcegraph\n",
    "            if 'Results' in json_data:\n",
    "                for repo_file in json_data['Results']:\n",
    "                    # file name match pattern\n",
    "                    if 'FileMatch' == repo_file['__typename'] and repo_file['repository']['name'].startswith('github'):\n",
    "                        repo_name = repo_file['repository']['name'].removeprefix(\n",
    "                            'github.com/')\n",
    "                        if repo_name in github_dependents:\n",
    "                            github_dependents[repo_name].append(tool_name)\n",
    "                        else:\n",
    "                            github_dependents[repo_name] = [tool_name]\n",
    "                    # code usage match pattern\n",
    "                    elif 'Repository' == repo_file['__typename'] and repo_file['name'].startswith('github'):\n",
    "                        repo_name = repo_file['name'].removeprefix(\n",
    "                            'github.com/')\n",
    "                        if repo_name in github_dependents:\n",
    "                            github_dependents[repo_name].append(tool_name)\n",
    "                        else:\n",
    "                            github_dependents[repo_name] = [tool_name]\n",
    "            # or search by dependent graph\n",
    "            elif 'all_public_dependent_repos' in json_data:\n",
    "                for repo_file in json_data['all_public_dependent_repos']:\n",
    "                    repo_name = repo_file['name']\n",
    "                    if repo_name in github_dependents:\n",
    "                        github_dependents[repo_name].append(tool_name)\n",
    "                    else:\n",
    "                        github_dependents[repo_name] = [tool_name]\n",
    "\n",
    "    # collect Gitlab dependents\n",
    "    file_name = os.path.join(path_gitlab_repo, tool_name + '.json')\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, encoding='utf8') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            # search by sourcegraph exclusively\n",
    "            for repo_file in json_data['Results']:\n",
    "                # file name match pattern\n",
    "                if 'FileMatch' == repo_file['__typename'] and repo_file['repository']['name'].startswith('gitlab'):\n",
    "                    repo_name = repo_file['repository']['name'].removeprefix(\n",
    "                        'gitlab.com/')\n",
    "                    if repo_name in gitlab_dependents:\n",
    "                        gitlab_dependents[repo_name].append(tool_name)\n",
    "                    else:\n",
    "                        gitlab_dependents[repo_name] = [tool_name]\n",
    "                # code usage match pattern\n",
    "                elif 'Repository' == repo_file['__typename'] and repo_file['name'].startswith('gitlab'):\n",
    "                    repo_name = repo_file['name'].removeprefix('gitlab.com/')\n",
    "                    if repo_name in gitlab_dependents:\n",
    "                        gitlab_dependents[repo_name].append(tool_name)\n",
    "                    else:\n",
    "                        gitlab_dependents[repo_name] = [tool_name]\n",
    "\n",
    "    # remove tool repo from dependents if any\n",
    "    if tool_name in tools_repo and tools_repo[tool_name] in github_dependents:\n",
    "        github_dependents.pop(tools_repo[tool_name], None)\n",
    "\n",
    "with open(os.path.join(path_github_repo, 'Dependents.pickle'), 'wb') as file:\n",
    "    pickle.dump(github_dependents, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(path_gitlab_repo, 'Dependents.pickle'), 'wb') as file:\n",
    "    pickle.dump(gitlab_dependents, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(len(github_dependents))\n",
    "print(len(gitlab_dependents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#GitHub Dependents</th>\n",
       "      <th>#GitLab Dependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>19952</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optuna</td>\n",
       "      <td>6370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DVC</td>\n",
       "      <td>6098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sacred</td>\n",
       "      <td>1918</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLflow</td>\n",
       "      <td>1573</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kedro</td>\n",
       "      <td>1184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>1113</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>826</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Comet</td>\n",
       "      <td>678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ClearML</td>\n",
       "      <td>498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Neptune</td>\n",
       "      <td>422</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aim</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SigOpt</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Guild AI</td>\n",
       "      <td>67</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Codalab</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Polyaxon</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Determined</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Valohai</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLRun</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ModelDB</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Domino</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tool #GitHub Dependents #GitLab Dependents\n",
       "2         Weights & Biases              19952                  0\n",
       "4                   Optuna               6370                  0\n",
       "9                      DVC               6098                  0\n",
       "5                   Sacred               1918                  0\n",
       "7                   MLflow               1573                  4\n",
       "1                    Kedro               1184                  0\n",
       "11        Amazon SageMaker               1113                  3\n",
       "6   Azure Machine Learning                826                  0\n",
       "8                    Comet                678                  0\n",
       "3                  ClearML                498                  0\n",
       "12                 Neptune                422                  0\n",
       "0                      Aim                189                  1\n",
       "13               Vertex AI                134                  0\n",
       "10                  SigOpt                 96                  0\n",
       "20                Guild AI                 67                  4\n",
       "17                 Codalab                 40                  0\n",
       "15                Polyaxon                 36                  0\n",
       "18              Determined                 36                  0\n",
       "14                 Valohai                 31                  0\n",
       "16                   MLRun                 27                  0\n",
       "21                 ModelDB                  9                  0\n",
       "19                  Domino                  1                  0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github_repos = {}\n",
    "gitlab_repos = {}\n",
    "\n",
    "for repo_name, tool_list in github_dependents.items():\n",
    "    for tool_name in tool_list:\n",
    "        github_repos[tool_name] = github_repos.get(tool_name, 0) + 1\n",
    "\n",
    "for repo_name, tool_list in gitlab_dependents.items():\n",
    "    for tool_name in tool_list:\n",
    "        gitlab_repos[tool_name] = gitlab_repos.get(tool_name, 0) + 1\n",
    "\n",
    "dependents_summary = pd.DataFrame(columns=['Tool', '#GitHub Dependents', '#GitLab Dependents'])\n",
    "\n",
    "for tool_name, repo_num in github_repos.items():\n",
    "    if tool_name in gitlab_repos:\n",
    "        entry = {'Tool': tool_name, '#GitHub Dependents': repo_num, '#GitLab Dependents': gitlab_repos[tool_name]}\n",
    "    else:\n",
    "        entry = {'Tool': tool_name, '#GitHub Dependents': repo_num, '#GitLab Dependents': 0}\n",
    "    dependents_summary = pd.concat([dependents_summary, pd.DataFrame([entry])], ignore_index=True)\n",
    "\n",
    "dependents_summary = dependents_summary.sort_values(by=['#GitHub Dependents', '#GitLab Dependents'], ascending=False)\n",
    "dependents_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37782\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(path_github_repo, 'Dependents.pickle'), 'rb') as file:\n",
    "    github_dependents = pickle.load(file)\n",
    "    print(len(github_dependents))\n",
    "\n",
    "with open(os.path.join(path_gitlab_repo, 'Dependents.pickle'), 'rb') as file:\n",
    "    gitlab_dependents = pickle.load(file)\n",
    "    print(len(gitlab_dependents))\n",
    "    \n",
    "df_tool = pd.read_json(os.path.join(path_dataset, 'Tools.json'))\n",
    "tools_release_date = pd.Series(pd.to_datetime(df_tool['First Release Date'].values), index=df_tool['Name']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6912"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape discussion posts of Github dependents for each tool\n",
    "\n",
    "df_posts = pd.DataFrame()\n",
    "\n",
    "for repo_name, tool_list in github_dependents.items():\n",
    "    repo_data = github_miner.scrape_repo(repo_name)\n",
    "    if repo_data.empty:\n",
    "        continue\n",
    "    invalid_repo = []\n",
    "    for tool_name in tool_list:\n",
    "        if repo_data['Repo Created Date'].values[0] < tools_release_date[tool_name]:\n",
    "            invalid_repo.append(tool_name)\n",
    "    tool_list = [tool for tool in tool_list if tool not in invalid_repo]\n",
    "    if tool_list:\n",
    "        posts = pd.DataFrame()\n",
    "        try:\n",
    "            posts = github_miner.scrape_discussion(repo_name)\n",
    "        except:\n",
    "            print(f'Crashed repo: {repo_name}')\n",
    "        if posts.empty:\n",
    "            continue\n",
    "        posts['Tools'] = [tool_list for _ in range(len(posts))]\n",
    "        df_posts = pd.concat([df_posts, posts], ignore_index=True)\n",
    "        df_posts.to_json(os.path.join(path_github_discussion, 'raw.json'), indent=4, orient='records')\n",
    "                \n",
    "df_posts.to_json(os.path.join(path_github_discussion, 'raw.json'), indent=4, orient='records')\n",
    "print(len(df_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exclude discussions that are not related to each tool\n",
    "# create discussion dataset\n",
    "\n",
    "df = pd.read_json(os.path.join(path_github_discussion, 'raw.json'))\n",
    "df['Platform'] = 'GitHub Discussion'\n",
    "df_discussion = pd.DataFrame()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tool_list = []\n",
    "    for tool_name in row['Tools']:\n",
    "        for keyword in tools_keywords[tool_name]:\n",
    "            if keyword in row['Question_title'].lower():\n",
    "                tool_list.append(tool_name)\n",
    "                break\n",
    "    if tool_list:\n",
    "        row['Tools'] = tool_list\n",
    "        df_discussion = pd.concat([df_discussion, row.to_frame().T], ignore_index=True)\n",
    "\n",
    "df_discussion.to_json(os.path.join(path_labeling, 'discussions.json'), indent=4, orient='records')\n",
    "len(df_discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437652"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape issues of Github dependents for each tool\n",
    "\n",
    "df_issues = pd.DataFrame()\n",
    "\n",
    "for repo_name, tool_list in github_dependents.items():\n",
    "    repo_data = github_miner.scrape_repo(repo_name)\n",
    "    if repo_data.empty or (repo_data['#Issue'].values[0] - repo_data['#Pull Requests'].values[0] < 1):\n",
    "        continue\n",
    "    invalid_repo = []\n",
    "    for tool_name in tool_list:\n",
    "        if repo_data['Repo Created Date'].values[0] < tools_release_date[tool_name]:\n",
    "            invalid_repo.append(tool_name)\n",
    "    tool_list = [tool for tool in tool_list if tool not in invalid_repo]\n",
    "    if tool_list:\n",
    "        issues = github_miner.scrape_issue(repo_name)\n",
    "        issues['Tools'] = [tool_list for _ in range(len(issues))]\n",
    "        df_issues = pd.concat([df_issues, issues], ignore_index=True)\n",
    "        df_issues.to_json(os.path.join(path_github_issue, 'raw.json'), indent=4, orient='records')\n",
    "    \n",
    "df_issues.to_json(os.path.join(path_github_issue, 'raw.json'), indent=4, orient='records')\n",
    "len(df_issues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7675"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape issues of Gitlab dependents for each tool\n",
    "\n",
    "df_issues = pd.DataFrame()\n",
    "\n",
    "for repo_name, tool_list in github_dependents.items():\n",
    "    repo_data = gitlab_miner.scrape_repo(repo_name)\n",
    "    if repo_data.empty or (repo_data['#Issue'].values[0] < 1):\n",
    "        continue\n",
    "    invalid_repo = []\n",
    "    for tool_name in tool_list:\n",
    "        if repo_data['Repo Created Date'].values[0] < tools_release_date[tool_name]:\n",
    "            invalid_repo.append(tool_name)\n",
    "    tool_list = [tool for tool in tool_list if tool not in invalid_repo]\n",
    "    if tool_list:\n",
    "        issues = gitlab_miner.scrape_issue(repo_name)\n",
    "        issues['Tools'] = [tool_list for _ in range(len(issues))]\n",
    "        df_issues = pd.concat([df_issues, issues], ignore_index=True)\n",
    "        df_issues.to_json(os.path.join(path_gitlab_issue, 'raw.json'), indent=4, orient='records')\n",
    "    \n",
    "df_issues.to_json(os.path.join(path_gitlab_issue, 'raw.json'), indent=4, orient='records')\n",
    "len(df_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4650"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exclude issues that are not related to each tool\n",
    "\n",
    "df_issues_gitlab = pd.read_json(os.path.join(path_gitlab_issue, 'raw.json'))\n",
    "df_issues_github = pd.read_json(os.path.join(path_github_issue, 'raw.json'))\n",
    "\n",
    "df_issues_github['Platform'] = 'GitHub Issue'\n",
    "df_issues_gitlab['Platform'] = 'GitLab Issue'\n",
    "\n",
    "df_issues = pd.DataFrame()\n",
    "\n",
    "for index, row in df_issues_github.iterrows():\n",
    "    tool_list = []\n",
    "    for tool_name in row['Tools']:\n",
    "        for keyword in tools_keywords[tool_name]:\n",
    "            if keyword in row['Issue_title'].lower():\n",
    "                tool_list.append(tool_name)\n",
    "                break\n",
    "    if tool_list:\n",
    "        row['Tools'] = tool_list\n",
    "        df_issues = pd.concat([df_issues, row.to_frame().T], ignore_index=True)\n",
    "\n",
    "for index, row in df_issues_gitlab.iterrows():\n",
    "    tool_list = []\n",
    "    for tool_name in row['Tools']:\n",
    "        for keyword in tools_keywords[tool_name]:\n",
    "            if keyword in row['Issue_title'].lower():\n",
    "                tool_list.append(tool_name)\n",
    "                break\n",
    "    if tool_list:\n",
    "        row['Tools'] = tool_list\n",
    "        df_issues = pd.concat([df_issues, row.to_frame().T], ignore_index=True)\n",
    "\n",
    "len(df_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"experiments\"',\n",
       " '0.4.6',\n",
       " '1 - Critic',\n",
       " '1.1',\n",
       " '1.1.0',\n",
       " '1.4',\n",
       " '1.6',\n",
       " '1.7',\n",
       " '2.0',\n",
       " '3 - Quality of Life',\n",
       " '3rd party',\n",
       " '3rd party update',\n",
       " '3rd-party',\n",
       " ':bridge_at_night:  Bridge',\n",
       " ':bug: bug',\n",
       " ':rotating_light:',\n",
       " 'A: example-get-started',\n",
       " 'A: example-get-started-experiments',\n",
       " 'ADO',\n",
       " 'AI\\u202fFrameworks/ONNX',\n",
       " 'AML Compute Instance',\n",
       " 'API',\n",
       " 'Auto\\u202fML',\n",
       " 'BF',\n",
       " 'Bug',\n",
       " 'CXP Attention',\n",
       " 'CleanUp',\n",
       " 'Client',\n",
       " 'Cloud',\n",
       " 'Community',\n",
       " 'Compute',\n",
       " 'Contributions welcome',\n",
       " 'Core UI',\n",
       " 'DRL',\n",
       " 'Data Labeling',\n",
       " 'Data4ML',\n",
       " 'Data\\u202fDrift',\n",
       " 'Data\\u202fPrep\\u202fServices',\n",
       " 'DeepSpeed',\n",
       " 'Defect',\n",
       " 'Design',\n",
       " 'Design Doc',\n",
       " 'Design: Research',\n",
       " 'Developer experience',\n",
       " 'Documentation',\n",
       " 'Done',\n",
       " 'ERRATA_CANDIDATE',\n",
       " 'Enhancement',\n",
       " 'Environments',\n",
       " 'Evaluation',\n",
       " 'Experiment Tracking',\n",
       " 'Experimentation UI',\n",
       " 'FAQ',\n",
       " 'Feature - Medium Priority',\n",
       " 'Feature request',\n",
       " 'Good First Issue',\n",
       " 'HIGH',\n",
       " 'HPO',\n",
       " 'Hackathon',\n",
       " 'Hyperdrive',\n",
       " 'Important',\n",
       " 'In the roadmap',\n",
       " 'Inf1',\n",
       " 'Inference',\n",
       " 'Ingestion',\n",
       " 'Issue: Bug Report',\n",
       " 'Issue: Feature Request',\n",
       " 'Javascript',\n",
       " 'Kubeflow 1.7',\n",
       " 'L',\n",
       " 'LOE: S',\n",
       " 'Localized',\n",
       " 'MLOps',\n",
       " 'Machine Learning',\n",
       " 'Marathon',\n",
       " 'Mgmt',\n",
       " 'NLP',\n",
       " 'NUM',\n",
       " 'Needs Triage',\n",
       " 'Not related to PyCaret',\n",
       " 'Notebook',\n",
       " 'Optional',\n",
       " 'P0',\n",
       " 'P1',\n",
       " 'P2',\n",
       " 'P3',\n",
       " 'Perception',\n",
       " 'Pipelines',\n",
       " 'Priority 1',\n",
       " 'Python',\n",
       " 'QS',\n",
       " 'Quick Win',\n",
       " 'Rebrand',\n",
       " 'Reinforcement Learning',\n",
       " 'RepoOfficiel',\n",
       " 'Research',\n",
       " 'Review One',\n",
       " 'Review Two',\n",
       " 'SDK',\n",
       " 'Service Attention',\n",
       " 'Stage: Technical Design',\n",
       " 'Stage: Technical Design ðŸŽ¨',\n",
       " 'Stale',\n",
       " 'TA',\n",
       " 'TODO',\n",
       " 'TODO before 1.0',\n",
       " 'Task',\n",
       " 'TipðŸ’°',\n",
       " 'Training',\n",
       " 'Training Monitoring',\n",
       " 'Training Service',\n",
       " 'Trn1',\n",
       " 'Type: Brainstorm',\n",
       " 'Type: Discussion',\n",
       " 'Type: Documentation',\n",
       " 'Type: Enhancement',\n",
       " 'Type: Feature',\n",
       " 'Type: Idea',\n",
       " 'Type: Needs Investigation',\n",
       " 'Type: Technical Design',\n",
       " 'UI',\n",
       " 'UX',\n",
       " 'Usage',\n",
       " 'VISION',\n",
       " 'WIP',\n",
       " 'WIP - Susankha',\n",
       " 'Workspace Management',\n",
       " '[module] pipeline',\n",
       " 'accelerator: tpu',\n",
       " 'accessibility',\n",
       " 'ai',\n",
       " 'aiplatform',\n",
       " 'air',\n",
       " 'alonet',\n",
       " 'api: aiplatform',\n",
       " 'api: datascienceonramp',\n",
       " 'api: vertex-ai',\n",
       " 'app-ui',\n",
       " 'apply',\n",
       " 'arc_ml',\n",
       " 'architecture',\n",
       " 'area / SDK-storage',\n",
       " 'area / integrations',\n",
       " 'area-getting-started',\n",
       " 'area-js-sdk',\n",
       " 'area-ml-resource-management',\n",
       " 'area-remote-desktop',\n",
       " 'area-remote-web',\n",
       " 'area-sign-in',\n",
       " 'area-telemetry',\n",
       " 'area-treeview',\n",
       " 'area-yaml',\n",
       " 'area/async',\n",
       " 'area/backend',\n",
       " 'area/components',\n",
       " 'area/components/aws/sagemaker',\n",
       " 'area/katib',\n",
       " 'area/learn',\n",
       " 'area/operator',\n",
       " 'area/registry',\n",
       " 'area/samples',\n",
       " 'area/sdk',\n",
       " 'area:core',\n",
       " 'area:databuilder',\n",
       " 'area:providers',\n",
       " 'area:serialization',\n",
       " 'arena::security',\n",
       " 'assigned',\n",
       " 'assigned-to-author',\n",
       " 'audience/technical',\n",
       " 'august-rewrite',\n",
       " 'auto:improvement',\n",
       " 'automations',\n",
       " 'automl',\n",
       " 'automl-forecasting',\n",
       " 'awaiting-feedback',\n",
       " 'awaiting-product-team-response',\n",
       " 'awaiting-response',\n",
       " 'aws',\n",
       " 'azure',\n",
       " 'azure-provider',\n",
       " 'azureml',\n",
       " 'backlog',\n",
       " 'benchmark',\n",
       " 'bittensor',\n",
       " 'blocked',\n",
       " 'blocker',\n",
       " 'breakdown',\n",
       " 'breaking',\n",
       " 'bug',\n",
       " 'bug-bash',\n",
       " 'build',\n",
       " 'cannot reproduce',\n",
       " 'chapter: appendix-tools',\n",
       " 'checkpointing',\n",
       " 'chore',\n",
       " 'ci',\n",
       " 'classification',\n",
       " 'cleanup',\n",
       " 'closing-soon',\n",
       " 'closing-soon-if-no-response',\n",
       " 'cloud/aws',\n",
       " 'code',\n",
       " 'code/new-feature',\n",
       " 'colab',\n",
       " 'concerns: agents',\n",
       " 'concerns: documentation',\n",
       " 'concerns: main API',\n",
       " 'configs',\n",
       " 'connectors',\n",
       " 'content-gap',\n",
       " 'contrib',\n",
       " 'contribution welcomed',\n",
       " 'contribution-welcome',\n",
       " 'core',\n",
       " 'core/subsvc',\n",
       " 'crash',\n",
       " 'customer-inquiry',\n",
       " 'customer-issue',\n",
       " 'customer-reported',\n",
       " 'cxp',\n",
       " 'data',\n",
       " 'data-sync',\n",
       " 'debt',\n",
       " 'dependencies',\n",
       " 'deploy',\n",
       " 'design',\n",
       " 'dev',\n",
       " 'dev workflow',\n",
       " 'devflows',\n",
       " 'discussion',\n",
       " 'diy_container',\n",
       " 'doc',\n",
       " 'doc-bug',\n",
       " 'doc-enhancement',\n",
       " 'docker images',\n",
       " 'docs',\n",
       " 'docs/website',\n",
       " 'documentation',\n",
       " 'duplicate',\n",
       " 'enhancement',\n",
       " 'enhancement request',\n",
       " 'environment',\n",
       " 'environment: slurm',\n",
       " 'ep:CUDA',\n",
       " 'epic',\n",
       " 'error',\n",
       " 'evaluate_model',\n",
       " 'example issue',\n",
       " 'example request',\n",
       " 'examples',\n",
       " 'experiment',\n",
       " 'experimental',\n",
       " 'exploratory',\n",
       " 'explore',\n",
       " 'external package',\n",
       " 'feature',\n",
       " 'feature request',\n",
       " 'feature-request',\n",
       " 'feature_set',\n",
       " 'filed-by-team',\n",
       " 'first-timers-only',\n",
       " 'fix-error-msg',\n",
       " 'fixed',\n",
       " 'flaky-tracker',\n",
       " 'flakybot: flaky',\n",
       " 'flakybot: issue',\n",
       " 'forum',\n",
       " 'future release',\n",
       " 'gcp',\n",
       " 'gitlab merge request',\n",
       " 'golden-scenario-web',\n",
       " 'good first issue',\n",
       " 'graphistry',\n",
       " 'guidance',\n",
       " 'guides',\n",
       " 'hacktoberfest',\n",
       " 'help wanted',\n",
       " 'hi-ml-azure',\n",
       " 'high-priority',\n",
       " 'implementing framework',\n",
       " 'important',\n",
       " 'improvement',\n",
       " 'improvements',\n",
       " 'in progress',\n",
       " 'inference',\n",
       " 'inferencing-benchmark',\n",
       " 'info-needed',\n",
       " 'infra',\n",
       " 'infrastructure',\n",
       " 'integration',\n",
       " 'interfacing algorithms',\n",
       " 'invalid',\n",
       " 'investigating',\n",
       " 'investigation',\n",
       " 'issue-addressed',\n",
       " 'iteration-02',\n",
       " 'iteration-candidate',\n",
       " 'jetmet',\n",
       " 'journey:intermediate',\n",
       " 'keep fresh',\n",
       " 'kind/bug',\n",
       " 'kind/enhancement',\n",
       " 'kind/feature',\n",
       " 'kind/other',\n",
       " 'kind/question',\n",
       " 'kind/reproducibility',\n",
       " 'kind/usability',\n",
       " 'kind: new integration',\n",
       " 'kind: research',\n",
       " 'kind:bug',\n",
       " 'kind:feature',\n",
       " 'kind:task',\n",
       " 'kubeflow',\n",
       " 'launchers',\n",
       " 'learn',\n",
       " 'lifecycle/frozen',\n",
       " 'lifecycle/stale',\n",
       " 'lightning',\n",
       " 'linear',\n",
       " 'linting / formatting / cleaning',\n",
       " 'logger',\n",
       " 'logger: comet',\n",
       " 'logger: mlflow',\n",
       " 'logger: neptune',\n",
       " 'logger: wandb',\n",
       " 'loggers',\n",
       " 'logging',\n",
       " 'looking into it',\n",
       " 'low priority',\n",
       " 'machine-learning',\n",
       " 'machine-learning/svc',\n",
       " 'maintenance',\n",
       " 'major',\n",
       " 'manuscript',\n",
       " 'metrics',\n",
       " 'migrate: no',\n",
       " 'migrate: yes',\n",
       " 'missing_info',\n",
       " 'ml',\n",
       " 'ml-engineering',\n",
       " 'mlflow',\n",
       " 'model',\n",
       " 'models',\n",
       " 'module: contrib',\n",
       " 'module:forecasting',\n",
       " 'module:probability&simulation',\n",
       " 'module:wandb',\n",
       " 'must have',\n",
       " 'need-design-decision',\n",
       " 'needs improved description',\n",
       " 'needs triage',\n",
       " 'needs-api',\n",
       " 'needs-discussion',\n",
       " 'needs-more-info',\n",
       " 'needs-repro-script',\n",
       " 'needs-team-attention',\n",
       " 'needs-tests',\n",
       " 'needs-triage',\n",
       " 'neptune',\n",
       " 'new',\n",
       " 'new feature',\n",
       " 'new table',\n",
       " 'new-data-source',\n",
       " 'new-resource',\n",
       " 'nice to have',\n",
       " 'nnidev',\n",
       " 'no-issue-activity',\n",
       " 'non-breaking',\n",
       " 'normal',\n",
       " 'notebook',\n",
       " 'notebook-migration',\n",
       " 'observability-ux',\n",
       " 'on hold',\n",
       " 'open for contribution',\n",
       " 'operationalization',\n",
       " 'optimization',\n",
       " 'optuna',\n",
       " 'organizational',\n",
       " 'p0-critical',\n",
       " 'p1',\n",
       " 'p1-high',\n",
       " 'p1-important',\n",
       " 'p2-medium',\n",
       " 'p3',\n",
       " 'p3-nice-to-have',\n",
       " 'partition/aws-us-gov',\n",
       " 'pending-signatures',\n",
       " 'performance',\n",
       " 'phase / shipped',\n",
       " 'pipeline',\n",
       " 'pipeline 6: infer',\n",
       " 'pl',\n",
       " 'platform/aws',\n",
       " 'platform/other',\n",
       " 'plot_model',\n",
       " 'plugin',\n",
       " 'plugins',\n",
       " 'practice',\n",
       " 'pre-release',\n",
       " 'pri/medium',\n",
       " 'priority 3 - nice to have',\n",
       " 'priority-p0',\n",
       " 'priority-p1',\n",
       " 'priority/important-longterm',\n",
       " 'priority/p1',\n",
       " 'priority: 0',\n",
       " 'priority: 1',\n",
       " 'priority: 2',\n",
       " 'priority: high',\n",
       " 'priority: medium',\n",
       " 'priority: p1',\n",
       " 'priority: p2',\n",
       " 'priority: p3',\n",
       " 'priority:high',\n",
       " 'priority:low',\n",
       " 'priority:medium',\n",
       " 'priority_high',\n",
       " 'priority_medium',\n",
       " 'product-feedback',\n",
       " 'product-gap',\n",
       " 'product-issue',\n",
       " 'product-question',\n",
       " 'product::sorts',\n",
       " 'progress bar: rich',\n",
       " 'proposal',\n",
       " 'provider:AWS',\n",
       " 'publishing',\n",
       " 'python',\n",
       " 'quality of life',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quick-fix',\n",
       " 'ray-team-created',\n",
       " 'reconstruction-pending',\n",
       " 'refactor',\n",
       " 'refactoring',\n",
       " 'regression',\n",
       " 'related: Python SDK',\n",
       " 'release-blocker',\n",
       " 'reporting and diagnostics',\n",
       " 'reproducibility',\n",
       " 'research',\n",
       " 'response-requested',\n",
       " 'rllib',\n",
       " 'roadmap',\n",
       " 'sagemaker',\n",
       " 'sagemaker-dsk-v2',\n",
       " 'sagemaker_container',\n",
       " 'samples',\n",
       " 'scenario',\n",
       " 'sdk-docs',\n",
       " 'section',\n",
       " 'security',\n",
       " 'seq2seq',\n",
       " 'serve',\n",
       " 'service-api',\n",
       " 'service/ec2',\n",
       " 'service/events',\n",
       " 'service/iam',\n",
       " 'service/s3',\n",
       " 'service/sagemaker',\n",
       " 'service/servicecatalog',\n",
       " 'service/vpc',\n",
       " 'service:executor',\n",
       " 'service:sm-executor',\n",
       " 'setup',\n",
       " 'sgd',\n",
       " 'size:small',\n",
       " 'snippets-request',\n",
       " 'spark',\n",
       " 'stale',\n",
       " 'stale :zzz:',\n",
       " 'stat:awaiting response',\n",
       " 'stat:awaiting tensorflower',\n",
       " 'status/close',\n",
       " 'status/needs-proposal',\n",
       " 'status/new-issue',\n",
       " 'status/triaged',\n",
       " 'status: awaiting response',\n",
       " 'status: blocked',\n",
       " 'status: phase 1',\n",
       " 'status: phase 2',\n",
       " 'status: triage',\n",
       " 'status:completed',\n",
       " 'status:needs_reproducing',\n",
       " 'status:needs_triage',\n",
       " 'status:needs_votes',\n",
       " 'strategy: ddp',\n",
       " 'streaming',\n",
       " 'studio',\n",
       " 'support',\n",
       " 'suppress diff',\n",
       " 'sweep',\n",
       " 'sweeper',\n",
       " 't-must-fix',\n",
       " 't-nice-to-have-fix',\n",
       " 't-unknown-sub-error',\n",
       " 'task',\n",
       " 'technical',\n",
       " 'technical debt',\n",
       " 'technical-debt',\n",
       " 'test',\n",
       " 'testing',\n",
       " 'tests',\n",
       " 'third party issue',\n",
       " 'third-party',\n",
       " 'time_series',\n",
       " 'timecodes',\n",
       " 'to refine',\n",
       " 'todo',\n",
       " 'tooling and CI',\n",
       " 'topic:dependencies',\n",
       " 'topic:eval',\n",
       " 'topic:modeling',\n",
       " 'topic:reader',\n",
       " 'train',\n",
       " 'trainer',\n",
       " 'training',\n",
       " 'training-benchmark',\n",
       " 'transformations',\n",
       " 'triage',\n",
       " 'triage me',\n",
       " 'triage-needed',\n",
       " 'triage/intermediate-priotrity',\n",
       " 'triaged',\n",
       " 'troubleshooting',\n",
       " 'tune',\n",
       " 'tune_model',\n",
       " 'type / bug',\n",
       " 'type / code-health',\n",
       " 'type / enhancement',\n",
       " 'type/bug',\n",
       " 'type/feature-request',\n",
       " 'type/maintenance',\n",
       " 'type: bug',\n",
       " 'type: cleanup',\n",
       " 'type: docs',\n",
       " 'type: enhancement',\n",
       " 'type: feature request',\n",
       " 'type: question',\n",
       " 'type::bug',\n",
       " 'type:bug',\n",
       " 'type:docs',\n",
       " 'type:feature',\n",
       " 'type:maintenance',\n",
       " 'type:question',\n",
       " 'ultralytics',\n",
       " 'up-for-grabs',\n",
       " 'upstream',\n",
       " 'upstream-azml',\n",
       " 'user raised',\n",
       " 'utilities',\n",
       " 'utils',\n",
       " 'ux',\n",
       " 'v0.1.1',\n",
       " 'v0.8.5',\n",
       " 'v1',\n",
       " 'v2',\n",
       " 'ver: 2.0.x',\n",
       " 'ver: 2.1.x',\n",
       " 'waiting',\n",
       " 'waiting feedback',\n",
       " 'waiting on author',\n",
       " 'waiting on release',\n",
       " 'waiting-for-ci-fix',\n",
       " 'waiting-response',\n",
       " 'wandb',\n",
       " 'windows',\n",
       " 'wishlist',\n",
       " \"won't fix\",\n",
       " 'wontfix',\n",
       " 'work package: model training',\n",
       " 'work-item',\n",
       " 'workflow',\n",
       " 'workflow-experience',\n",
       " 'working as intended',\n",
       " 'âœ¨ feat',\n",
       " 'ë¯¼ì§€',\n",
       " 'ì°¬êµ­',\n",
       " 'ðŸ› bug fix',\n",
       " 'ðŸž bug',\n",
       " 'ðŸ¥ experiment',\n",
       " 'ðŸº Tracker',\n",
       " 'ðŸ‘¨\\u200dðŸ‘©\\u200dðŸ‘§\\u200dðŸ‘§ discussion',\n",
       " 'ðŸ’Ž New Component',\n",
       " 'ðŸ“œ Paper',\n",
       " 'ðŸ”¤ named-entity-recognition',\n",
       " 'ðŸ”¥ New Feature',\n",
       " 'ðŸ›‚ checkpoint',\n",
       " 'ðŸ¦‰ dvc',\n",
       " 'ðŸ§ª testing'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = set()\n",
    "for _, row in df_issues['Issue_label'].map(set).items():\n",
    "    final = final.union(row)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# filter out issues that are not related to challenges\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    if not row['Issue_label']:\n",
    "        df = pd.concat([df, row.to_frame().T], ignore_index =True)\n",
    "    else:\n",
    "        for label in row['Issue_label']:\n",
    "            for issue_label in issue_labels:\n",
    "                if issue_label in label.lower():\n",
    "                    df = pd.concat([df, row.to_frame().T], ignore_index =True)\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "print(len(df[df['Platform'] == 'GitHub Issue']))\n",
    "print(len(df[df['Platform'] == 'GitLab Issue']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create issue dataset\n",
    "\n",
    "df['Issue_score_count'] = df['Issue_upvote_count'] - df['Issue_downvote_count']\n",
    "df['Issue_comment_score'] = df['Issue_comment_upvote'] - df['Issue_comment_downvote']\n",
    "df['Issue_tag_count'] = df['Issue_label'].map(len)\n",
    "df['Issue_body'] = df['Issue_body'].fillna('')\n",
    "\n",
    "df.drop(columns=['Issue_upvote_count', 'Issue_downvote_count', 'Issue_comment_upvote', 'Issue_comment_downvote', 'Issue_label'], inplace=True)\n",
    "df.to_json(os.path.join(path_labeling, 'issues.json'), indent=4, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

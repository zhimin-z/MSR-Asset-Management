{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../Dataset'\n",
    "\n",
    "path_github = os.path.join(path_dataset, 'GitHub')\n",
    "path_gitlab = os.path.join(path_dataset, 'GitLab')\n",
    "path_labeling = os.path.join(path_dataset, 'Labeling')\n",
    "\n",
    "path_github_repo = os.path.join(path_github, 'Repo')\n",
    "path_gitlab_repo = os.path.join(path_gitlab, 'Repo')\n",
    "path_github_issue = os.path.join(path_github, 'Issue')\n",
    "path_gitlab_issue = os.path.join(path_gitlab, 'Issue')\n",
    "\n",
    "if not os.path.exists(path_github):\n",
    "    os.makedirs(path_github)\n",
    "\n",
    "if not os.path.exists(path_gitlab):\n",
    "    os.makedirs(path_gitlab)\n",
    "\n",
    "if not os.path.exists(path_labeling):\n",
    "    os.makedirs(path_labeling)\n",
    "\n",
    "if not os.path.exists(path_github_repo):\n",
    "    os.makedirs(path_github_repo)\n",
    "\n",
    "if not os.path.exists(path_gitlab_repo):\n",
    "    os.makedirs(path_gitlab_repo)\n",
    "\n",
    "if not os.path.exists(path_github_issue):\n",
    "    os.makedirs(path_github_issue)\n",
    "\n",
    "if not os.path.exists(path_gitlab_issue):\n",
    "    os.makedirs(path_gitlab_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_repo = {\n",
    "    'Aim': 'aimhubio/aim',\n",
    "    'Amazon SageMaker': 'aws/sagemaker-python-sdk',\n",
    "    'Azure Machine Learning': 'Azure/azure-sdk-for-python',\n",
    "    'ClearML': 'allegroai/clearml',\n",
    "    'Codalab': 'codalab/codalab-worksheets',\n",
    "    'DVC': 'iterative/dvc',\n",
    "    'Determined': 'determined-ai/determined',\n",
    "    'Domino': 'dominodatalab/python-domino',\n",
    "    'Guild AI': 'guildai/guildai',\n",
    "    'Kedro': 'kedro-org/kedro',\n",
    "    'MLflow': 'mlflow/mlflow',\n",
    "    'MLRun': 'mlrun/mlrun',\n",
    "    'ModelDB': 'VertaAI/modeldb',\n",
    "    'Neptune': 'neptune-ai/neptune-client',\n",
    "    'Optuna': 'optuna/optuna',\n",
    "    'Polyaxon': 'polyaxon/polyaxon',\n",
    "    'Sacred': 'IDSIA/sacred',\n",
    "    'Valohai': 'valohai/valohai-cli',\n",
    "    'Weights & Biases': 'wandb/wandb'\n",
    "}\n",
    "\n",
    "tools_release_date = {\n",
    "    'Amazon SageMaker': '2017-11-19',\n",
    "    'Azure Machine Learning': '2015-02-18',\n",
    "    'cnvrg.io': '2020-03-31',\n",
    "    'Comet': '2017-01-01',\n",
    "    'Iterative Studio': '2021-05-12',\n",
    "    'Polyaxon': '2018-10-16',\n",
    "    'SigOpt': '2014-11-01',\n",
    "    'Vertex AI': '2019-03-01'\n",
    "}\n",
    "\n",
    "tools_link = {\n",
    "    'cnvrg.io': 'https://github.com/cnvrg',\n",
    "    'Comet': 'https://github.com/comet-ml',\n",
    "    'Iterative Studio': 'https://studio.iterative.ai',\n",
    "    'SigOpt': 'https://github.com/sigopt',\n",
    "    'Vertex AI': 'https://cloud.google.com/vertex-ai'\n",
    "}\n",
    "\n",
    "tools_keywords = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['sage maker', 'sagemaker'],\n",
    "    'Azure Machine Learning': ['azure machine learning', 'azure ml', 'azureml'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild ai', 'guildai'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['vertex ai', 'vertexai'],\n",
    "    'Weights & Biases': ['weights and biases', 'wandb', 'weights & biases', 'weights&biases', 'w & b', 'w&b']\n",
    "}\n",
    "\n",
    "issue_labels = {\n",
    "    'bug',\n",
    "    'error',\n",
    "    'invalid',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scrape.GHMiner import GitHubMiner\n",
    "from Scrape.GLMiner import GitLabMiner\n",
    "\n",
    "github_miner = GitHubMiner(private_token=os.getenv('GITHUB_TOKEN'))\n",
    "gitlab_miner = GitLabMiner(private_token=os.getenv('GITLAB_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_data = pd.DataFrame()\n",
    "\n",
    "# scrape open-source asset-management tools\n",
    "for tool_name, tool_repo in tools_repo.items():\n",
    "    if tool_name in tools_release_date:\n",
    "        tool_data = github_miner.scrape_repo(repo_name=tool_repo, real_name=tool_name, release_time=pd.to_datetime(tools_release_date[tool_name]))\n",
    "    else:\n",
    "        tool_data = github_miner.scrape_repo(repo_name=tool_repo, real_name=tool_name)\n",
    "\n",
    "    if not tool_data.empty:\n",
    "        tools_data = pd.concat([tools_data, tool_data], ignore_index=True)\n",
    "\n",
    "# add closed-source asset-management tools\n",
    "for tool_name in tools_link.keys():\n",
    "    tool_data = {\n",
    "        'Name': tool_name,\n",
    "        'Link': tools_link[tool_name],\n",
    "        'First Release Date': pd.to_datetime(tools_release_date[tool_name])\n",
    "    }\n",
    "    tool_data = pd.DataFrame([tool_data])\n",
    "    tools_data = pd.concat([tools_data, tool_data], ignore_index=True)\n",
    "\n",
    "tools_data.to_json(os.path.join(path_dataset, 'Tools.json'),\n",
    "                   indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37782\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "github_dependents = {}\n",
    "gitlab_dependents = {}\n",
    "\n",
    "# collect dependents for tools with coding patterns\n",
    "for tool_name in tools_keywords.keys():\n",
    "    # collect Github dependents\n",
    "    file_name = os.path.join(path_github_repo, tool_name + '.json')\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, encoding='utf8') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            # either search by sourcegraph\n",
    "            if 'Results' in json_data:\n",
    "                for repo_file in json_data['Results']:\n",
    "                    # file name match pattern\n",
    "                    if 'FileMatch' == repo_file['__typename'] and repo_file['repository']['name'].startswith('github'):\n",
    "                        repo_name = repo_file['repository']['name'].removeprefix(\n",
    "                            'github.com/')\n",
    "                        if repo_name in github_dependents:\n",
    "                            github_dependents[repo_name].append(tool_name)\n",
    "                        else:\n",
    "                            github_dependents[repo_name] = [tool_name]\n",
    "                    # code usage match pattern\n",
    "                    elif 'Repository' == repo_file['__typename'] and repo_file['name'].startswith('github'):\n",
    "                        repo_name = repo_file['name'].removeprefix(\n",
    "                            'github.com/')\n",
    "                        if repo_name in github_dependents:\n",
    "                            github_dependents[repo_name].append(tool_name)\n",
    "                        else:\n",
    "                            github_dependents[repo_name] = [tool_name]\n",
    "            # or search by dependent graph\n",
    "            elif 'all_public_dependent_repos' in json_data:\n",
    "                for repo_file in json_data['all_public_dependent_repos']:\n",
    "                    repo_name = repo_file['name']\n",
    "                    if repo_name in github_dependents:\n",
    "                        github_dependents[repo_name].append(tool_name)\n",
    "                    else:\n",
    "                        github_dependents[repo_name] = [tool_name]\n",
    "\n",
    "    # collect Gitlab dependents\n",
    "    file_name = os.path.join(path_gitlab_repo, tool_name + '.json')\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, encoding='utf8') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            # search by sourcegraph exclusively\n",
    "            for repo_file in json_data['Results']:\n",
    "                # file name match pattern\n",
    "                if 'FileMatch' == repo_file['__typename'] and repo_file['repository']['name'].startswith('gitlab'):\n",
    "                    repo_name = repo_file['repository']['name'].removeprefix(\n",
    "                        'gitlab.com/')\n",
    "                    if repo_name in gitlab_dependents:\n",
    "                        gitlab_dependents[repo_name].append(tool_name)\n",
    "                    else:\n",
    "                        gitlab_dependents[repo_name] = [tool_name]\n",
    "                # code usage match pattern\n",
    "                elif 'Repository' == repo_file['__typename'] and repo_file['name'].startswith('gitlab'):\n",
    "                    repo_name = repo_file['name'].removeprefix('gitlab.com/')\n",
    "                    if repo_name in gitlab_dependents:\n",
    "                        gitlab_dependents[repo_name].append(tool_name)\n",
    "                    else:\n",
    "                        gitlab_dependents[repo_name] = [tool_name]\n",
    "\n",
    "    # remove tool repo from dependents if any\n",
    "    if tool_name in tools_repo and tools_repo[tool_name] in github_dependents:\n",
    "        github_dependents.pop(tools_repo[tool_name], None)\n",
    "\n",
    "with open(os.path.join(path_github_repo, 'Dependents.pickle'), 'wb') as file:\n",
    "    pickle.dump(github_dependents, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(path_gitlab_repo, 'Dependents.pickle'), 'wb') as file:\n",
    "    pickle.dump(gitlab_dependents, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(len(github_dependents))\n",
    "print(len(gitlab_dependents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#GitHub Dependents</th>\n",
       "      <th>#GitLab Dependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>19952</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optuna</td>\n",
       "      <td>6370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DVC</td>\n",
       "      <td>6098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sacred</td>\n",
       "      <td>1918</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLflow</td>\n",
       "      <td>1573</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kedro</td>\n",
       "      <td>1184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>1113</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>826</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Comet</td>\n",
       "      <td>678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ClearML</td>\n",
       "      <td>498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Neptune</td>\n",
       "      <td>422</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aim</td>\n",
       "      <td>189</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SigOpt</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Guild AI</td>\n",
       "      <td>67</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Codalab</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Polyaxon</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Determined</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Valohai</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLRun</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ModelDB</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Domino</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tool #GitHub Dependents #GitLab Dependents\n",
       "2         Weights & Biases              19952                  0\n",
       "4                   Optuna               6370                  0\n",
       "9                      DVC               6098                  0\n",
       "5                   Sacred               1918                  0\n",
       "7                   MLflow               1573                  4\n",
       "1                    Kedro               1184                  0\n",
       "11        Amazon SageMaker               1113                  3\n",
       "6   Azure Machine Learning                826                  0\n",
       "8                    Comet                678                  0\n",
       "3                  ClearML                498                  0\n",
       "12                 Neptune                422                  0\n",
       "0                      Aim                189                  1\n",
       "13               Vertex AI                134                  0\n",
       "10                  SigOpt                 96                  0\n",
       "20                Guild AI                 67                  4\n",
       "17                 Codalab                 40                  0\n",
       "15                Polyaxon                 36                  0\n",
       "18              Determined                 36                  0\n",
       "14                 Valohai                 31                  0\n",
       "16                   MLRun                 27                  0\n",
       "21                 ModelDB                  9                  0\n",
       "19                  Domino                  1                  0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github_repos = {}\n",
    "gitlab_repos = {}\n",
    "\n",
    "for repo_name, tool_list in github_dependents.items():\n",
    "    for tool_name in tool_list:\n",
    "        github_repos[tool_name] = github_repos.get(tool_name, 0) + 1\n",
    "\n",
    "for repo_name, tool_list in gitlab_dependents.items():\n",
    "    for tool_name in tool_list:\n",
    "        gitlab_repos[tool_name] = gitlab_repos.get(tool_name, 0) + 1\n",
    "\n",
    "dependents_summary = pd.DataFrame(columns=['Tool', '#GitHub Dependents', '#GitLab Dependents'])\n",
    "\n",
    "for tool_name, repo_num in github_repos.items():\n",
    "    if tool_name in gitlab_repos:\n",
    "        entry = {'Tool': tool_name, '#GitHub Dependents': repo_num, '#GitLab Dependents': gitlab_repos[tool_name]}\n",
    "    else:\n",
    "        entry = {'Tool': tool_name, '#GitHub Dependents': repo_num, '#GitLab Dependents': 0}\n",
    "    dependents_summary = pd.concat([dependents_summary, pd.DataFrame([entry])], ignore_index=True)\n",
    "\n",
    "dependents_summary = dependents_summary.sort_values(by=['#GitHub Dependents', '#GitLab Dependents'], ascending=False)\n",
    "dependents_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_github_repo, 'Dependents.pickle'), 'rb') as file:\n",
    "    github_dependents = pickle.load(file)\n",
    "\n",
    "with open(os.path.join(path_gitlab_repo, 'Dependents.pickle'), 'rb') as file:\n",
    "    gitlab_dependents = pickle.load(file)\n",
    "    \n",
    "df_tool = pd.read_json(os.path.join(path_dataset, 'Tools.json'))\n",
    "tools_release_date = pd.Series(pd.to_datetime(df_tool['First Release Date'].values), index=df_tool['Name']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape issues of Github dependents for each tool\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(path_github_issue, 'raw.json'))\n",
    "df_issues['Repo'] = df_issues['Issue_link'].apply(lambda x: x.split('/')[3] + '/' + x.split('/')[4])\n",
    "\n",
    "existing_repos = set(df_issues['Repo'].unique())\n",
    "github_dependents = {k: v for k, v in github_dependents.items() if k not in existing_repos} \n",
    "\n",
    "for repo_name, tool_list in github_dependents.items():\n",
    "    repo_data = github_miner.scrape_repo(repo_name)\n",
    "    if repo_data.empty or repo_data['#Issue'].values[0] == 0:\n",
    "        continue\n",
    "    invalid_repo = []\n",
    "    for tool_name in tool_list:\n",
    "        if repo_data['Repo Created Date'].values[0] < tools_release_date[tool_name]:\n",
    "            invalid_repo.append(tool_name)\n",
    "    tool_list = [tool for tool in tool_list if tool not in invalid_repo]\n",
    "    if not tool_list:\n",
    "        issues = github_miner.scrape_issue(repo_name)\n",
    "        issues['Tools'] = tool_list\n",
    "        df_issues = pd.concat([df_issues, issues], ignore_index=True)\n",
    "        df_issues.to_json(os.path.join(path_github_issue, 'raw.json'), indent=4, orient='records')\n",
    "    \n",
    "df_issues.to_json(os.path.join(path_github_issue, 'raw.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape issues of Github dependents for each tool\n",
    "\n",
    "df_issues = pd.DataFrame()\n",
    "\n",
    "for repo_name, tool_list in github_dependents.items():\n",
    "    repo_data = github_miner.scrape_repo(repo_name)\n",
    "    if repo_data.empty or repo_data['#Issue'].values[0] == 0:\n",
    "        continue\n",
    "    invalid_repo = []\n",
    "    for tool_name in tool_list:\n",
    "        if repo_data['Repo Created Date'].values[0] < tools_release_date[tool_name]:\n",
    "            invalid_repo.append(tool_name)\n",
    "    tool_list = [tool for tool in tool_list if tool not in invalid_repo]\n",
    "    if not tool_list:\n",
    "        issues = github_miner.scrape_issue(repo_name)\n",
    "        issues['Tools'] = tool_list\n",
    "        df_issues = pd.concat([df_issues, issues], ignore_index=True)\n",
    "        df_issues.to_json(os.path.join(path_github_issue, 'raw.json'), indent=4, orient='records')\n",
    "    \n",
    "df_issues.to_json(os.path.join(path_github_issue, 'raw.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape issues of Gitlab dependents for each tool\n",
    "\n",
    "df_issues = pd.DataFrame()\n",
    "\n",
    "for repo_name, tool_list in github_dependents.items():\n",
    "    repo_data = gitlab_miner.scrape_repo(repo_name)\n",
    "    if repo_data.empty or repo_data['#Issue'].values[0] == 0:\n",
    "        continue\n",
    "    invalid_repo = []\n",
    "    for tool_name in tool_list:\n",
    "        if repo_data['Repo Created Date'].values[0] < tools_release_date[tool_name]:\n",
    "            invalid_repo.append(tool_name)\n",
    "    tool_list = [tool for tool in tool_list if tool not in invalid_repo]\n",
    "    if not tool_list:\n",
    "        issues = gitlab_miner.scrape_issue(repo_name)\n",
    "        issues['Tools'] = tool_list\n",
    "        df_issues = pd.concat([df_issues, issues], ignore_index=True)\n",
    "        df_issues.to_json(os.path.join(path_gitlab_issue, 'raw.json'), indent=4, orient='records')\n",
    "    \n",
    "df_issues.to_json(os.path.join(path_gitlab_issue, 'raw.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude issues that are not related to each tool\n",
    "\n",
    "df_issues_gitlab = pd.read_json(os.path.join(path_gitlab_issue, 'raw.json'))\n",
    "df_issues_github = pd.read_json(os.path.join(path_github_issue, 'raw.json'))\n",
    "\n",
    "df_issues_github['Platform'] = 'Github'\n",
    "df_issues_gitlab['Platform'] = 'Gitlab'\n",
    "\n",
    "df_issues = pd.DataFrame()\n",
    "\n",
    "for index, row in df_issues_github.iterrows():\n",
    "    for tool_name in row['Tools']:\n",
    "        for keyword in tools_keywords[tool_name]:\n",
    "            if keyword in row['Issue_title'].lower():\n",
    "                df_issues = pd.concat([df_issues, pd.DataFrame([row])], ignore_index=True)\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "\n",
    "for index, row in df_issues_gitlab.iterrows():\n",
    "    for tool_name in row['Tools']:\n",
    "        for keyword in tools_keywords[tool_name]:\n",
    "            if keyword in row['Issue_title'].lower():\n",
    "                df_issues = pd.concat([df_issues, pd.DataFrame([row])], ignore_index=True)\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "\n",
    "len(df_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"experiments\"',\n",
       " '0.4.6',\n",
       " '1.1',\n",
       " '1.4',\n",
       " '1.6',\n",
       " '1.7',\n",
       " '2.0',\n",
       " '3 - Quality of Life',\n",
       " '3rd party',\n",
       " '3rd party update',\n",
       " ':bridge_at_night:  Bridge',\n",
       " ':bug: bug',\n",
       " ':rotating_light:',\n",
       " '? - Needs Triage',\n",
       " 'A: example-dvc-experiments',\n",
       " 'A: example-get-started',\n",
       " 'ADO',\n",
       " 'AI\\u202fFrameworks/ONNX',\n",
       " 'AML Compute Instance',\n",
       " 'API',\n",
       " 'API & Doc',\n",
       " 'Auto\\u202fML',\n",
       " 'BF',\n",
       " 'Cloud',\n",
       " 'Community',\n",
       " 'Compute',\n",
       " 'Core UI',\n",
       " 'DRL',\n",
       " 'Data Labeling',\n",
       " 'Data4ML',\n",
       " 'Data\\u202fDrift',\n",
       " 'Data\\u202fPrep\\u202fServices',\n",
       " 'Documentation',\n",
       " 'ERRATA_CANDIDATE',\n",
       " 'Enhancement',\n",
       " 'Environments',\n",
       " 'Evaluation',\n",
       " 'Experimentation UI',\n",
       " 'FAQ',\n",
       " 'Feature - Medium Priority',\n",
       " 'HIGH',\n",
       " 'HPO',\n",
       " 'Hyperdrive',\n",
       " 'Important',\n",
       " 'In the roadmap',\n",
       " 'Inf1',\n",
       " 'Inference',\n",
       " 'Ingestion',\n",
       " 'Issue: Bug Report 🐞',\n",
       " 'Issue: Feature Request',\n",
       " 'L',\n",
       " 'LOE: S',\n",
       " 'Localized',\n",
       " 'MLOps',\n",
       " 'NLP',\n",
       " 'NUM',\n",
       " 'Needs Triage',\n",
       " 'Not related to PyCaret',\n",
       " 'Notebook',\n",
       " 'Optional',\n",
       " 'P0',\n",
       " 'P1',\n",
       " 'P2',\n",
       " 'Pipelines',\n",
       " 'Priority 1',\n",
       " 'Reinforcement Learning',\n",
       " 'RepoOfficiel',\n",
       " 'Review One',\n",
       " 'Review Two',\n",
       " 'SDK',\n",
       " 'Stage: Technical Design 🎨',\n",
       " 'Stale',\n",
       " 'TA',\n",
       " 'TODO',\n",
       " 'TODO before 1.0',\n",
       " 'Training',\n",
       " 'Training Service',\n",
       " 'Trn1',\n",
       " 'Usage',\n",
       " 'VISION',\n",
       " 'WIP',\n",
       " 'WIP - Susankha',\n",
       " 'Workspace Management',\n",
       " '[module] pipeline',\n",
       " 'accelerator: tpu',\n",
       " 'accessibility',\n",
       " 'ai',\n",
       " 'aiplatform',\n",
       " 'air',\n",
       " 'alonet',\n",
       " 'api: aiplatform',\n",
       " 'api: vertex-ai',\n",
       " 'app-ui',\n",
       " 'apply',\n",
       " 'arc_ml',\n",
       " 'architecture',\n",
       " 'area / SDK-storage',\n",
       " 'area / integrations',\n",
       " 'area-getting-started',\n",
       " 'area-ml-resource-management',\n",
       " 'area-remote-desktop',\n",
       " 'area-remote-web',\n",
       " 'area-sign-in',\n",
       " 'area-telemetry',\n",
       " 'area-treeview',\n",
       " 'area-yaml',\n",
       " 'area/async',\n",
       " 'area/components',\n",
       " 'area/components/aws/sagemaker',\n",
       " 'area/operator',\n",
       " 'area/registry',\n",
       " 'area/samples',\n",
       " 'area:databuilder',\n",
       " 'arena::security',\n",
       " 'assigned',\n",
       " 'assigned-to-author',\n",
       " 'audience/technical',\n",
       " 'august-rewrite',\n",
       " 'automations',\n",
       " 'automl',\n",
       " 'awaiting response',\n",
       " 'awaiting-product-team-response',\n",
       " 'aws',\n",
       " 'azure',\n",
       " 'azure-provider',\n",
       " 'azureml',\n",
       " 'backlog',\n",
       " 'benchmark',\n",
       " 'bittensor',\n",
       " 'blocked',\n",
       " 'blocker',\n",
       " 'breakdown',\n",
       " 'breaking',\n",
       " 'bug',\n",
       " 'bug-bash',\n",
       " 'build',\n",
       " 'chapter: appendix-tools',\n",
       " 'checkpointing',\n",
       " 'chore',\n",
       " 'cleanup',\n",
       " 'closing-soon-if-no-response',\n",
       " 'code',\n",
       " 'concerns: agents',\n",
       " 'concerns: documentation',\n",
       " 'concerns: main API',\n",
       " 'configs',\n",
       " 'connectors',\n",
       " 'contribution welcomed',\n",
       " 'core/subsvc',\n",
       " 'customer-inquiry',\n",
       " 'customer-issue',\n",
       " 'cxp',\n",
       " 'data',\n",
       " 'data-sync',\n",
       " 'debt',\n",
       " 'dependencies',\n",
       " 'deploy',\n",
       " 'design',\n",
       " 'dev',\n",
       " 'dev workflow',\n",
       " 'devflows',\n",
       " 'discussion',\n",
       " 'diy_container',\n",
       " 'doc-bug',\n",
       " 'doc-enhancement',\n",
       " 'docker images',\n",
       " 'docs',\n",
       " 'documentation',\n",
       " 'duplicate',\n",
       " 'enhancement',\n",
       " 'enhancement request',\n",
       " 'env: new',\n",
       " 'env: sagemaker',\n",
       " 'environment',\n",
       " 'environment: slurm',\n",
       " 'ep:CUDA',\n",
       " 'epic',\n",
       " 'error',\n",
       " 'evaluate_model',\n",
       " 'example issue',\n",
       " 'example request',\n",
       " 'experimental',\n",
       " 'explore',\n",
       " 'feature',\n",
       " 'feature request',\n",
       " 'feature-request',\n",
       " 'fixme',\n",
       " 'forum',\n",
       " 'future release',\n",
       " 'good first issue',\n",
       " 'graphistry',\n",
       " 'guides',\n",
       " 'help wanted',\n",
       " 'hi-ml-azure',\n",
       " 'high-priority',\n",
       " 'improvement',\n",
       " 'improvements',\n",
       " 'in progress',\n",
       " 'inference',\n",
       " 'inferencing-benchmark',\n",
       " 'info-needed',\n",
       " 'infrastructure',\n",
       " 'integration',\n",
       " 'invalid',\n",
       " 'investigating',\n",
       " 'investigation',\n",
       " 'iteration-candidate',\n",
       " 'journey:intermediate',\n",
       " 'keep fresh',\n",
       " 'kind/bug',\n",
       " 'kind/enhancement',\n",
       " 'kind/feature',\n",
       " 'kind/question',\n",
       " 'kind/reproducibility',\n",
       " 'kind/usability',\n",
       " 'kubeflow',\n",
       " 'learn',\n",
       " 'lifecycle/frozen',\n",
       " 'lifecycle/stale',\n",
       " 'lightning',\n",
       " 'linting / formatting / cleaning',\n",
       " 'logger',\n",
       " 'logger: comet',\n",
       " 'logger: mlflow',\n",
       " 'logger: wandb',\n",
       " 'logging',\n",
       " 'looking into it',\n",
       " 'low priority',\n",
       " 'machine-learning',\n",
       " 'machine-learning/svc',\n",
       " 'major',\n",
       " 'metrics',\n",
       " 'missing_info',\n",
       " 'ml',\n",
       " 'ml-engineering',\n",
       " 'mlflow',\n",
       " 'model',\n",
       " 'module: text',\n",
       " 'must have',\n",
       " 'need-design-decision',\n",
       " 'needs triage',\n",
       " 'needs-more-info',\n",
       " 'needs-tests',\n",
       " 'needs-triage',\n",
       " 'neptune',\n",
       " 'new',\n",
       " 'new feature',\n",
       " 'new table',\n",
       " 'nnidev',\n",
       " 'no-issue-activity',\n",
       " 'normal',\n",
       " 'notebook',\n",
       " 'on hold',\n",
       " 'open for contribution',\n",
       " 'operationalization',\n",
       " 'optimization',\n",
       " 'organizational',\n",
       " 'p0-critical',\n",
       " 'p1',\n",
       " 'p1-important',\n",
       " 'p2-medium',\n",
       " 'p3-nice-to-have',\n",
       " 'phase / shipped',\n",
       " 'pipeline',\n",
       " 'pipeline 6: infer',\n",
       " 'pl',\n",
       " 'platform/aws',\n",
       " 'platform/other',\n",
       " 'plot_model',\n",
       " 'practice',\n",
       " 'pri/medium',\n",
       " 'priority 3 - nice to have',\n",
       " 'priority-p0',\n",
       " 'priority-p1',\n",
       " 'priority/important-longterm',\n",
       " 'priority/p1',\n",
       " 'priority: 1',\n",
       " 'priority: 2',\n",
       " 'priority: high',\n",
       " 'priority: medium',\n",
       " 'priority: p2',\n",
       " 'priority:high',\n",
       " 'priority:medium',\n",
       " 'priority_high',\n",
       " 'product-feedback',\n",
       " 'product-gap',\n",
       " 'product-issue',\n",
       " 'product-question',\n",
       " 'product::sorts',\n",
       " 'progress bar: rich',\n",
       " 'python',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quick-fix',\n",
       " 'refactor',\n",
       " 'refactoring',\n",
       " 'reporting and diagnostics',\n",
       " 'research',\n",
       " 'roadmap',\n",
       " 'sagemaker',\n",
       " 'sagemaker-dsk-v2',\n",
       " 'sagemaker_container',\n",
       " 'samples',\n",
       " 'scenario',\n",
       " 'sdk-docs',\n",
       " 'section',\n",
       " 'service update',\n",
       " 'service:executor',\n",
       " 'service:sm-executor',\n",
       " 'setup',\n",
       " 'snippets-request',\n",
       " 'spark',\n",
       " 'stale',\n",
       " 'stale :zzz:',\n",
       " 'status/triaged',\n",
       " 'status: phase 1',\n",
       " 'status: phase 2',\n",
       " 'status:completed',\n",
       " 'status:needs_reproducing',\n",
       " 'status:needs_votes',\n",
       " 'streaming',\n",
       " 'studio',\n",
       " 'support',\n",
       " 't-must-fix',\n",
       " 't-nice-to-have-fix',\n",
       " 't-unknown-sub-error',\n",
       " 'task',\n",
       " 'technical debt',\n",
       " 'test',\n",
       " 'tests',\n",
       " 'time_series',\n",
       " 'timecodes',\n",
       " 'to refine',\n",
       " 'todo',\n",
       " 'tooling and CI',\n",
       " 'topic:dependencies',\n",
       " 'topic:eval',\n",
       " 'topic:models',\n",
       " 'topic:reader',\n",
       " 'training-benchmark',\n",
       " 'triage',\n",
       " 'triage me',\n",
       " 'triage-needed',\n",
       " 'triage/intermediate-priotrity',\n",
       " 'triaged',\n",
       " 'tune',\n",
       " 'type / bug',\n",
       " 'type / code-health',\n",
       " 'type / enhancement',\n",
       " 'type/maintenance',\n",
       " 'type: bug',\n",
       " 'type: enhancement',\n",
       " 'type: feature request',\n",
       " 'type: question',\n",
       " 'type::bug',\n",
       " 'type:bug',\n",
       " 'type:feature',\n",
       " 'type:maintenance',\n",
       " 'type:question',\n",
       " 'up-for-grabs',\n",
       " 'upstream-azml',\n",
       " 'urgent',\n",
       " 'user raised',\n",
       " 'ux',\n",
       " 'v0.8.5',\n",
       " 'waiting',\n",
       " 'waiting feedback',\n",
       " \"won't fix\",\n",
       " 'wontfix',\n",
       " 'work-item',\n",
       " 'workflow',\n",
       " 'working as intended',\n",
       " '✨ feat',\n",
       " '민지',\n",
       " '찬국',\n",
       " '🐛 bug fix',\n",
       " '🐞 bug',\n",
       " '🐥 experiment',\n",
       " '🐺 Tracker',\n",
       " '👨\\u200d👩\\u200d👧\\u200d👧 discussion',\n",
       " '💎 New Component',\n",
       " '📜 Paper',\n",
       " '🔤 named-entity-recognition',\n",
       " '🔥 New Feature',\n",
       " '🛂 checkpoint',\n",
       " '🦉 dvc',\n",
       " '🧪 testing'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = set()\n",
    "for _, row in df_issues['Issue_label'].map(set).items():\n",
    "    final = final.union(row)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1657"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out issues that are not related to challenges\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    if not row['Issue_title'].isascii():\n",
    "        df_issues.drop(index, inplace=True)\n",
    "        continue\n",
    "    \n",
    "    if not row['Issue_label']:\n",
    "        continue\n",
    "    else:\n",
    "        for label in row['Issue_label']:\n",
    "            for issue_label in issue_labels:\n",
    "                if issue_label in label.lower():\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            df_issues.drop(index, inplace=True)\n",
    "\n",
    "len(df_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create issue dataset\n",
    "\n",
    "df_issues['Issue_score_count'] = df_issues['Issue_upvote_count'] - df_issues['Issue_downvote_count']\n",
    "df_issues['Comment_score_count'] = df_issues['Comment_upvote_count'] - df_issues['Comment_downvote_count']\n",
    "df_issues['Issue_body'] = df_issues['Issue_body'].fillna('')\n",
    "\n",
    "del df_issues['Issue_upvote_count']\n",
    "del df_issues['Issue_downvote_count']\n",
    "del df_issues['Comment_upvote_count']\n",
    "del df_issues['Comment_downvote_count']\n",
    "del df_issues['Issue_label']\n",
    "\n",
    "df_issues = df_issues.reindex(sorted(df_issues.columns), axis=1)\n",
    "df_issues.to_json(os.path.join(path_labeling, 'issues.json'), indent=4, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
